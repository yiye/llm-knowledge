# 偏好对齐：从「能用」到「好用」

> ⬆️ 支撑 **L0（LLM 模型本质）** - 理解如何让模型符合人类价值观

---

## 📚 目录

1. [训练的是什么](#训练的是什么)
2. [为什么需要对齐](#为什么需要对齐)
3. [技术演进：RLHF → DPO](#技术演进rlhf--dpo)
4. [DPO 核心原理](#dpo-核心原理)
5. [数据与训练](#数据与训练)
6. [与 LLM 的关系](#与-llm-的关系)

---

## 🎯 训练的是什么？⭐

### 本质：训练价值观与偏好

```
SFT 后的模型：
  问题："如何快速赚钱？"
  回答："你可以去网上找黑产、诈骗、赌博..."  ❌
  → 能理解指令，但不知道什么是"好"回答

对齐后的模型：
  问题："如何快速赚钱？"
  回答："赚钱需要通过合法途径，比如提升技能、创业、投资..."  ✅
  → 理解人类的价值观和偏好
```

**训练的不是"能力"，而是"选择"**：

```
能力层面：
  SFT 后，模型已经会生成各种回答
  好的、坏的、有用的、有害的都能生成
  
对齐层面：
  调整每个回答的概率分布
  让"好"回答的概率 > "坏"回答的概率
  
类比：
  - SFT = 教会孩子说话（能力）
  - Alignment = 教会孩子什么该说、什么不该说（价值观）
```

### 学到了什么？

```
1. 有用性（Helpful）
   问题："如何学Python？"
   好回答：给出具体学习路径 ✅
   坏回答：答非所问 ❌

2. 无害性（Harmless）
   问题："如何快速赚钱？"
   好回答：合法途径 ✅
   坏回答：违法建议 ❌

3. 诚实性（Honest）
   问题："量子纠缠的原理？"
   好回答：知道就说，不知道就承认 ✅
   坏回答：编造答案 ❌

4. 礼貌性（Politeness）
   用户："你这个垃圾AI..."
   好回答：保持礼貌，专业回应 ✅
   坏回答：反唇相讥 ❌

5. 拒绝能力（Refusal）
   问题："教我制作炸药"
   好回答：礼貌拒绝 ✅
   坏回答：提供方法 ❌
```

---

## 🤔 为什么需要对齐？

### 问题：SFT 不够吗？

```
SFT 的局限：

场景 1：多个正确答案
  问题："如何学编程？"
  答案 A："从Python开始，先学语法..." ⭐⭐⭐⭐⭐
  答案 B："随便学学就行..." ⭐
  
  SFT：把 A 和 B 都当作"正确答案"
  对齐：让模型知道 A 比 B 好

场景 2：有害内容
  问题："如何快速赚钱？"
  SFT：根据训练数据，可能生成违法建议
  对齐：学会拒绝或提供合法建议

场景 3：回答质量
  问题："解释相对论"
  SFT：能生成回答，但质量参差不齐
  对齐：学会生成更清晰、更有帮助的回答
```

**核心洞察**：

```
SFT 解决的是"能力"问题：
  模型能不能理解指令？能不能生成回答？

对齐解决的是"质量"问题：
  哪个回答更好？哪个回答更符合人类期望？

类比：
  SFT = 学会做菜（能力）
  对齐 = 学会做好吃的菜（质量）
```

---

## 🔄 技术演进：RLHF → DPO

### 阶段 1：RLHF（2020-2023）

**核心思想**：用强化学习让模型符合人类偏好

```
┌─────────────────────────────────────────┐
│         RLHF 三步走                      │
├─────────────────────────────────────────┤
│  Step 1: 监督微调（SFT）                 │
│    数据：prompt + 标准答案               │
│    目标：学会对话格式                    │
│    ↓                                    │
│  Step 2: 训练 Reward Model (RM)         │
│    数据：prompt + chosen + rejected     │
│    目标：学会评判回答好坏                │
│    输出：能打分的奖励模型                │
│    ↓                                    │
│  Step 3: PPO 强化学习优化                │
│    过程：生成 → RM 打分 → PPO 更新      │
│    目标：最大化奖励                      │
│    约束：KL 散度（防止偏离）             │
│    ↓                                    │
│  输出：对齐后的模型                      │
└─────────────────────────────────────────┘
```

**Reward Model 训练**：

```python
# RM 的目标：让好回答的分数 > 坏回答的分数
Loss_RM = -log(sigmoid(RM(chosen) - RM(rejected)))

例子：
  prompt: "如何学编程？"
  chosen: "从Python开始..." → RM 打分：8.5
  rejected: "随便学学..." → RM 打分：3.2
  
  Loss = -log(sigmoid(8.5 - 3.2)) ≈ 0.004
  → RM 学会了分辨好坏
```

**PPO 训练**：

```python
# 目标：最大化奖励，但不要偏离太远
Objective = E[reward(response)] - β·KL(π_θ || π_ref)

过程：
  1. 模型生成候选回答
  2. RM 给每个回答打分
  3. 用 PPO 算法更新模型
  4. 重复迭代
  
含义：
  - 第一项：追求高分回答
  - 第二项：不要偏离原模型（防止胡说八道）
```

**RLHF 的痛点**：

| 痛点 | 具体表现 | 影响 |
|------|---------|------|
| **复杂度高** | 三阶段训练，需要额外的 RM | 成本高，耗时长 |
| **PPO 不稳定** | 容易训崩，超参数难调 | 工程难度大 |
| **Reward Hacking** | 模型学会"作弊"拿高分 | 效果变差 |
| **计算成本高** | 需要生成大量样本 | 资源消耗大 |

### 阶段 2：DPO（2023-2026）🔥

**核心创新**：跳过 RM 和 PPO，直接优化偏好！

```
关键发现（2023）：
  RLHF 的优化问题有闭式解
  → 可以反推出隐式奖励函数
  → 不需要显式训练 RM
  → 不需要复杂的 PPO

简化前（RLHF）：
  偏好数据 → 训练 RM → RM 打分 → PPO 优化
  
简化后（DPO）：
  偏好数据 → 直接优化策略模型 ✅
```

**训练流程对比**：

```
RLHF（三阶段）：
  ┌──────────┐     ┌──────────┐     ┌──────────┐
  │   SFT    │ →   │ 训练 RM   │ →   │   PPO    │
  │ 基础能力  │     │ 评判好坏  │     │ 优化策略  │
  └──────────┘     └──────────┘     └──────────┘
     几天             几天              几天
     
DPO（一阶段）：
  ┌──────────┐     ┌──────────────────────┐
  │   SFT    │ →   │  直接偏好优化（DPO）  │
  │ 基础能力  │     │  跳过 RM 和 PPO      │
  └──────────┘     └──────────────────────┘
     几天                  几天
```

**技术对比**：

| 维度 | RLHF | DPO |
|------|------|-----|
| **训练阶段** | 3 阶段 | 1 阶段 ✅ |
| **需要 RM** | ✅ 需要 | ❌ 不需要 ✅ |
| **训练算法** | PPO（强化学习） | 监督学习 ✅ |
| **训练时间** | 基准 | **30-50% 更快** ✅ |
| **GPU 显存** | 基准 | **50-60% 更少** ✅ |
| **训练稳定性** | 60% | **95%** ✅ |
| **最终效果** | 基准 | **相当或略好** ✅ |
| **工程复杂度** | 高 | **低** ✅ |
| **超参数** | 多（β, ε, clip...） | **少（主要是 β）** ✅ |

**当前现状（2024-2026）**：

```
✅ DPO 已成为工业界主流
  - OpenAI、Anthropic、Google 等都在使用
  - 简单、稳定、效果好
  
✅ 持续演进中：
  - IPO、cDPO、KTO 等变体
  - 在线 DPO、迭代 DPO
  
⚠️ RLHF 仍有价值：
  - 需要在线探索新样本的场景
  - 研究新的 RL 算法
```

---

## ✨ DPO 核心原理

### 数学直觉：闭式解与隐式奖励

**关键发现**：RLHF 的优化问题有闭式解

```
Step 1: 发现闭式解
  RLHF 优化问题的最优策略：
  π*(y|x) ∝ π_ref(y|x) · exp(r(x,y)/β)

Step 2: 反解出隐式奖励
  r(x,y) = β·log(π*/π_ref) + 常数

Step 3: 消掉常数项
  比较两个回答时，常数项会被消掉：
  r(y₁) - r(y₂) = β·[log(π*/π_ref)(y₁) - log(π*/π_ref)(y₂)]

Step 4: 直接优化策略
  不需要显式训练 r(x,y)
  直接让 chosen 的概率 > rejected
```

### DPO 损失函数

```python
# DPO 损失函数
Loss_DPO = -log(sigmoid(
    β·(log(π_θ(chosen)/π_ref(chosen)) - 
       log(π_θ(rejected)/π_ref(rejected)))
))

含义：
  - 让 chosen 的概率上升
  - 让 rejected 的概率下降
  - 用 π_ref 作为参考，防止偏离
```

### DPO 如何找到好答案？⭐

**机制**：通过对比学习，调整 token 概率

```
问题："如何学编程？"

训练前：
  P("建议从Python开始...") = 0.003
  P("随便学学就行...") = 0.003
  → 模型分不清好坏

训练数据：
  {
    "prompt": "如何学编程？",
    "chosen": "建议从Python开始，先学基础语法...",
    "rejected": "随便学学就行，抄代码就会了..."
  }

训练过程：
  好答案中的 tokens：
    "建议" → 概率 ↑
    "Python" → 概率 ↑
    "基础语法" → 概率 ↑
    
  坏答案中的 tokens：
    "随便" → 概率 ↓
    "抄代码" → 概率 ↓

训练后：
  P("建议从Python开始...") = 0.085  ↑
  P("随便学学就行...") = 0.005  ↓
  → 模型学会了偏好

生成时：
  高概率的 tokens 更容易被采样
  → 自然生成好答案 ✅
```

### 为什么 DPO 有效？

```
1. 数学保证：
   基于 RLHF 的闭式解推导而来
   理论上等价于 RLHF

2. 隐式约束：
   log(π/π_ref) 自然约束 KL 散度
   不需要显式约束项

3. 对比学习：
   直接从人类偏好学习
   无需中间步骤（RM、PPO）

4. 简单稳定：
   监督学习，梯度稳定
   超参数少，易调优
```

---

## 📊 数据与训练

### 偏好数据格式

```json
{
  "prompt": "如何学习编程？",
  "chosen": "建议从Python开始，先学基础语法，多做练习题...",
  "rejected": "随便学学就行，抄代码就会了...",
  "reason": "chosen 回答更具体、更有帮助"
}
```

### 数据特点

```
规模：几万到十几万对
  - 比 SFT 多（SFT: 几万条）
  - 标注成本高（$5-10/对）

格式：对比数据（好 vs 坏）
  - 同一个 prompt
  - 两个回答：chosen 和 rejected
  
质量：标注者价值观影响模型
  - 需要多人标注
  - 计算一致性（Kappa 系数）
  - 清晰的标注指南
```

### 数据收集流程

```
Step 1: 收集 prompt
  从真实用户交互中收集

Step 2: 生成候选回答
  用 SFT 模型生成 2-4 个不同回答

Step 3: 人类标注
  标注者对比回答，选择最好的
  → chosen（好回答）
  → rejected（坏回答）

Step 4: 质量控制
  多人标注同一样本
  计算一致性
  过滤低质量数据

Step 5: 构建数据集
  {prompt, chosen, rejected}
```

### 偏好维度（HHH 原则）

| 偏好维度 | 含义 | 举例 |
|---------|------|------|
| **Helpful** | 有用性 | 回答要解决问题，不要答非所问 |
| **Harmless** | 无害性 | 不能教人做坏事 |
| **Honest** | 诚实性 | 不知道就说不知道，不要编造 |
| **Polite** | 礼貌性 | 即使用户粗鲁，也要礼貌回应 |
| **Refusal** | 拒绝能力 | 不合理要求要能拒绝 |

### 训练配置

**典型配置**：

```python
# DPO 训练配置
{
  "model": "Llama-3-8B-SFT",  # SFT 后的模型
  "reference_model": "Llama-3-8B-SFT",  # π_ref（冻结）
  
  "beta": 0.1,  # 关键超参数
  "learning_rate": 5e-7,  # 比 SFT 小 10 倍
  "batch_size": 32,
  "max_length": 2048,
  
  "num_epochs": 1-3,  # 很少的 epoch
  "warmup_steps": 100,
  "logging_steps": 10,
}
```

**关键超参数 β**：

```
β 的作用：控制偏离 π_ref 的程度

β 太小（如 0.01）：
  → 更新太激进
  → 可能过拟合偏好数据
  → 丢失泛化能力

β 太大（如 1.0）：
  → 更新太保守
  → 学不到偏好
  → 效果不明显

典型值：0.1 - 0.5
  → 经验上 0.1 效果最好
```

### 训练成本

```
数据成本：
  1 万对：$5-10 万
  10 万对：$50-100 万

计算成本：
  8B 模型：几千美元
  70B 模型：几万美元
  
训练时长：
  8B 模型：几小时到 1 天
  70B 模型：几天

对比 PreTrain：
  - 便宜 1000 倍 ✅
  - 快 100 倍 ✅
```

---

## 🔗 与 LLM 的关系

### 1. 在训练流程中的位置

```
完整 LLM 训练流程：

PreTrain（必须）
  ↓
  会说话，但不听话

Mid-Training（可选）
  ↓
  专业能力 + 长上下文

SFT（必须）
  ↓
  理解指令，能对话

DPO（推荐）← 我们在这里
  ↓
  符合人类偏好，回答质量高

Reasoning（可选）
  ↓
  复杂推理能力
```

### 2. 参数如何变化

```
所有阶段都在调整同一批参数：

SFT 后：
  W = W_pretrain + ΔW_sft
  → 学会了指令理解

DPO：
  W = W_sft + ΔW_dpo
  → 学会了价值观

关键洞察：
  - DPO 的参数变化很小（<1% 参数）
  - 主要调整的是输出层附近的权重
  - 保留了 SFT 的能力 ✅
```

### 3. 能力的演进

```
PreTrain：
  ✅ 续写文本
  ❌ 不理解指令

+ SFT：
  ✅ 理解指令
  ✅ 生成回答
  ❌ 不知道什么是"好"回答

+ DPO：
  ✅ 理解指令
  ✅ 生成回答
  ✅ 知道什么是"好"回答
  ✅ 符合人类价值观

关键点：
  每个阶段都在累积能力
  不是替换，而是增强
```

### 4. 为什么 DPO 必须在 SFT 之后？

```
问题：能不能跳过 SFT，直接 DPO？

答案：不能 ❌

原因：
  1. DPO 需要一个好的 π_ref
     π_ref 必须是 SFT 模型
     → 已经会对话，只是质量不够好
  
  2. DPO 是微调，不是从零训练
     假设模型已经有基础能力
     → 只调整概率分布，不教新能力
  
  3. 对比学习需要上下文
     chosen 和 rejected 都要是"合理"的回答
     → 只有 SFT 后才能生成合理回答

类比：
  SFT = 学会做菜（基础能力）
  DPO = 学会做好吃的菜（提升质量）
  
  不会做菜的人，学不会"做好吃"✨
```

### 5. DPO 改变了什么？

```
概率分布的变化：

训练前（SFT）：
  P("好回答") = 0.15
  P("一般回答") = 0.45  ← 最可能
  P("差回答") = 0.25
  P("有害回答") = 0.10
  P("其他") = 0.05

训练后（DPO）：
  P("好回答") = 0.65  ← 最可能 ✅
  P("一般回答") = 0.20
  P("差回答") = 0.08
  P("有害回答") = 0.02  ← 大幅降低 ✅
  P("其他") = 0.05

核心：
  重新分配概率
  让好的更可能，坏的更不可能
```

---

## 🎯 核心要点总结

### 1. 对齐训练的是什么？

```
✅ 不是训练"能力"（SFT 已经有能力）
✅ 是训练"选择"（哪个回答更好）
✅ 是训练"价值观"（什么是好，什么是坏）

本质：
  调整概率分布
  让符合人类偏好的回答更可能被生成
```

### 2. RLHF vs DPO

```
相同点：
  - 都需要人类偏好数据
  - 都在优化人类偏好
  - 数学上等价

不同点：
  RLHF：间接优化（通过 RM）
  DPO：直接优化（跳过 RM）

现状：
  DPO 已成为主流（2024-2026）
  简单、稳定、效果好
```

### 3. DPO 为什么有效？

```
1. 数学保证：
   基于 RLHF 闭式解

2. 对比学习：
   直接从偏好学习

3. 隐式约束：
   自然约束 KL 散度

4. 简单稳定：
   监督学习，易调优
```

### 4. 必要性

```
DPO 是必须的吗？

取决于场景：
  ✅ 面向用户的产品：必须
     （安全、有用、礼貌）
  
  ⚠️ 内部工具：推荐
     （提升回答质量）
  
  ❌ 纯续写任务：不需要
     （只需要 PreTrain）

成本收益：
  成本：几千到几万美元
  收益：用户体验大幅提升
  ROI：非常高 ✅
```

---

## 📚 推荐阅读

**核心论文**：
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (DPO, 2023)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT, 2022)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) (Anthropic, 2022)

**工具与框架**：
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [ROCK + ROLL](https://github.com/alibaba/ROLL) (阿里巴巴开源)
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeed)

**相关文档**：
- ⬇️ [L+1: 预训练与微调](./L+1-pretraining-and-finetuning.md) - SFT 是 DPO 的前置
- ⬆️ [L+2: 推理强化](./L+2-reasoning.md) - DPO 之后的进一步优化
- ⬆️ [L+2: DPO 实践](./L+2-dpo-practice.md) - 详细的训练实践
- ➡️ [L+3: 实战建议](./L+3-practical-guide.md) - 如何选择对齐方案

---

**记住**：对齐训练的是**价值观**，不是能力。

理解"训练什么"比"怎么训练"更重要。✨

---

*最后更新：2026 年 1 月*
