# DPO 实践：从数据到部署

> ⬆️ 支撑 **L+2（对齐与推理）** - 掌握 DPO 训练的完整流程

---

## 📚 目录

1. [训练的是什么](#训练的是什么)
2. [训练数据准备](#训练数据准备)
3. [参数更新机制](#参数更新机制)
4. [防止过拟合](#防止过拟合)
5. [模型量化](#模型量化)
6. [训练配置](#训练配置)
7. [与 LLM 的关系](#与-llm-的关系)

---

## 🎯 训练的是什么？⭐

### 本质：微调概率分布

```
DPO 训练的核心：
  不是教模型新能力
  而是调整已有回答的概率分布
  
  让"好"回答的概率 ↑
  让"坏"回答的概率 ↓
```

**具体机制**：

```python
# DPO 损失函数
Loss = -log σ(β·[log π_θ(chosen)/π_ref(chosen) - 
                  log π_θ(rejected)/π_ref(rejected)])

训练过程：
  1. chosen 回答中的每个 token
     → 概率提升 ↑
     
  2. rejected 回答中的每个 token
     → 概率降低 ↓
     
  3. 用 π_ref 作为锚点
     → 防止偏离太远
```

### 训练前后的变化

```
问题："如何学编程？"

训练前（SFT）：
  模型能生成多种回答，概率相近：
  
  "建议从Python开始，先学基础..." → P = 0.15
  "可以学Java，或者C++..." → P = 0.12
  "随便学学就行了..." → P = 0.10
  "抄代码就会了..." → P = 0.08
  
  → 模型不知道哪个更好

训练数据：
  {
    "prompt": "如何学编程？",
    "chosen": "建议从Python开始，先学基础...",
    "rejected": "随便学学就行了..."
  }

训练中：
  反向传播调整参数：
  
  chosen 中的 tokens：
    "建议" → 权重 ↑
    "Python" → 权重 ↑
    "基础" → 权重 ↑
    
  rejected 中的 tokens：
    "随便" → 权重 ↓
    "抄" → 权重 ↓

训练后：
  概率分布被重新调整：
  
  "建议从Python开始，先学基础..." → P = 0.45  ✅ 大幅提升
  "可以学Java，或者C++..." → P = 0.18  （保持）
  "随便学学就行了..." → P = 0.03  ❌ 大幅降低
  "抄代码就会了..." → P = 0.01  ❌ 大幅降低
  
  → 模型学会了偏好
```

### 学到了什么？

```
1. Token 级别的偏好
   好词："建议"、"推荐"、"建立"
   坏词："随便"、"抄"、"作弊"

2. 短语级别的偏好
   好短语："从基础开始"、"系统学习"
   坏短语："没必要"、"浪费时间"

3. 风格级别的偏好
   好风格：具体、有步骤、有建议
   坏风格：敷衍、模糊、不负责任

4. 价值观级别的偏好
   好价值观：合法、有用、诚实
   坏价值观：违法、有害、欺骗
```

---

## 📊 训练数据准备

### 数据格式

DPO 训练需要**偏好对数据**（与 RLHF 完全相同）：

```json
{
  "prompt": "如何学习编程？",
  "chosen": "建议从Python开始，先学基础语法，多做练习题...",
  "rejected": "随便学学就行，抄代码就会了..."
}
```

### 数据收集流程

```
Step 1: 收集 prompt
  从用户交互中收集真实问题

Step 2: 生成候选回答
  用基础模型生成 2-4 个不同回答

Step 3: 人类标注
  标注者对比回答，选择最好的
  → chosen（好回答）
  → rejected（坏回答）

Step 4: 构建数据集
  {prompt, chosen, rejected}
```

### 数据规模

| 数据集大小 | 适用场景 | 标注成本 |
|-----------|---------|---------|
| 1万对 | 快速验证、领域微调 | $5-10万 |
| 5万对 | 中型项目 | $25-50万 |
| 10万对 | 大型项目 | $50-100万 |

**成本说明**：按 $5-10/对 计算

---

## 🔄 参数更新机制

### 完整训练循环

```
For each batch:
  1️⃣ 正向传播
     输入 → 模型计算 → 输出概率 → 计算损失

  2️⃣ 反向传播
     损失 → 计算梯度 → 梯度回传到每层

  3️⃣ 参数更新
     参数 = 参数 - 学习率 × 梯度

  4️⃣ 清空梯度
     准备下一个 batch
```

### 正向传播：计算概率

```python
# 对 chosen 和 rejected 分别计算对数概率
log_prob_chosen = model.log_prob(prompt, chosen)
log_prob_rejected = model.log_prob(prompt, rejected)

# 同时计算参考模型的概率（冻结，不更新）
log_prob_chosen_ref = ref_model.log_prob(prompt, chosen)
log_prob_rejected_ref = ref_model.log_prob(prompt, rejected)

# 计算 DPO 损失
logits = beta * (
    (log_prob_chosen - log_prob_chosen_ref) -
    (log_prob_rejected - log_prob_rejected_ref)
)
loss = -log(sigmoid(logits))
```

### 反向传播：计算梯度

**核心机制**：链式法则，从后往前传递梯度

```
损失 L
  ↓ ∂L/∂logits
logits
  ↓ ∂logits/∂log_prob
log_prob (chosen, rejected)
  ↓ ∂log_prob/∂模型输出
模型输出概率
  ↓ ∂概率/∂logits
模型 logits
  ↓ ∂logits/∂hidden
隐藏层
  ↓ ∂hidden/∂参数
模型参数（权重、偏置）
```

**效果**：
- 好答案中的 tokens → 梯度为正 → 参数调整后概率提升
- 坏答案中的 tokens → 梯度为负 → 参数调整后概率降低

### 参数更新：优化器

**基础梯度下降**：

```python
param_new = param_old - learning_rate * gradient
```

**Adam 优化器**（推荐）：

```python
# 维护动量和自适应学习率
m = beta1 * m + (1 - beta1) * grad         # 一阶矩（动量）
v = beta2 * v + (1 - beta2) * grad²        # 二阶矩（方差）

param = param - lr * m / (sqrt(v) + eps)
```

**关键超参数**：
- 学习率：1e-6 ~ 5e-5（通常 1e-5）
- Beta1：0.9
- Beta2：0.999
- Weight decay：0.01（L2 正则化）

---

## 🛡️ 防止过拟合

### 什么是过拟合？

```
症状：
  训练集：准确率 99% ✅
  测试集：准确率 60% ❌
  → 模型"死记硬背"训练数据，泛化能力差

表现：
  对训练过的问题：回答完美
  对新问题（措辞稍变）：答非所问
```

### DPO 内置防护：KL 散度约束

**机制**：DPO 损失函数中的 `log(π_θ/π_ref)` 自动约束模型不能偏离参考模型太远

```
如果模型变化太大：
  → log(π_θ/π_ref) 变大
  → 损失增加
  → 梯度会"拉回"模型
  
效果：
  模型保持在参考模型附近
  → 保留原有泛化能力
  → 只做温和调整
```

### 实用防过拟合技术

#### 1. 调整 Beta 参数

**Beta 的作用**：控制 KL 约束强度

```
beta 大（如 0.5）：
  → KL 约束强
  → 模型变化小
  → 不容易过拟合 ✅
  → 但可能学习不充分 ⚠️

beta 小（如 0.01）：
  → KL 约束弱
  → 模型变化大
  → 学习充分 ✅
  → 但容易过拟合 ⚠️

推荐值：0.05 - 0.3
```

#### 2. Early Stopping（早停）

**策略**：当验证集性能不再提升时，停止训练

```python
best_val_loss = infinity
patience = 3  # 容忍 3 轮不改善
counter = 0

for epoch in epochs:
    train_loss = train()
    val_loss = validate()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            stop_training()
            load_best_model()
```

#### 3. 正则化技术

**Dropout**（训练时随机失活神经元）：

```python
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    dropout=0.1  # 10% dropout
)
```

**L2 正则化**（权重衰减）：

```python
optimizer = AdamW(
    model.parameters(),
    lr=1e-5,
    weight_decay=0.01  # L2 正则化系数
)
```

**梯度裁剪**（防止梯度爆炸）：

```python
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_grad_norm=1.0
)
```

#### 4. 数据增强

**增加数据多样性**：

```
原始 prompt: "如何学习编程？"

增强 prompts:
  - "怎样学习编程？"
  - "想学编程，从哪开始？"
  - "编程入门建议"
  - "零基础如何学编程？"

使用相同的 chosen/rejected
```

#### 5. 验证集监控

**多维度评估**：

| 指标 | 目标 | 警戒线 |
|------|------|--------|
| 训练/验证损失差距 | < 10% | > 15% 🚨 |
| KL 散度 | < 1.0 | > 1.5 🚨 |
| 域外准确率 | > 70% | < 50% 🚨 |

---

## ⚙️ 模型量化

### 什么是模型量化？

**定义**：用更少的位数表示模型参数

```
FP32 (4字节) → FP16 (2字节) → INT8 (1字节)

牺牲一点精度，换取：
  ✅ 内存占用减少 75%
  ✅ 推理速度提升 2-4x
  ✅ 部署成本降低
```

### 不同精度对比

| 类型 | 位数 | 字节 | 精度 | 适用场景 |
|------|------|------|------|---------|
| **FP32** | 32 | 4 | 7位有效数字 | 训练基准 |
| **FP16** | 16 | 2 | 3-4位有效数字 | 训练/推理 ✅ |
| **BF16** | 16 | 2 | 2-3位有效数字 | 训练 ✅ |
| **INT8** | 8 | 1 | 256级别 | 推理 ✅ |
| **INT4** | 4 | 0.5 | 16级别 | 移动端 ✅ |

### 量化原理（INT8 为例）

```
Step 1: 计算缩放因子
  scale = max(|最小值|, |最大值|) / 127

Step 2: 量化
  value_int8 = round(value_fp32 / scale)
  限制在 [-127, 127]

Step 3: 反量化（推理时）
  value_fp32_approx = value_int8 * scale
```

**示例**：
```
原始值：0.523
scale = 0.00786
量化：round(0.523 / 0.00786) = 67
反量化：67 * 0.00786 = 0.5265
误差：0.0035 (0.67%)
```

### 量化方法

#### 1. 训练后量化（PTQ）

```
优点：
  ✅ 快速（几分钟）
  ✅ 不需要重新训练

缺点：
  ❌ 精度损失 2-5%

适用：快速部署、资源有限
```

#### 2. 量化感知训练（QAT）

```
优点：
  ✅ 精度损失 < 1%
  ✅ 模型适应量化误差

缺点：
  ❌ 需要重新训练
  ❌ 时间长

适用：精度要求高、有充足资源
```

#### 3. 混合精度

```
策略：敏感层高精度，不敏感层低精度

示例：
  - 词嵌入层：FP16（敏感）
  - 注意力层：FP16（敏感）
  - 前馈网络：INT8（不敏感）
  - 输出层：FP16（敏感）
```

### 模型大小对比

**LLaMA-7B 为例**：

| 精度 | 文件大小 | 推理速度 | 精度损失 |
|------|---------|---------|---------|
| FP32 | 26 GB | 基准 | 0% |
| FP16 | 13 GB | 1.5-2x | < 0.5% |
| INT8 | 6.5 GB | 2-4x | 1-3% |
| INT4 | 3.25 GB | 4-8x | 3-5% |

### 量化选择建议

```
训练阶段：
  ✅ FP16/BF16 混合精度训练
  ❌ 不推荐 INT8

云端推理：
  ✅ FP16（平衡精度和速度）
  ✅ INT8（成本优先）

边缘设备：
  ✅ INT8（推荐）
  ✅ INT4（资源极限）

移动端：
  ✅ INT4（几乎唯一选择）
```

---

## 🔧 训练配置

### 完整配置示例

```python
training_config = {
    # 数据
    'train_data_path': 'train.jsonl',
    'val_data_path': 'val.jsonl',
    'batch_size': 32,
    
    # 模型
    'model_name': 'meta-llama/Llama-2-7b',
    'ref_model': 'same_as_policy',  # 参考模型
    
    # DPO 超参数
    'beta': 0.1,           # KL 约束强度
    'max_length': 512,     # 最大序列长度
    
    # 优化器
    'optimizer': 'adamw',
    'learning_rate': 1e-5,
    'weight_decay': 0.01,
    'warmup_steps': 100,
    
    # 训练控制
    'num_epochs': 3,
    'max_grad_norm': 1.0,  # 梯度裁剪
    'eval_every': 100,     # 多少步评估一次
    
    # 防过拟合
    'dropout': 0.1,
    'early_stopping_patience': 3,
    
    # 精度
    'mixed_precision': 'fp16',
    
    # 监控
    'log_every': 10,
    'save_every': 500,
}
```

### 训练流程

```
1. 初始化
   - 加载策略模型
   - 加载参考模型（冻结）
   - 准备数据集

2. 训练循环
   For each epoch:
     For each batch:
       - 正向传播
       - 计算 DPO 损失
       - 反向传播
       - 梯度裁剪
       - 参数更新
       - 定期验证

3. 早停检查
   - 监控验证损失
   - 触发条件：patience 轮不改善
   - 加载最佳模型

4. 保存模型
   - 保存最终权重
   - 保存训练配置
```

### 监控指标

**训练阶段**：

| 指标 | 含义 | 健康值 |
|------|------|--------|
| Loss | DPO 损失 | 持续下降 |
| Accuracy | 好答案概率 > 坏答案 | > 85% |
| KL Divergence | 与参考模型的偏离 | < 1.0 |
| Learning Rate | 动态学习率 | 逐步衰减 |

**验证阶段**：

| 指标 | 含义 | 健康值 |
|------|------|--------|
| Val Loss | 验证集损失 | 接近训练损失 |
| Overfitting Gap | 训练/验证差距 | < 10% |
| Out-of-Domain Acc | 域外泛化能力 | > 70% |

---

## ❓ 常见问题

### Q1: DPO 和 RLHF 哪个更好？

**答**：取决于场景

```
选 DPO 如果：
  ✅ 追求训练效率
  ✅ 计算资源有限
  ✅ 需要快速迭代
  ✅ 当前工业界主流

选 RLHF 如果：
  ✅ 需要在线探索
  ✅ 研究新 RL 算法
  ✅ 有充足资源
```

### Q2: Beta 参数如何设置？

**答**：0.05 - 0.3，根据任务调整

```
起始值：0.1
观察 KL 散度：
  - KL > 1.5 → 增大 beta（约束更强）
  - KL < 0.3 → 减小 beta（学习更充分）
```

### Q3: 需要多少训练数据？

**答**：取决于任务复杂度

```
最小可行：1万对（快速验证）
推荐规模：3-5万对（一般项目）
大型项目：10万+对（复杂任务）
```

### Q4: 训练需要多久？

**答**：取决于模型大小和数据量

```
7B 模型 + 5万对数据：
  - 单卡 A100：2-3天
  - 8卡 A100：6-12小时

70B 模型 + 10万对数据：
  - 8卡 A100：3-7天
  - 64卡 A100：1-2天
```

### Q5: 如何判断模型是否过拟合？

**答**：监控多个指标

```
过拟合信号：
  🚨 训练/验证损失差距 > 15%
  🚨 KL 散度 > 1.5
  🚨 域外准确率 < 50%
  🚨 验证损失先降后升

解决方案：
  ✅ 增大 beta
  ✅ 启用早停
  ✅ 增加正则化
```

### Q6: 量化会损失多少精度？

**答**：取决于量化方法

```
FP16：< 0.5%（几乎无损）
INT8 + PTQ：1-3%
INT8 + QAT：< 1%
INT4 + GPTQ：3-5%
```

### Q7: 能否用合成数据代替人类标注？

**答**：可以，但有限制

```
✅ 可行场景：
  - 用强模型生成弱模型训练数据
  - 代码/数学等可验证任务
  - 混合使用（人工 + 合成）

❌ 不推荐：
  - 完全依赖合成数据
  - 创意性任务
  - 主观判断任务
```

---

## 🎯 最佳实践总结

### 训练前

- [ ] 准备高质量偏好数据（至少 1 万对）
- [ ] 设置合理的 beta 值（0.1 起始）
- [ ] 配置验证集（至少 10%）
- [ ] 启用 Dropout 和 L2 正则化

### 训练中

- [ ] 每 100 步验证一次
- [ ] 监控 KL 散度（< 1.0）
- [ ] 观察训练/验证损失差距（< 10%）
- [ ] 使用梯度裁剪（max_norm=1.0）

### 训练后

- [ ] 在多样化测试集上评估
- [ ] 人工抽查生成质量
- [ ] 考虑量化部署（FP16/INT8）
- [ ] 持续监控线上表现

---

## 📚 参考资料

**工具和库**：
- [Hugging Face TRL](https://github.com/huggingface/trl) - DPO 训练库
- [ROCK + ROLL](https://github.com/alibaba/ROLL) - 阿里开源 RL 平台
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) - 开源 RLHF 框架

**相关文档**：
- ⬇️ [L+2: 对齐原理](./L+2-alignment-rlhf-dpo.md) - DPO 核心原理
- ➡️ [L+2: 推理强化](./L+2-reasoning.md) - 下一步优化方向
- ➡️ [L+3: 实战建议](./L+3-practical-guide.md) - 如何选择训练方案

---

## 🔗 与 LLM 的关系

### 1. 在训练流程中的位置

```
完整训练流程：
  PreTrain → SFT → DPO ← 我们在这里

DPO 的作用：
  - 不改变模型架构
  - 不增加新能力
  - 只调整概率分布

类比：
  PreTrain = 学说话（语言能力）
  SFT = 学对话（指令理解）
  DPO = 学礼貌（价值观） ← 这里
```

### 2. 参数更新的特点

```
参数变化量：
  PreTrain: 从随机初始化 → 语言模型
            变化量：100%
  
  SFT: 从语言模型 → 对话模型
       变化量：~5%
  
  DPO: 从对话模型 → 对齐模型
       变化量：<1% ✅

关键洞察：
  - DPO 只需要很小的参数变化
  - 主要调整输出层附近的权重
  - 保留了 SFT 和 PreTrain 的能力
```

### 3. 为什么需要 π_ref？

```
π_ref 的作用：
  作为锚点，防止模型偏离太远

没有 π_ref 会怎样？
  训练中（没有 π_ref）：
    为了最大化 chosen 的概率
    → 模型可能过拟合训练数据
    → 失去泛化能力
  
  训练后：
    只会回答训练集里见过的问题 ❌
    没见过的问题答不好了 ❌

有 π_ref 约束：
  训练后：
    - 保持了泛化能力 ✅
    - 学会了偏好 ✅

数学表达：
  KL(π_θ || π_ref) 
  → 测量偏离程度
  → 自动约束在合理范围
```

---

**记住**：DPO 训练的是**选择**，不是能力。

理解参数更新的机制，才能调好超参数。✨

---

*最后更新：2026 年 1 月*

---

*最后更新：2026年1月*
