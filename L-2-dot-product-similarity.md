# 点积与相似度：Attention 的数学基础

> ⬆️ 支撑 **L0（LLM 模型本质）** - 理解 Attention 为什么用点积

---

## 📚 目录

1. [点积的两个定义](#点积的两个定义)
2. [点积的几何意义](#点积的几何意义)
3. [余弦相似度](#余弦相似度)
4. [为什么 Attention 用点积](#为什么-attention-用点积)
5. [与 LLM 的关系](#与-llm-的关系)

---

## 🎯 点积的两个定义

### 定义 1：代数定义（计算公式）

```python
A = [a₁, a₂, a₃, ..., aₙ]
B = [b₁, b₂, b₃, ..., bₙ]

A · B = a₁×b₁ + a₂×b₂ + a₃×b₃ + ... + aₙ×bₙ
```

**例子**：

```python
A = [1, 2, 3]
B = [4, 5, 6]

A · B = 1×4 + 2×5 + 3×6
      = 4 + 10 + 18
      = 32
```

### 定义 2：几何定义（本质）⭐

```
A · B = |A| × |B| × cos(θ)

其中：
  |A| = A 的模（长度）= √(a₁² + a₂² + ... + aₙ²)
  |B| = B 的模（长度）
  θ   = A 和 B 之间的夹角
```

**例子**：

```
A = [3, 4]
B = [3, 4]

代数计算：
  A · B = 3×3 + 4×4 = 9 + 16 = 25

几何计算：
  |A| = √(3² + 4²) = √25 = 5
  |B| = √(3² + 4²) = 5
  θ = 0°（方向相同）
  
  A · B = 5 × 5 × cos(0°)
        = 5 × 5 × 1
        = 25  ✅（结果相同）
```

### 两个定义的关系

```
核心洞察：
  代数定义 = 计算方法（怎么算）
  几何定义 = 意义（为什么这样算）

连接代数和几何的桥梁：
  点积把"逐元素相乘求和"
  转化为"长度×长度×角度余弦"
  
这就是数学的美妙之处！✨
```

### θ 是怎么来的？⭐

**问题**：只知道两个向量的坐标，夹角 θ 怎么得到？

**答案**：θ 不是直接"看到"的，而是通过点积**反推**出来的！

```
已知：向量 A = [a₁, a₂, ...] 和 B = [b₁, b₂, ...]

步骤 1：用代数定义计算点积（能直接算）
  A·B = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ

步骤 2：计算两个向量的模（能直接算）
  |A| = √(a₁² + a₂² + ... + aₙ²)
  |B| = √(b₁² + b₂² + ... + bₙ²)

步骤 3：反推 cos(θ)
  因为两个定义等价：A·B = |A| × |B| × cos(θ)
  
  所以：cos(θ) = (A·B) / (|A| × |B|)  ← 余弦相似度！
  
  如果需要角度：θ = arccos((A·B) / (|A| × |B|))
```

**具体例子**：

```python
A = [3, 4]
B = [4, 3]

# 步骤 1：代数定义算点积
A·B = 3×4 + 4×3 = 24

# 步骤 2：算模
|A| = √(3² + 4²) = 5
|B| = √(4² + 3²) = 5

# 步骤 3：反推 cos(θ)
cos(θ) = 24 / (5 × 5) = 0.96

# 如果要角度
θ = arccos(0.96) ≈ 16.3°
```

**关键理解**：

```
  代数定义（能算）  ═══════════  几何定义（有意义）
       ↓                              ↓
    A·B = Σaᵢbᵢ    ←───等价───→   A·B = |A||B|cos(θ)
       │                              │
       └──────────┬───────────────────┘
                  ↓
         cos(θ) = (A·B) / (|A||B|)
                  ↓
         用简单的乘加运算 → 得到抽象的角度关系
```

**与 LLM 的关系**：
- ✅ Attention 的核心：Q·K^T（点积）
- ✅ 计算用代数定义（快）
- ✅ 理解用几何定义（直观）

---

## 📐 点积的几何意义

### 核心直觉：投影 ⭐

**点积 = 投影长度 × 被投影向量的长度**

```
        B
       ↗
      /
     /  θ
    /___________→ A
    └─投影

B 在 A 方向的投影 = |B| × cos(θ)

点积 A·B = |A| × (|B| × cos(θ))
         = |A| × 投影长度
```

### 可视化理解

```
场景 1：同向（θ = 0°）
  A →
  B →
  
  投影 = |B|（完全投影）
  点积 = |A| × |B|  ← 最大


场景 2：垂直（θ = 90°）
  A →
  B ↑
  
  投影 = 0（没有投影）
  点积 = 0  ← 完全无关


场景 3：反向（θ = 180°）
  A →
  B ←
  
  投影 = -|B|（反方向投影）
  点积 = -|A| × |B|  ← 最小（负值）


场景 4：夹角 45°
  A →
  B ↗
  
  投影 = |B| × cos(45°) ≈ 0.707|B|
  点积 = |A| × 0.707|B|  ← 中等
```

### 具体例子：2D 空间

```python
A = [4, 0]  # 沿 x 轴，长度 4
B = [3, 3]  # 斜向 45°

# 代数计算
A · B = 4×3 + 0×3 = 12

# 几何理解
|A| = 4
|B| = √(3² + 3²) = √18 ≈ 4.24
θ = 45°

B 在 A 方向的投影 = |B| × cos(45°)
                    = 4.24 × 0.707
                    ≈ 3

点积 = |A| × 投影 = 4 × 3 = 12 ✅
```

### 不同角度的点积规律

```
角度 θ     cos(θ)    点积特征        含义
───────────────────────────────────────────
0°         1.0       最大（正值）    完全同向
30°        0.87      大（正值）      基本同向
45°        0.71      中等（正值）    部分同向
60°        0.50      小（正值）      略微同向
90°        0.0       零             垂直（无关）
120°      -0.50      小（负值）      略微反向
135°      -0.71      中等（负值）    部分反向
180°      -1.0       最小（负值）    完全反向
```

**规律**：
```
✅ 点积 > 0  → 夹角 < 90°  → 方向一致  → 相似
✅ 点积 = 0  → 夹角 = 90°  → 垂直      → 无关
✅ 点积 < 0  → 夹角 > 90°  → 方向相反  → 相反
```

### 高维空间的点积

```
2D 空间：容易可视化
  [1, 2] · [3, 4] = 11

768 维空间（LLM 常用）：无法可视化
  vec₁ · vec₂ = v₁₁×v₂₁ + v₁₂×v₂₂ + ... + v₁₇₆₈×v₂₇₆₈

但几何意义相同：
  ✅ 点积大 → 方向一致 → 语义相似
  ✅ 点积小 → 方向不同 → 语义不相似
  ✅ 点积为 0 → 垂直 → 完全无关

→ 高维空间中，点积仍然测量"方向一致性"
```

**与 LLM 的关系**：
- ✅ Attention 中的 Q·K 就是测量方向一致性
- ✅ 点积大 → Query 和 Key 相似 → 注意力权重高
- ✅ 点积小 → Query 和 Key 不相似 → 注意力权重低

---

## 📏 余弦相似度

### 定义

```
余弦相似度(A, B) = cos(θ) = (A · B) / (|A| × |B|)

本质：
  把点积归一化到 [-1, 1] 区间
  只看方向，忽略长度
```

### 从点积到余弦相似度

```
点积：
  A · B = |A| × |B| × cos(θ)
  → 受长度影响

余弦相似度：
  cos(θ) = (A · B) / (|A| × |B|)
  → 消除长度影响，只看角度
```

### 具体例子

```python
# 例子 1：相同方向，不同长度
A = [1, 2]
B = [2, 4]  # B 是 A 的 2 倍

点积：
  A · B = 1×2 + 2×4 = 10

余弦相似度：
  |A| = √(1² + 2²) = √5 ≈ 2.24
  |B| = √(2² + 4²) = √20 ≈ 4.47
  
  cos(θ) = 10 / (2.24 × 4.47)
         = 10 / 10
         = 1.0  ✅（完全相同方向）


# 例子 2：不同方向
猫 = [0.8, 0.6, 0.2]
狗 = [0.7, 0.5, 0.3]

点积：
  猫 · 狗 = 0.8×0.7 + 0.6×0.5 + 0.2×0.3
         = 0.56 + 0.30 + 0.06
         = 0.92

余弦相似度：
  |猫| = √(0.8² + 0.6² + 0.2²) ≈ 1.02
  |狗| = √(0.7² + 0.5² + 0.3²) ≈ 0.91
  
  cos(θ) = 0.92 / (1.02 × 0.91)
         ≈ 0.99  ← 非常相似！


# 例子 3：完全无关
猫 = [1, 0]
苹果 = [0, 1]

点积：
  猫 · 苹果 = 1×0 + 0×1 = 0

余弦相似度：
  cos(θ) = 0 / (1 × 1) = 0  ← 垂直（无关）
```

### 余弦相似度的取值范围

```
+1.0：完全相同方向（最相似）
      例：猫 vs 小猫

+0.7：夹角约 45°（比较相似）
      例：猫 vs 狗

 0.0：夹角 90°（完全无关）
      例：猫 vs 苹果

-0.7：夹角约 135°（有些相反）
      例：好 vs 坏

-1.0：完全相反方向（最不相似）
      例：热 vs 冷
```

### 可视化：余弦相似度

```
2D 空间可视化：

      猫 ●
     ↗ 
    /  θ=20°
   /    cos(θ)=0.94
  ● 起点
   \    θ=30°
    \   cos(θ)=0.87
     ↘
      狗 ●


垂直方向：
  
  猫 ●
  ↑
  |  θ=90°
  |  cos(θ)=0
  ● 起点 ───→ 苹果 ●
```

### 余弦 vs 欧氏距离

```
场景：比较词向量的相似度

方法 1：欧氏距离
  distance(A, B) = √(Σ(aᵢ - bᵢ)²)
  
  问题：
    A = [1, 0]
    B = [2, 0]  # 方向相同，但长度不同
    C = [0, 1]  # 方向完全不同
    
    distance(A, B) = 1  ← 距离近
    distance(A, C) = √2 ≈ 1.4  ← 距离也近
    
    ❌ 无法区分方向差异


方法 2：余弦相似度
  similarity(A, B) = cos(θ)
  
  A = [1, 0]
  B = [2, 0]  # 方向相同
  C = [0, 1]  # 方向不同
  
  similarity(A, B) = 1.0  ← 完全相同
  similarity(A, C) = 0.0  ← 完全不同
  
  ✅ 正确反映语义关系
```

### 为什么 NLP 用余弦相似度？

```
词向量的特性：
  长度（模）≈ 词频（出现频率）
  方向 = 词义（语义信息）

例子：
  "猫" 在语料中出现 10000 次 → 向量长度大
  "小猫" 在语料中出现 100 次 → 向量长度小
  
  但两者语义相似！
  → 应该忽略长度，只看方向
  → 用余弦相似度 ✅

欧氏距离的问题：
  "猫"（高频词）的向量长
  "小猫"（低频词）的向量短
  → 欧氏距离可能很大
  → 误判为不相似 ❌
```

### 点积与余弦相似度何时等价？⭐

**关键结论：当向量归一化后，点积 = 余弦相似度**

```
数学证明：

余弦相似度：
  cos(θ) = (A·B) / (|A| × |B|)

如果向量已归一化（长度 = 1）：
  |A| = 1，|B| = 1

那么：
  cos(θ) = (A·B) / (1 × 1) = A·B = 点积 ✅
```

**具体例子**：

```python
import numpy as np

A = np.array([3, 4])
B = np.array([4, 3])

# 未归一化：点积 ≠ 余弦
dot = np.dot(A, B)                                    # 24
cos = dot / (np.linalg.norm(A) * np.linalg.norm(B))   # 0.96
# 不相等！

# 归一化后：点积 = 余弦
A_norm = A / np.linalg.norm(A)  # [0.6, 0.8]
B_norm = B / np.linalg.norm(B)  # [0.8, 0.6]

dot_norm = np.dot(A_norm, B_norm)  # 0.96
# 等于余弦相似度！✅
```

**哪些场景会先归一化？**

```
┌─────────────────────────────┬──────────────┬─────────────────┐
│ 场景                        │ 是否归一化   │ 点积 = 余弦？   │
├─────────────────────────────┼──────────────┼─────────────────┤
│ 向量数据库（Faiss、Milvus） │ ✅ 通常会    │ ✅ 等价         │
│ 对比学习（SimCLR、CLIP）    │ ✅ 会        │ ✅ 等价         │
│ Sentence Transformers       │ ✅ 会        │ ✅ 等价         │
│ Attention 机制              │ ❌ 不会      │ ❌ 不等价       │
│ 原始词向量                  │ ❌ 不会      │ ❌ 不等价       │
└─────────────────────────────┴──────────────┴─────────────────┘
```

**向量数据库的实践技巧**：

```python
# Faiss 的做法：先归一化，再用点积
import faiss

# 1. 对向量做 L2 归一化
faiss.normalize_L2(vectors)

# 2. 使用内积（点积）索引
index = faiss.IndexFlatIP(d)  # IP = Inner Product
index.add(vectors)

# 3. 查询时也归一化
faiss.normalize_L2(query)
scores, indices = index.search(query, k)

# 结果：点积检索 = 余弦相似度检索，但更快！
```

**一句话总结**：

```
归一化 + 点积 = 快速的余弦相似度 ✅

想要余弦的效果，又想要点积的速度？
→ 先归一化，再用点积！
```

**与 LLM 的关系**：
- ✅ 词向量检索：用余弦相似度（或归一化+点积）
- ✅ Attention 机制：用点积（已有 LayerNorm，不需要显式归一化）
- ✅ 高维空间：余弦相似度依然有效

---

## 🎯 为什么 Attention 用点积？

### Attention 的核心公式回顾

```python
Attention(Q, K, V) = softmax(Q·K^T / √d_k) · V

关键步骤：
  Score = Q · K^T  ← 为什么用点积？
```

### 五大核心原因

#### 原因 1：计算效率极高 ⚡

```python
# 点积计算
Score = Q · K^T
时间复杂度：O(d)
空间复杂度：O(1)
参数量：0  ← 无需额外参数

# 对比：加法注意力（Bahdanau, 2014）
Score = W · tanh(W₁·Q + W₂·K)
时间复杂度：O(3d)
参数量：2d×d_h  ❌

# 对比：双线性注意力
Score = Q · W · K^T
时间复杂度：O(d²)
参数量：d²  ❌
```

**效率对比**：

| 方法 | 计算复杂度 | 参数量 | GPU 友好度 | 实际速度 |
|------|----------|--------|-----------|---------|
| **点积** | O(d) | 0 | ⭐⭐⭐⭐⭐ | 基准 1x |
| 余弦相似度 | O(2d) | 0 | ⭐⭐⭐⭐ | ~1.5x |
| 加法注意力 | O(3d) | 2d×d_h | ⭐⭐⭐ | ~30x ❌ |
| 双线性 | O(d²) | d² | ⭐⭐ | ~100x ❌ |

**A100 GPU 实测**（1000×1000 规模）：
```
点积注意力：   0.5 ms  ✅
加法注意力：   15 ms   ❌（慢 30 倍）
双线性注意力： 50 ms   ❌（慢 100 倍）
```

#### 原因 2：几何意义清晰 📐

```
点积 = 相似度测量

A · B = |A| × |B| × cos(θ)

在 Attention 中：
  Q: "我想找什么信息？"
  K: "我这里有什么信息？"
  
  Q·K 大 → 匹配度高 → 注意力权重高 ✅
  Q·K 小 → 匹配度低 → 注意力权重低 ✅
```

**直观例子**：

```
句子："猫在睡觉"

Query("猫") · Key("睡觉") = 0.8  ← 相关性强
  → softmax 后权重高
  → "猫"会关注"睡觉"

Query("猫") · Key("在") = 0.1  ← 相关性弱
  → softmax 后权重低
  → "猫"基本不关注"在"
```

#### 原因 3：完美并行化 🚀

```python
# 批量计算
Q = [32, 768]    # 32 个 Query
K = [100, 768]   # 100 个 Key

# 一步完成 3200 个分数的计算
scores = Q @ K.T  # [32, 100]

# GPU 可以完全并行
# 利用 SIMD（单指令多数据）
# 利用 Tensor Core（专用硬件）
```

**为什么点积可以高效并行？**

```
点积 = 逐元素相乘 + 求和

GPU 并行优势：
  1. 逐元素相乘：完全并行（无依赖）
  2. 求和：树状归约（log(n) 步）
  3. 矩阵乘法：硬件优化（Tensor Core）

对比：加法注意力
  tanh(W₁·Q + W₂·K)
  → 需要先加法，再激活函数
  → 有依赖关系，难以完全并行 ❌
```

#### 原因 4：梯度特性优良 📊

```python
# 点积的梯度
Score = Q · K

∂Score/∂Q = K  ← 简单！
∂Score/∂K = Q  ← 简单！

优点：
  ✅ 计算高效（一步求导）
  ✅ 数值稳定（不易梯度消失/爆炸）
  ✅ 反向传播快
```

**对比：复杂的注意力机制**

```python
# 加法注意力
Score = W · tanh(W₁·Q + W₂·K)

梯度链：
  ∂Score/∂Q = W · sech²(...) · W₁  ← 复杂
  ∂Score/∂K = W · sech²(...) · W₂  ← 复杂
  ∂Score/∂W = ...  ← 还要求 W 的梯度

问题：
  ❌ 计算慢（多次链式求导）
  ❌ 可能梯度消失（tanh 饱和）
  ❌ 数值不稳定
```

#### 原因 5：实践验证 🏆

```
历史发展：

2014: Bahdanau Attention（加法）
  论文：Neural Machine Translation by Jointly Learning to Align and Translate
  问题：慢，参数多

2015: Luong Attention（点积）
  论文：Effective Approaches to Attention-based Neural Machine Translation
  改进：更快，效果相当

2017: Scaled Dot-Product（Transformer）
  论文：Attention Is All You Need
  突破：快速、稳定、效果好 ✅
  
2017-现在：
  所有主流模型都用点积
  ├─ BERT（Google, 2018）
  ├─ GPT-2/3（OpenAI, 2019/2020）
  ├─ T5（Google, 2019）
  ├─ LLaMA（Meta, 2023）
  └─ DeepSeek-V3（2024）
  
  → 点积已成为事实标准
```

### 为什么点积这么完美？

```
数学美：
  ✅ 形式简洁（A·B）
  ✅ 意义明确（相似度）
  ✅ 计算高效（O(d)）

工程美：
  ✅ 无需额外参数
  ✅ GPU 硬件优化
  ✅ 数值稳定

实践美：
  ✅ 效果好
  ✅ 训练快
  ✅ 所有模型都在用

→ 点积是理论和实践的完美结合 ✨
```

### 为什么不用余弦相似度？⭐

**问题**：余弦相似度也能衡量相似度，为什么 Attention 不用它？

#### 原因 1：效率差距

```
点积：
  Q·K = 乘法 + 加法
  计算量：O(d)

余弦相似度：
  cos(θ) = (Q·K) / (|Q| × |K|)
         = 点积 + 两次求模 + 两次除法
  计算量：O(3d) + 额外运算
  
→ 余弦相似度慢约 2-3 倍
→ 对于 Attention 的海量计算（n² 次），差距很大
```

#### 原因 2：归一化已经在别处做了

```
现代 Transformer 架构：

  输入 → LayerNorm/RMSNorm → Q, K, V → Attention
              ↑
         已经归一化！

LayerNorm/RMSNorm 的效果：
  - 让向量的分量分布在相近的范围
  - Q 和 K 的"长度"已经被间接控制了
  
→ 点积的结果已经接近余弦相似度的效果
→ 不需要再显式除以长度
```

#### 原因 3：长度信息可能有用

```
余弦相似度：只看方向，完全忽略长度
点积：方向 + 长度都考虑

例子：
  Q₁ = [1, 0]      ← 普通 query
  Q₂ = [10, 0]     ← "强调"的 query（长度大）
  K  = [1, 0]
  
余弦相似度：
  cos(Q₁, K) = 1.0
  cos(Q₂, K) = 1.0  ← 完全一样
  
点积：
  Q₁·K = 1
  Q₂·K = 10  ← Q₂ 的注意力更强！
  
→ 点积允许模型通过调整向量长度来"加强"注意力
→ 这可能是有用的学习信号
```

#### 原因 4：Scaled 已经够用了

```
Attention 公式：
  Score = Q·K^T / √d_k
          ────────────
          Scaled Dot-Product

这个 √d_k 的作用：
  ✅ 控制方差（防止 softmax 饱和）
  ✅ 让数值范围稳定
  
但它不是归一化长度！只是除以一个常数（如 √768 ≈ 27.7）

实践证明：Scaled Dot-Product 就够了，不需要完整的余弦归一化
```

#### 总结对比

| 特性 | 点积 | 余弦相似度 |
|------|------|-----------|
| 计算速度 | ⭐⭐⭐⭐⭐ 快 | ⭐⭐⭐ 慢 |
| 保留长度信息 | ✅ 是 | ❌ 否 |
| 需要额外归一化 | ❌ 不需要 | ✅ 需要 |
| Transformer 使用 | ✅ 标准做法 | ❌ 不用 |

**一句话**：点积 + LayerNorm + Scaled = 够用了，不需要再算余弦相似度

### Q、K、V 向量里的值是什么？⭐

**问题**：向量 [0.1, 0.2, -0.3, ...] 这些数值是什么？是点积还是余弦相似度？

**答案**：都不是！这些是**学习到的特征表示**

#### 向量的来源

```python
# Attention 的输入
X = 词向量  # [seq_len, d_model]，如 [100, 768]

# Q、K、V 是通过线性变换得到的
Q = X @ W_Q  # W_Q 是可学习的权重矩阵 [768, 768]
K = X @ W_K  # W_K 是可学习的权重矩阵
V = X @ W_V  # W_V 是可学习的权重矩阵

# Q[i] = [0.1, 0.2, -0.3, ...] 这 768 个数
# 是 X[i] 经过 W_Q 线性变换后的结果
# 它们是"学习"出来的，不是点积或余弦相似度
```

#### 不同阶段的值的含义

```
┌─────────────────────────────────────────────────────────┐
│  阶段           │  值的类型        │  含义              │
├─────────────────────────────────────────────────────────┤
│  X（输入）      │  词向量          │  词的语义表示      │
│  Q = X @ W_Q    │  Query 向量      │  "我想找什么"      │
│  K = X @ W_K    │  Key 向量        │  "我有什么特征"    │
│  V = X @ W_V    │  Value 向量      │  "我的实际内容"    │
├─────────────────────────────────────────────────────────┤
│  Q @ K.T        │  点积 ⭐         │  相似度分数        │
│  softmax(...)   │  概率分布        │  注意力权重        │
│  weights @ V    │  加权和          │  输出表示          │
└─────────────────────────────────────────────────────────┘
```

#### 具体例子

```python
# 假设处理句子 "猫 在 睡觉"

X = [[0.8, 0.2, ...],   # "猫" 的词向量
     [0.1, 0.3, ...],   # "在" 的词向量
     [0.5, 0.7, ...]]   # "睡觉" 的词向量

# 经过线性变换
Q = X @ W_Q  # 每个词的 Query 向量
K = X @ W_K  # 每个词的 Key 向量

# Q[0] = [0.1, 0.2, -0.3, ...]  ← 这是"猫"的 Query
# 这 768 个数是学习出来的特征，不是点积！

# 点积在这里发生：
scores = Q @ K.T  # [3, 3] 矩阵
# scores[0, 2] = Q[0] · K[2] = "猫"对"睡觉"的注意力分数
# 这个 scores[0, 2] 才是点积的结果！
```

#### 一图总结

```
输入词向量 X                    Q、K、V 向量              相似度分数
[学习的表示]                    [学习的表示]              [点积结果]
     │                              │                        │
     │    W_Q, W_K, W_V            │     Q @ K.T            │
     ├────────────────────→        ├──────────────────→     │
     │    线性变换                  │     点积计算            │
     ↓                              ↓                        ↓
[0.8, 0.2, ...]  ──→  Q=[0.1, 0.2, ...]  ──→  score=2.5
                      K=[0.3, 0.4, ...]
                      
向量里的值：特征表示          点积的结果：相似度分数
（不是点积！）               （这才是点积！）
```

**与 LLM 的关系**：
- ✅ Attention(Q,K,V) 的核心：Q·K^T
- ✅ 这是 Transformer 成功的关键之一
- ✅ 所有现代 LLM 都基于点积 Attention

---

## 🔗 与 LLM 的关系

### 1. Attention 中的点积

```
Attention 计算的每一步都用到点积：

Step 1: 计算相似度
  Q = [batch, seq_q, d]
  K = [batch, seq_k, d]
  
  scores = Q @ K.T  ← 点积
  → [batch, seq_q, seq_k]
  
  含义：每个 Query 与每个 Key 的相似度
```

### 1.1 为什么 K 需要转置？⭐

**核心原因：让矩阵维度匹配，实现批量点积计算**

```
目标：计算每个 Query 和每个 Key 的点积
结果：(n, n) 的相似度矩阵（n 个 query × n 个 key）

Q 的形状：(n, d)  ← n 个 query，每个 d 维
K 的形状：(n, d)  ← n 个 key，每个 d 维
```

**不转置会怎样？**

```
Q × K = (n, d) × (n, d) = ❌ 无法相乘！
        ↑         ↑
       列数 d ≠ 行数 n
```

**转置后**

```
Q × K^T = (n, d) × (d, n) = (n, n) ✅
          ↑         ↑
         列数 d = 行数 d   匹配！
```

**矩阵乘法的本质：行×列 = 点积**

```
        K^T (转置后)
        ┌─────────────┐
        │ k₁ k₂ k₃ k₄ │  ← K 的每一行变成了列
        │ ↓  ↓  ↓  ↓  │
        └─────────────┘
           (d, n)

Q       ┌───┐
┌─────┐ │   │
│ q₁→ │ │ s₁₁ s₁₂ s₁₃ s₁₄ │  q₁ 和所有 k 的点积
│ q₂→ │ │ s₂₁ s₂₂ s₂₃ s₂₄ │  q₂ 和所有 k 的点积
│ q₃→ │ │ s₃₁ s₃₂ s₃₃ s₃₄ │  q₃ 和所有 k 的点积
└─────┘ └─────────────────┘
(n, d)        (n, n)
              相似度矩阵

sᵢⱼ = qᵢ · kⱼ = 第 i 个 query 对第 j 个 key 的注意力分数
```

**具体例子**

```python
# 假设 2 个词，每个词用 3 维向量表示
Q = [[1, 0, 1],    # query: "猫"
     [0, 1, 0]]    # query: "狗"

K = [[1, 0, 1],    # key: "猫" 
     [0, 1, 0]]    # key: "狗"

# K 转置
K.T = [[1, 0],
       [0, 1],
       [1, 0]]

# Q × K.T
Q @ K.T = [[1×1+0×0+1×1, 1×0+0×1+1×0],   [[2, 0],
           [0×1+1×0+0×1, 0×0+1×1+0×0]] =   [0, 1]]
                                           
# 结果解读：
# (0,0)=2: "猫"问"猫" → 很相似！
# (0,1)=0: "猫"问"狗" → 不相似
# (1,0)=0: "狗"问"猫" → 不相似  
# (1,1)=1: "狗"问"狗" → 相似
```

**一句话总结**

```
转置 = 把"行向量×行向量"的点积
       转化为"行向量×列向量"的矩阵乘法
     = 让 GPU 能高效并行计算所有 query-key 对的相似度 ✅
```

```

Step 2: 归一化
  scores = scores / √d_k  ← 为什么？见下文

Step 3: Softmax
  attention_weights = softmax(scores)
  
Step 4: 加权求和
  output = attention_weights @ V  ← 又是矩阵乘法（点积）
```

### 2. 为什么要除以 √d_k？⭐

**问题**：点积的方差随维度增大

```
假设 Q, K 的元素独立同分布，均值0，方差1

那么：
  Q·K = Σᵢ Qᵢ×Kᵢ
  
  Var(Q·K) = Σᵢ Var(Qᵢ×Kᵢ)
           = d_k × 1
           = d_k  ← 方差正比于维度

如果 d_k = 768：
  Q·K 的标准差 = √768 ≈ 27.7
  → 数值可能很大（如 ±100）
  → softmax(±100) 会饱和 ❌
  → 梯度接近 0 ❌
```

**解决方案**：除以 √d_k

```
Scaled Score = (Q·K) / √d_k

Var(Scaled Score) = Var(Q·K) / d_k
                  = d_k / d_k
                  = 1  ✅ 归一化！

效果：
  ✅ 无论 d_k 多大，方差保持 1
  ✅ softmax 不会饱和
  ✅ 梯度健康
  ✅ 训练稳定
```

**可视化**：

```
未缩放（d_k=768）：
  scores = [-50, -30, -10, 0, 10, 30, 50]
  softmax([...]) ≈ [0, 0, 0, 0, 0, 0, 1]  ❌ 饱和

缩放后（除以 √768 ≈ 27.7）：
  scaled = [-1.8, -1.1, -0.4, 0, 0.4, 1.1, 1.8]
  softmax([...]) ≈ [0.01, 0.02, 0.05, 0.10, 0.15, 0.25, 0.42]  ✅ 分布合理
```

### 3. 点积在 LLM 其他地方的应用

```
1. 词向量检索
   query_vec · doc_vec  ← 相似度匹配

2. 分类任务
   hidden_state · class_weight  ← 分类得分

3. 神经网络线性层
   output = W · x + b
   → W·x 就是点积

4. 余弦相似度计算
   similarity = (A·B) / (|A|×|B|)
```

### 4. 点积的计算优化

```
现代硬件对点积的优化：

1. SIMD 指令（CPU）
   一次指令处理多个数据
   → 加速向量运算

2. Tensor Core（GPU）
   专用矩阵乘法硬件
   → 点积速度极快

3. Flash Attention（算法）
   分块计算 + 重计算
   → 降低显存占用
   → 提升 2-4x 速度

→ 点积是深度学习硬件优化的重点
```

---

## 🎯 核心要点总结

### 1. 点积的本质

```
✅ 代数定义：A·B = Σ aᵢ×bᵢ（计算方法）
✅ 几何定义：A·B = |A|×|B|×cos(θ)（意义）
✅ 核心意义：测量方向一致性
✅ 高维空间：点积仍然有效
```

### 2. 点积的几何意义

```
✅ 点积 = 投影长度 × 被投影向量长度
✅ 点积 > 0 → 方向一致 → 相似
✅ 点积 = 0 → 垂直 → 无关
✅ 点积 < 0 → 方向相反 → 相反
```

### 3. 余弦相似度

```
✅ 公式：cos(θ) = (A·B) / (|A|×|B|)
✅ 本质：归一化的点积
✅ 范围：[-1, 1]
✅ 优势：只看方向，忽略长度
✅ 应用：NLP 中的语义相似度
```

### 4. Attention 为什么用点积

```
✅ 计算效率：O(d)，无额外参数
✅ 几何意义：测量 Q 和 K 的相似度
✅ 并行化：GPU 完全并行
✅ 梯度优良：简单、稳定
✅ 实践验证：所有模型都在用
```

### 5. Scaled Dot-Product

```
✅ 问题：点积方差随维度增大
✅ 解决：除以 √d_k 归一化
✅ 效果：方差保持 1，训练稳定
✅ 应用：所有 Transformer 的标准做法
```

---

## 📚 推荐阅读

**核心论文**：
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformer, 2017)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (Bahdanau Attention, 2014)
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) (Luong Attention, 2015)

**相关文档**：
- ⬆️ [L0: LLM 模型本质](./L0-llm-model-essence.md) - 理解 Attention 在模型中的作用
- ⬇️ [L-1: 数学基础](./L-1-math-foundations.md) - 向量、余弦函数、线性代数
- ⬆️ [L-3: 表示学习基础](./L-3-representation-learning.md) - 词向量的相似度

---

**记住**：点积是连接线性代数和语义理解的桥梁。

理解点积，就理解了 Attention 的核心。✨

---

*最后更新：2026年1月*
