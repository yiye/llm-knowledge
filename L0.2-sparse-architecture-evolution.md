# L0.2 ç¨€ç–æ¶æ„æ¼”è¿›ï¼šä» Transformer åˆ° MoE å†åˆ° Engram

> **ä¸»é¢˜**ï¼šå¤§æ¨¡å‹ç¨€ç–åŒ–æ¶æ„çš„æ¼”è¿›å†ç¨‹  
> **æ ¸å¿ƒ**ï¼šç†è§£ MoEï¼ˆæ¡ä»¶è®¡ç®—ï¼‰å’Œ Engramï¼ˆæ¡ä»¶è®°å¿†ï¼‰å¦‚ä½•æ”¹è¿› Transformer  
> **æ—¶é—´çº¿**ï¼š2017 Transformer â†’ 2022 MoE ä¸»æµåŒ– â†’ 2026 Engram æå‡º  
> **è´¯ç©¿æ¡ˆä¾‹**ï¼šç”¨åŒä¸€ä¸ªé—®é¢˜ **"What is the capital of France?"** çœ‹ä¸‰ä»£æ¶æ„çš„å¤„ç†å·®å¼‚

---

## ğŸ¯ å¼€ç¯‡æ¡ˆä¾‹ï¼šä¸€ä¸ªç®€å•çš„é—®é¢˜

å‡è®¾æˆ‘ä»¬é—®æ¨¡å‹ï¼š

```
è¾“å…¥ï¼š"What is the capital of France?"
æœŸæœ›è¾“å‡ºï¼š"Paris"
```

è¿™ä¸ªç®€å•çš„é—®é¢˜ï¼Œåœ¨ä¸åŒæ¶æ„çš„æ¨¡å‹ä¸­ï¼Œå¤„ç†æ–¹å¼å®Œå…¨ä¸åŒï¼š

| æ¶æ„ | å¤„ç†æ–¹å¼ | æ•ˆç‡ | å¹´ä»½ |
|------|---------|------|------|
| **Dense Transformer** | æ¯å±‚ç”¨å…¨éƒ¨å‚æ•°é€æ­¥"å›å¿†" | ä½ | 2017 |
| **MoE Transformer** | è·¯ç”±åˆ°ä¸“å®¶ï¼Œä¸“å®¶"å›å¿†" | ä¸­ | 2022 |
| **MoE + Engram** | ç›´æ¥æŸ¥è¡¨è·å¾—ï¼Œä¸“å®¶åšæ¨ç† | é«˜ | 2026 |

è®©æˆ‘ä»¬æ·±å…¥çœ‹çœ‹æ¯ç§æ¶æ„æ˜¯å¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜çš„...

---

## ğŸ“‹ ç›®å½•

- [ä¸€ã€ä¼ ç»Ÿ Transformerï¼šå¯†é›†è®¡ç®—çš„å›°å¢ƒ](#ä¸€ä¼ ç»Ÿ-transformerå¯†é›†è®¡ç®—çš„å›°å¢ƒ)
- [äºŒã€MoEï¼šæ¡ä»¶è®¡ç®—çš„å´›èµ·](#äºŒmoeæ¡ä»¶è®¡ç®—çš„å´›èµ·)
- [ä¸‰ã€Engramï¼šæ¡ä»¶è®°å¿†çš„çªç ´](#ä¸‰engramæ¡ä»¶è®°å¿†çš„çªç ´)
- [å››ã€æ¶æ„å¯¹æ¯”ä¸ååŒ](#å››æ¶æ„å¯¹æ¯”ä¸ååŒ)
- [äº”ã€æ€§èƒ½æå‡ä¸æœªæ¥å±•æœ›](#äº”æ€§èƒ½æå‡ä¸æœªæ¥å±•æœ›)

---

## ä¸€ã€ä¼ ç»Ÿ Transformerï¼šå¯†é›†è®¡ç®—çš„å›°å¢ƒ

### 1.1 æ¡ˆä¾‹ï¼šDense Transformer å¦‚ä½•å›ç­” "capital of France"

è®©æˆ‘ä»¬å…ˆçœ‹ä¼ ç»Ÿ Transformer å¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜ï¼š

```
è¾“å…¥ï¼š"What is the capital of France?"
Token åŒ–ï¼š[What] [is] [the] [capital] [of] [France] [?]

å¤„ç†æµç¨‹ï¼š
  Layer 1:  æ‰€æœ‰ token çš„ embedding ç»è¿‡æ³¨æ„åŠ›å’Œ FFN
            â†’ å¼€å§‹ç†è§£å¥å­ç»“æ„
  
  Layer 2:  ç»§ç»­ç”¨å…¨éƒ¨å‚æ•°å¤„ç†
            â†’ è¯†åˆ«è¿™æ˜¯ä¸€ä¸ªé—®å¥
  
  Layer 3-5: æŒç»­ç”¨å…¨éƒ¨å‚æ•°è®¡ç®—
            â†’ FFN å¼€å§‹ä»æƒé‡ä¸­"å›å¿†"France ç›¸å…³ä¿¡æ¯
  
  Layer 6-8: è¿˜åœ¨ç”¨å…¨éƒ¨å‚æ•°
            â†’ é€æ¸æ¿€æ´»ä¸"é¦–éƒ½"ç›¸å…³çš„ç¥ç»å…ƒ
  
  Layer 9-10: ç»§ç»­å…¨å‚æ•°è®¡ç®—
             â†’ ç»ˆäº"æƒ³èµ·"Paris æ˜¯ France çš„é¦–éƒ½
  
  Layer 11-12: å…¨å‚æ•°å¤„ç†è¾“å‡º
              â†’ ç»„ç»‡ç­”æ¡ˆï¼Œç”Ÿæˆ "Paris"

é—®é¢˜ï¼š
  âŒ æ¯å±‚éƒ½ç”¨å…¨éƒ¨å‚æ•°ï¼ˆå³ä½¿å¾ˆå¤šå‚æ•°ä¸è¿™ä¸ªé—®é¢˜æ— å…³ï¼‰
  âŒ éœ€è¦ 10+ å±‚æ‰èƒ½"å›å¿†"å‡ºä¸€ä¸ªç®€å•äº‹å®
  âŒ çŸ¥è¯†éšå¼å­˜å‚¨åœ¨ FFN æƒé‡ä¸­ï¼Œè®¿é—®æ•ˆç‡ä½
```

### 1.2 Transformer æ¶æ„å›é¡¾ï¼ˆ2017ï¼‰

**æ¥æº**ï¼š[Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., NeurIPS 2017)

#### æ ‡å‡†æ¶æ„

```
è¾“å…¥ Token IDs
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding                 â”‚  â† 3% å‚æ•°
â”‚ [vocab_size Ã— d_model]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer Ã— N           â”‚
â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Multi-Head Attention        â”‚ â”‚  â† 15% å‚æ•°
â”‚ â”‚ - Q, K, V çº¿æ€§æŠ•å½±          â”‚ â”‚
â”‚ â”‚ - æ³¨æ„åŠ›è®¡ç®—                â”‚ â”‚
â”‚ â”‚ - è¾“å‡ºæŠ•å½±                  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â†“                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Feed-Forward Network (FFN)  â”‚ â”‚  â† 65% å‚æ•° ğŸ”¥
â”‚ â”‚ - Linear 1: d â†’ 4d          â”‚ â”‚
â”‚ â”‚ - Activation (GELU/ReLU)    â”‚ â”‚
â”‚ â”‚ - Linear 2: 4d â†’ d          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer (LM Head)          â”‚  â† 3% å‚æ•°
â”‚ [d_model Ã— vocab_size]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º Logits
```

#### å‚æ•°åˆ†å¸ƒ

| ç»„ä»¶ | å‚æ•°å æ¯” | ä½œç”¨ | ç‰¹æ€§ |
|------|---------|------|------|
| Embedding | ~3% | è¯çš„åŸºç¡€è¯­ä¹‰ | é™æ€æŸ¥è¡¨ |
| Attention | ~15% | ä¸Šä¸‹æ–‡å…³ç³»å»ºæ¨¡ | åŠ¨æ€è®¡ç®— |
| **FFN** | **~65%** | **çŸ¥è¯†å­˜å‚¨ + æ¨ç†** | **å¯†é›†è®¡ç®—** ğŸ”¥ |
| Output | ~3% | è¯è¡¨æ˜ å°„ | é™æ€æŸ¥è¡¨ |

### 1.3 æ ¸å¿ƒé—®é¢˜ï¼šå¯†é›†è®¡ç®—çš„å›°å¢ƒ

#### é—®é¢˜ 1ï¼šæ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®—ï¼ˆå³ä½¿ä¸ç›¸å…³ï¼‰

å›åˆ°æˆ‘ä»¬çš„ä¾‹å­ï¼š

```
é—®é¢˜ï¼š"What is the capital of France?"

Dense FFN çš„å¤„ç†ï¼š
  
  Token "capital":
    âœ“ æ¿€æ´»"é¦–éƒ½"ç›¸å…³ç¥ç»å…ƒ (ç›¸å…³)
    âœ— ä¹Ÿæ¿€æ´»"é‡‘èèµ„æœ¬"ç›¸å…³ç¥ç»å…ƒ (ä¸ç›¸å…³)
    âœ— ä¹Ÿæ¿€æ´»"å¤§å†™å­—æ¯"ç›¸å…³ç¥ç»å…ƒ (ä¸ç›¸å…³)
    âœ— è¿˜æ¿€æ´»æ•°å­¦ã€ä»£ç ç­‰æ— å…³é¢†åŸŸçš„ç¥ç»å…ƒ
    
    â†’ 134M å‚æ•°å…¨éƒ¨å‚ä¸è®¡ç®—ï¼
  
  Token "France":
    âœ“ æ¿€æ´»æ³•å›½ç›¸å…³ä¿¡æ¯ (ç›¸å…³)
    âœ— ä¹Ÿæ¿€æ´»æ‰€æœ‰å…¶ä»–å›½å®¶çš„ä¿¡æ¯ (ä¸ç›¸å…³)
    âœ— è¿˜æ¿€æ´»æ— å…³é¢†åŸŸçš„çŸ¥è¯†
    
    â†’ åˆæ˜¯ 134M å‚æ•°å…¨éƒ¨è®¡ç®—ï¼
```

```python
# å¯†é›† FFN å±‚
class DenseFFN(nn.Module):
    def __init__(self, d_model=4096, d_ff=16384):
        self.w1 = nn.Linear(d_model, d_ff)     # 4096 Ã— 16384 å‚æ•°
        self.w2 = nn.Linear(d_ff, d_model)     # 16384 Ã— 4096 å‚æ•°
        # æ€»å‚æ•°ï¼š2 Ã— 4096 Ã— 16384 â‰ˆ 134M
    
    def forward(self, x):
        # æ¯ä¸ª token éƒ½è¦ç”¨å…¨éƒ¨ 134M å‚æ•°è®¡ç®—ï¼
        # "capital" â†’ ç”¨ 134M å‚æ•°
        # "of" â†’ ç”¨ 134M å‚æ•°
        # "France" â†’ ç”¨ 134M å‚æ•°
        # ...æ‰€æœ‰ token éƒ½ä¸€æ ·ï¼
        return self.w2(F.gelu(self.w1(x)))

é—®é¢˜ï¼š
  âŒ æ— æ³•é€‰æ‹©æ€§æ¿€æ´»ç›¸å…³å‚æ•°
  âŒ å¤§é‡è®¡ç®—æµªè´¹åœ¨ä¸ç›¸å…³çš„å‚æ•°ä¸Š
  âŒ è®¡ç®—æˆæœ¬éšæ¨¡å‹è§„æ¨¡çº¿æ€§å¢é•¿
```

#### é—®é¢˜ 2ï¼šæ‰©å±•å›°å¢ƒï¼ˆScaling Dilemmaï¼‰

```
ç›®æ ‡ï¼šæå‡æ¨¡å‹èƒ½åŠ›
æ‰‹æ®µï¼šå¢åŠ å‚æ•°é‡

ä¼ ç»Ÿ Dense æ¨¡å‹çš„å›°å¢ƒï¼š
  
  æ¨¡å‹ A: 7B å‚æ•°
    è®­ç»ƒæˆæœ¬ï¼šX
    æ¨ç†æˆæœ¬ï¼šY
  
  æ¨¡å‹ B: 70B å‚æ•°ï¼ˆ10å€ï¼‰
    è®­ç»ƒæˆæœ¬ï¼š10X  â† çº¿æ€§å¢é•¿
    æ¨ç†æˆæœ¬ï¼š10Y  â† çº¿æ€§å¢é•¿
    
  æ¨¡å‹ C: 700B å‚æ•°ï¼ˆ100å€ï¼‰
    è®­ç»ƒæˆæœ¬ï¼š100X  â† ä¸å¯æ‰¿å— ğŸ’€
    æ¨ç†æˆæœ¬ï¼š100Y  â† ä¸å¯æ‰¿å— ğŸ’€

ç»“è®ºï¼š
  å¯†é›†æ¨¡å‹æ— æ³•æŒç»­æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°è§„æ¨¡
```

#### é—®é¢˜ 3ï¼šçŸ¥è¯†å­˜å‚¨ä½æ•ˆï¼ˆå›å¿†æ…¢ï¼‰

æ ¹æ®ç ”ç©¶ï¼ˆ[Geva et al., 2021](https://arxiv.org/abs/2012.14913) - Transformer Feed-Forward Layers Are Key-Value Memoriesï¼‰ï¼š

> FFN å±‚çš„æƒé‡çŸ©é˜µéšå¼å­˜å‚¨äº†äº‹å®çŸ¥è¯†ï¼Œä½†è¿™ç§å­˜å‚¨æ–¹å¼æ˜¯**åˆ†å¸ƒå¼ä¸”ä½æ•ˆ**çš„ã€‚

**å›åˆ°æˆ‘ä»¬çš„ä¾‹å­**ï¼š

```
äº‹å®ï¼š"Paris is the capital of France"

Dense FFN å¦‚ä½•å­˜å‚¨è¿™ä¸ªçŸ¥è¯†ï¼Ÿ

  çŸ¥è¯†ç¼–ç åœ¨æƒé‡çŸ©é˜µä¸­ï¼š
    W1[2048, 5] = 0.23   â† å¯èƒ½ä¸ "capital" ç›¸å…³
    W1[1024, 17] = -0.45 â† å¯èƒ½ä¸ "France" ç›¸å…³
    W2[5, 3072] = 0.78   â† å¯èƒ½ä¸ "Paris" ç›¸å…³
    ...
    æ•°åƒä¸ªæƒé‡å…±åŒç¼–ç è¿™ä¸€ä¸ªäº‹å®ï¼
  
  è®¿é—®è¿™ä¸ªçŸ¥è¯†éœ€è¦ï¼š
    1. è¾“å…¥ "capital of France" çš„å‘é‡
    2. å®Œæ•´çš„çŸ©é˜µä¹˜æ³•ï¼šx @ W1 @ W2
    3. æ¿€æ´»å‡½æ•°è®¡ç®—
    4. é€šè¿‡æ¿€æ´»æ¨¡å¼"å›å¿†"å‡º Paris
    â†’ æ—¶é—´å¤æ‚åº¦ï¼šO(d Ã— d_ff) â‰ˆ O(4096 Ã— 16384) = 67M æ¬¡ä¹˜æ³•ï¼

å¯¹æ¯”äººç±»ï¼š
  é—®ï¼š"æ³•å›½é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
  ç­”ï¼š"å·´é»"ï¼ˆå‡ ä¹ç¬é—´ï¼Œç›´æ¥ä»è®°å¿†æå–ï¼‰
  
  æ¨¡å‹ï¼šéœ€è¦ 67M æ¬¡è®¡ç®—æ‰èƒ½"å›å¿†"å‡ºæ¥ï¼
```

**é—®é¢˜æ€»ç»“**ï¼š

```
Dense Transformer å¤„ç† "capital of France"ï¼š

âœ— æ•ˆç‡ä½ä¸‹ï¼š
  - æ¯ä¸ª token ç”¨å…¨éƒ¨å‚æ•°ï¼ˆæ— é€‰æ‹©ï¼‰
  - éœ€è¦å¤šå±‚å‰å‘ä¼ æ’­"å›å¿†"äº‹å®
  - 67M æ¬¡è®¡ç®—æ‰èƒ½è®¿é—®ä¸€ä¸ªçŸ¥è¯†ç‚¹

âœ— æ— æ³•æ‰©å±•ï¼š
  - æ¨¡å‹è¶Šå¤§ï¼Œæ¯æ¬¡è®¡ç®—æˆæœ¬è¶Šé«˜
  - 10B â†’ 100B â†’ 1T å‚æ•°éƒ½è¦å…¨éƒ¨æ¿€æ´»
  - æˆæœ¬çº¿æ€§å¢é•¿ï¼Œä¸å¯æŒç»­

âœ— çŸ¥è¯†éšå¼ï¼š
  - åˆ†æ•£åœ¨æ•°ç™¾ä¸‡æƒé‡ä¸­
  - æ— æ³•ç›´æ¥"æŸ¥æ‰¾"
  - éš¾ä»¥æ›´æ–°å’Œè§£é‡Š
```

---

## äºŒã€MoEï¼šæ¡ä»¶è®¡ç®—çš„å´›èµ·

### 2.1 æ¡ˆä¾‹ï¼šMoE å¦‚ä½•æ”¹è¿› "capital of France" çš„å¤„ç†

MoEï¼ˆæ··åˆä¸“å®¶ï¼‰çš„æ ¸å¿ƒæ€æƒ³ï¼š**ä¸ç”¨æ‰€æœ‰å‚æ•°ï¼Œåªæ¿€æ´»ç›¸å…³çš„ä¸“å®¶**ã€‚

```
è¾“å…¥ï¼š"What is the capital of France?"
Token åŒ–ï¼š[What] [is] [the] [capital] [of] [France] [?]

MoE çš„å¤„ç†æµç¨‹ï¼š

Layer 1 (MoE):
  Token "capital":
    è·¯ç”±å™¨åˆ¤æ–­ â†’ é€‰æ‹©ã€åœ°ç†ä¸“å®¶ã€‘å’Œã€æ”¿æ²»ä¸“å®¶ã€‘
    åªç”¨è¿™ 2 ä¸ªä¸“å®¶è®¡ç®—ï¼ˆè€Œéå…¨éƒ¨ 64 ä¸ªï¼‰
    æ¿€æ´»å‚æ•°ï¼š2/64 = 3.125%
  
  Token "France":
    è·¯ç”±å™¨åˆ¤æ–­ â†’ é€‰æ‹©ã€æ¬§æ´²åœ°ç†ä¸“å®¶ã€‘å’Œã€æ³•å›½æ–‡åŒ–ä¸“å®¶ã€‘
    åªç”¨è¿™ 2 ä¸ªä¸“å®¶è®¡ç®—
    æ¿€æ´»å‚æ•°ï¼š3.125%
  
Layer 2-5: ç±»ä¼¼çš„ç¨€ç–æ¿€æ´»
  â†’ é€æ­¥"å›å¿†"å‡º France-Paris çš„å…³è”
  
Layer 6-8: ç»§ç»­ç¨€ç–è®¡ç®—
  â†’ ç¡®å®šç­”æ¡ˆæ˜¯ "Paris"
  
Layer 9-12: ç»„ç»‡è¾“å‡º
  â†’ ç”Ÿæˆ "Paris"

æ”¹è¿›ï¼š
  âœ… æ¯ä¸ª token åªæ¿€æ´»ç›¸å…³ä¸“å®¶ï¼ˆ3%ï¼‰
  âœ… ä¸“å®¶è‡ªåŠ¨ä¸“ä¸šåŒ–ï¼ˆåœ°ç†ã€å†å²ã€æ•°å­¦ç­‰ï¼‰
  âœ… æ€»å‚æ•°å¯ä»¥å¾ˆå¤§ï¼Œä½†æ¿€æ´»å‚æ•°å¯æ§
  
ä»å­˜åœ¨ï¼š
  âŒ ä»éœ€å¤šå±‚å‰å‘ä¼ æ’­"å›å¿†"äº‹å®
  âŒ çŸ¥è¯†ä»éšå¼å­˜å‚¨åœ¨ä¸“å®¶æƒé‡ä¸­
  âŒ æµ…å±‚ä»åœ¨èŠ±ç®—åŠ›"æƒ³èµ·"Paris
```

**å…³é”®çªç ´**ï¼šä»"å…¨éƒ¨è®¡ç®—"åˆ°"é€‰æ‹©æ€§è®¡ç®—"

```
Dense: 
  æ‰€æœ‰ token â†’ ä½¿ç”¨å…¨éƒ¨å‚æ•° â†’ è¾“å‡º
  
MoE:
  æ¯ä¸ª token â†’ è·¯ç”±å™¨é€‰æ‹© â†’ åªç”¨ç›¸å…³ä¸“å®¶ â†’ è¾“å‡º
  
æ•ˆç‡æå‡ï¼š
  64 ä¸ªä¸“å®¶ï¼Œåªæ¿€æ´» 2 ä¸ª
  â†’ ç”¨ 3% çš„è®¡ç®—è·å¾— 32 å€çš„å®¹é‡ï¼âœ¨
```

### 2.2 MoE çš„æ¼”è¿›å†ç¨‹

#### æ—©æœŸæ¢ç´¢ï¼ˆ2017-2021ï¼‰

**é‡Œç¨‹ç¢‘è®ºæ–‡**ï¼š

1. **Outrageously Large Neural Networks (2017)**
   - ä½œè€…ï¼šShazeer et al., Google Brain
   - è®ºæ–‡ï¼š[The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
   - è´¡çŒ®ï¼šé¦–æ¬¡åœ¨ LSTM ä¸Šå®ç°å¤§è§„æ¨¡ MoE

2. **GShard (2020)**
   - ä½œè€…ï¼šGoogle
   - è®ºæ–‡ï¼š[GShard: Scaling Giant Models with Conditional Computation](https://arxiv.org/abs/2006.16668)
   - è´¡çŒ®ï¼š600B å‚æ•°ç¿»è¯‘æ¨¡å‹ï¼Œè¯æ˜ MoE å¯æ‰©å±•æ€§

3. **Switch Transformer (2021)**
   - ä½œè€…ï¼šFedus et al., Google
   - è®ºæ–‡ï¼š[Switch Transformers: Scaling to Trillion Parameter Models](https://arxiv.org/abs/2101.03961)
   - è´¡çŒ®ï¼š1.6T å‚æ•°æ¨¡å‹ï¼Œç®€åŒ–ä¸º top-1 è·¯ç”±

#### ä¸»æµåŒ–é˜¶æ®µï¼ˆ2022-2025ï¼‰

**ä»£è¡¨æ¨¡å‹**ï¼š

| æ¨¡å‹ | æ—¶é—´ | è§„æ¨¡ | ç‰¹ç‚¹ |
|------|------|------|------|
| GLaM | 2022 | 1.2T (64E Ã— æ¯ä¸ª13B) | Googleï¼Œé¦–ä¸ªè¶…å¤§ MoE |
| Mixtral 8Ã—7B | 2023 | 47B æ€»å‚æ•°ï¼Œ13B æ¿€æ´» | Mistral AIï¼Œå¼€æº |
| DeepSeek-V2 | 2024 | 236B æ€»å‚æ•°ï¼Œ21B æ¿€æ´» | å¼•å…¥ MLA æ³¨æ„åŠ› |
| **DeepSeek-V3** | **2024** | **671B æ€»å‚æ•°ï¼Œ37B æ¿€æ´»** | **MoE + å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›** |

### 2.2 MoE æ ¸å¿ƒåŸç†

#### æ¶æ„è®¾è®¡

```python
class MoELayer(nn.Module):
    """æ··åˆä¸“å®¶å±‚ï¼šæ›¿æ¢ä¼ ç»Ÿ FFN"""
    
    def __init__(self, d_model=4096, num_experts=64, expert_capacity=4096):
        super().__init__()
        
        # 1. è·¯ç”±ç½‘ç»œï¼ˆRouter/Gateï¼‰
        self.router = nn.Linear(d_model, num_experts)
        
        # 2. ä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff=16384)  # æ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ª FFN
            for _ in range(num_experts)
        ])
        
        self.num_experts = num_experts
        self.top_k = 2  # æ¯æ¬¡æ¿€æ´» 2 ä¸ªä¸“å®¶
    
    def forward(self, x):
        """
        x: [batch, seq_len, d_model]
        """
        batch_size, seq_len, d_model = x.shape
        
        # 1. è·¯ç”±å†³ç­–ï¼šé€‰æ‹©å“ªäº›ä¸“å®¶
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        
        # 2. Top-K é€‰æ‹©
        routing_weights, selected_experts = torch.topk(
            router_logits, 
            self.top_k, 
            dim=-1
        )
        routing_weights = F.softmax(routing_weights, dim=-1)
        
        # 3. ä¸“å®¶è®¡ç®—ï¼ˆåªè®¡ç®—è¢«é€‰ä¸­çš„ï¼‰
        final_output = torch.zeros_like(x)
        
        for i in range(self.top_k):
            expert_idx = selected_experts[:, :, i]  # [batch, seq_len]
            expert_weight = routing_weights[:, :, i]  # [batch, seq_len]
            
            # æ‰¹å¤„ç†ï¼šå°†ç›¸åŒä¸“å®¶çš„ token èšåˆ
            for expert_id in range(self.num_experts):
                # æ‰¾åˆ°è·¯ç”±åˆ°è¯¥ä¸“å®¶çš„ token
                mask = (expert_idx == expert_id)
                
                if mask.any():
                    # æå–è¿™äº› token
                    expert_input = x[mask]
                    
                    # ä¸“å®¶è®¡ç®—
                    expert_output = self.experts[expert_id](expert_input)
                    
                    # åŠ æƒç´¯åŠ åˆ°è¾“å‡º
                    final_output[mask] += expert_weight[mask].unsqueeze(-1) * expert_output
        
        return final_output


class Expert(nn.Module):
    """å•ä¸ªä¸“å®¶ï¼šæ ‡å‡† FFN"""
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        return self.w2(F.gelu(self.w1(x)))
```

#### å…³é”®æœºåˆ¶ï¼šç¨€ç–æ¿€æ´»

```
å¯¹æ¯”ï¼šDense vs MoE

Dense FFNï¼š
  è¾“å…¥ token â†’ ä½¿ç”¨å…¨éƒ¨å‚æ•° â†’ è¾“å‡º
  æ¿€æ´»å‚æ•°ï¼š100%

MoE (64 ä¸ªä¸“å®¶ï¼Œtop-2)ï¼š
  è¾“å…¥ token â†’ è·¯ç”± â†’ é€‰æ‹© 2 ä¸ªä¸“å®¶ â†’ è¾“å‡º
  æ¿€æ´»å‚æ•°ï¼š2/64 = 3.125%
  
å‚æ•°æ•ˆç‡ï¼š
  æ€»å‚æ•°ï¼š64 ä¸ªä¸“å®¶ Ã— å•ä¸ªä¸“å®¶å¤§å°
  æ¿€æ´»å‚æ•°ï¼š2 ä¸ªä¸“å®¶ Ã— å•ä¸ªä¸“å®¶å¤§å°
  
  â†’ ç”¨ 3% çš„è®¡ç®—æˆæœ¬è·å¾— 32 å€çš„å®¹é‡ï¼âœ¨
```

### 2.3 MoE çš„ä¼˜åŠ¿ä¸å±€é™

#### âœ… ä¼˜åŠ¿

1. **å‚æ•°-è®¡ç®—è§£è€¦**
```
ä¼ ç»Ÿæ¨¡å‹ï¼šå‚æ•°é‡ = è®¡ç®—é‡ï¼ˆçº¿æ€§å…³ç³»ï¼‰
MoE æ¨¡å‹ï¼šå‚æ•°é‡ >> è®¡ç®—é‡ï¼ˆè§£è€¦ï¼ï¼‰

ç¤ºä¾‹ï¼š
  æ€»å‚æ•°ï¼š671B (DeepSeek-V3)
  æ¿€æ´»å‚æ•°ï¼š37B
  â†’ ç”¨ 37B çš„è®¡ç®—æˆæœ¬è·å¾— 671B çš„å®¹é‡
```

2. **ä¸“å®¶ä¸“ä¸šåŒ–**
```
ä¸åŒä¸“å®¶è‡ªåŠ¨å­¦ä¹ ä¸åŒé¢†åŸŸçŸ¥è¯†ï¼š

ä¸“å®¶ #1ï¼šæ•°å­¦æ¨ç†
ä¸“å®¶ #2ï¼šä»£ç ç”Ÿæˆ
ä¸“å®¶ #3ï¼šå†å²çŸ¥è¯†
ä¸“å®¶ #4ï¼šè¯­è¨€ç¿»è¯‘
...

â†’ è‡ªç„¶çš„ä»»åŠ¡åˆ†å·¥ï¼
```

3. **å¯æ‰©å±•æ€§å¼º**
```
å¢åŠ ä¸“å®¶æ•°é‡å‡ ä¹ä¸å¢åŠ è®¡ç®—æˆæœ¬ï¼š

64 ä¸“å®¶ï¼Œtop-2ï¼šæ¿€æ´» 3.125%
128 ä¸“å®¶ï¼Œtop-2ï¼šæ¿€æ´» 1.56%
256 ä¸“å®¶ï¼Œtop-2ï¼šæ¿€æ´» 0.78%

â†’ å¯æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°è§„æ¨¡
```

#### âŒ å±€é™

1. **è®­ç»ƒä¸ç¨³å®š**
```
é—®é¢˜ï¼šè´Ÿè½½ä¸å‡è¡¡

ç†æƒ³æƒ…å†µï¼š
  æ¯ä¸ªä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨
  
å®é™…æƒ…å†µï¼š
  çƒ­é—¨ä¸“å®¶ï¼šè¢« 80% çš„ token é€‰æ‹© ğŸ”¥
  å†·é—¨ä¸“å®¶ï¼šå‡ ä¹ä¸è¢«é€‰æ‹© â„ï¸
  
è§£å†³æ–¹æ¡ˆï¼š
  - è´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆLoad Balance Lossï¼‰
  - ä¸“å®¶å®¹é‡é™åˆ¶ï¼ˆExpert Capacityï¼‰
  - è¾…åŠ©æŸå¤±ï¼ˆAuxiliary Lossï¼‰
```

2. **é€šä¿¡å¼€é”€**
```
åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼š

é—®é¢˜ï¼š
  Token éœ€è¦è·¯ç”±åˆ°ä¸åŒ GPU ä¸Šçš„ä¸“å®¶
  â†’ All-to-All é€šä¿¡
  â†’ å¸¦å®½ç“¶é¢ˆ

ä¼˜åŒ–ï¼š
  - ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelismï¼‰
  - å±€éƒ¨ä¸“å®¶ï¼ˆLocal Expertsï¼‰
```

3. **ä»éœ€å¤§é‡å‰å‘è®¡ç®—**
```
è™½ç„¶åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œä½†ï¼š

  âŒ è·¯ç”±å†³ç­–éœ€è¦è®¡ç®—
  âŒ ä¸“å®¶å‰å‘ä¼ æ’­éœ€è¦æ—¶é—´
  âŒ æµ…å±‚ä»éœ€å¤„ç†çŸ¥è¯†å›å¿†
  
â†’ å­˜åœ¨è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´
```

---

## ä¸‰ã€Engramï¼šæ¡ä»¶è®°å¿†çš„çªç ´

### 3.1 æ¡ˆä¾‹ï¼šEngram å¦‚ä½•å½»åº•æ”¹å˜ "capital of France" çš„å¤„ç†

Engram çš„é©å‘½æ€§æ€æƒ³ï¼š**çŸ¥è¯†ä¸éœ€è¦"è®¡ç®—"ï¼Œå¯ä»¥ç›´æ¥"æŸ¥æ‰¾"**ï¼

```
è¾“å…¥ï¼š"What is the capital of France?"
Token åŒ–ï¼š[What] [is] [the] [capital] [of] [France] [?]

Engram çš„å¤„ç†æµç¨‹ï¼š

Layer 1-3 (æµ…å±‚ MoE):
  åŸºç¡€å¤„ç†ï¼šç†è§£è¿™æ˜¯ä¸€ä¸ªé—®å¥
  æ¿€æ´»å‚æ•°ï¼š3%
  
Layer 4 - ğŸ”¥ Engram è®°å¿†æ¨¡å—ï¼š
  
  æ­¥éª¤ 1ï¼šæå– N-gram æ¨¡å¼
    1-gram: "capital", "of", "France"
    2-gram: "capital of", "of France"
    3-gram: "capital of France" â† å…³é”®ï¼
  
  æ­¥éª¤ 2ï¼šå“ˆå¸ŒæŸ¥è¡¨ï¼ˆO(1)ï¼‰
    hash("capital of France") % 10^7 â†’ æ§½ä½ #1234567
    
  æ­¥éª¤ 3ï¼šç›´æ¥è¯»å–æ§½ä½ï¼ˆæ— éœ€è®¡ç®—ï¼ï¼‰
    Memory[#1234567] â†’ [0.23, -0.45, 0.78, ...]
    è¿™ä¸ªå‘é‡å·²ç»ç¼–ç äº†"France é¦–éƒ½æ˜¯ Paris"çš„çŸ¥è¯†ï¼
    
  æ­¥éª¤ 4ï¼šèåˆåˆ°éšè—çŠ¶æ€
    hidden_state = hidden_state + memory_embedding
    â†’ æµ…å±‚å°±è·å¾—äº† Paris çš„å…ˆéªŒçŸ¥è¯†ï¼âœ¨
  
Layer 5-8 (æ·±å±‚ MoE):
  ä¸éœ€è¦"å›å¿†"äº†ï¼Œç›´æ¥åšæ¨ç†ï¼š
    - ç†è§£é—®å¥æ„å›¾
    - ç»„ç»‡ç­”æ¡ˆè¡¨è¾¾
    - ç”Ÿæˆ "Paris"
  
  â†’ æ·±å±‚æœ‰æ›´å¤šå®¹é‡åšå¤æ‚æ¨ç†ï¼

å…³é”®åˆ›æ–°ï¼š
  âœ… O(1) æŸ¥è¡¨ï¼šä¸€æ¬¡å“ˆå¸Œï¼Œç›´æ¥è·å–çŸ¥è¯†
  âœ… æµ…å±‚è½»æ¾ï¼šä¸éœ€è¦å¤šå±‚"å›å¿†"
  âœ… æ·±å±‚å¢å¼ºï¼šé‡Šæ”¾å‡ºçš„å®¹é‡ç”¨äºæ¨ç†
  âœ… æ˜¾å¼å­˜å‚¨ï¼šçŸ¥è¯†åœ¨è®°å¿†è¡¨ä¸­ï¼Œå¯ç›´æ¥è®¿é—®
```

**å¯¹æ¯”ä¸‰ä»£æ¶æ„å¤„ç†åŒä¸€ä¸ªé—®é¢˜**ï¼š

```
é—®ï¼š"What is the capital of France?"

Dense Transformer (2017):
  Layer 1-12: æ¯å±‚ç”¨å…¨éƒ¨å‚æ•°é€æ¸"æƒ³èµ·"Paris
  è®¡ç®—é‡ï¼š12 å±‚ Ã— 100% å‚æ•° = 1200%
  æ—¶é—´ï¼šO(12 Ã— n)
  
MoE Transformer (2022):
  Layer 1-12: æ¯å±‚é€‰æ‹©ä¸“å®¶ï¼Œé€æ¸"æƒ³èµ·"Paris
  è®¡ç®—é‡ï¼š12 å±‚ Ã— 3% å‚æ•° = 36%
  æ—¶é—´ï¼šO(12 Ã— n)  (n å˜å°äº†)
  
MoE + Engram (2026):
  Layer 1-3: åŸºç¡€å¤„ç† (3 å±‚ Ã— 3% = 9%)
  Layer 4: Engram æŸ¥è¡¨ï¼Œç›´æ¥è·å¾—çŸ¥è¯† (O(1))
  Layer 5-12: æ·±å±‚æ¨ç† (8 å±‚ Ã— 3% = 24%)
  è®¡ç®—é‡ï¼š33% + O(1) æŸ¥è¡¨
  æ—¶é—´ï¼šæµ…å±‚å¿«é€Ÿï¼Œæ·±å±‚å……è£•
  
  â†’ æ›´å¿«ã€æ›´å‡†ã€æ›´æ·±ï¼âœ¨
```

### 3.2 Engram çš„æå‡ºèƒŒæ™¯ï¼ˆ2026ï¼‰

**æ¥æº**ï¼š[Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://github.com/deepseek-ai/Engram) (DeepSeek & åŒ—äº¬å¤§å­¦, 2026)

#### æ ¸å¿ƒæ´å¯Ÿ

> MoE é€šè¿‡æ¡ä»¶è®¡ç®—å®ç°ç¨€ç–åŒ–ï¼Œä½† Transformer ç¼ºä¹**åŸç”Ÿçš„çŸ¥è¯†æŸ¥æ‰¾åŸè¯­**ï¼Œåªèƒ½é€šè¿‡è®¡ç®—è¿‡ç¨‹ä½æ•ˆåœ°æ¨¡æ‹Ÿæ£€ç´¢è¡Œä¸ºã€‚

DeepSeek çš„å…³é”®æ€è€ƒï¼š

```
è§‚å¯Ÿäººç±»å›ç­”é—®é¢˜ï¼š
  
  é—®ï¼š"æ³•å›½é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
  äººç±»ï¼š
    â†’ ç›´æ¥ä»è®°å¿†ä¸­æå–ï¼š"å·´é»"
    â†’ æ—¶é—´ï¼šå‡ ä¹ç¬é—´ï¼ˆO(1)ï¼‰
    â†’ æ–¹å¼ï¼šæŸ¥æ‰¾ï¼Œä¸æ˜¯è®¡ç®—
  
å¯¹æ¯”ä¼ ç»Ÿ LLMï¼š
  
  è¾“å…¥ï¼š"capital of France"
  æ¨¡å‹ï¼š
    â†’ Layer 1: å¼€å§‹å¤„ç†...
    â†’ Layer 2: ç»§ç»­å¤„ç†...
    â†’ Layer 3-5: åŠªåŠ›"å›å¿†"...
    â†’ Layer 6-8: é€æ¸"æƒ³èµ·"...
    â†’ Layer 9-12: ç»ˆäºç¡®å®šæ˜¯ Paris
    â†’ æ—¶é—´ï¼šéœ€è¦ 12 å±‚è®¡ç®—ï¼ˆO(n)ï¼‰
    â†’ æ–¹å¼ï¼šè®¡ç®—ï¼Œä¸æ˜¯æŸ¥æ‰¾

é—®é¢˜ï¼š
  ä¸ºä»€ä¹ˆç®€å•çš„äº‹å®æ£€ç´¢éœ€è¦è¿™ä¹ˆå¤šè®¡ç®—ï¼Ÿ
  èƒ½å¦åƒäººç±»ä¸€æ ·ï¼Œç›´æ¥"æŸ¥æ‰¾"è€Œé"è®¡ç®—"ï¼Ÿ
```

#### æå‡ºçš„è§£å†³æ–¹æ¡ˆï¼šæ¡ä»¶è®°å¿†

**æ ¸å¿ƒæ€æƒ³**ï¼šå¼•å…¥ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯ç›´æ¥æŸ¥æ‰¾çš„é™æ€è®°å¿†æ¨¡å—ã€‚

```
ç¨€ç–åŒ–çš„ä¸¤ä¸ªæ­£äº¤ç»´åº¦ï¼š

ç»´åº¦ 1ï¼šæ¡ä»¶è®¡ç®—ï¼ˆMoEï¼‰
  - åŠ¨æ€é€‰æ‹©è®¡ç®—è·¯å¾„
  - ç¥ç»ç½‘ç»œå‚æ•°
  - é€‚åˆï¼šæ¨ç†ã€ç”Ÿæˆ

ç»´åº¦ 2ï¼šæ¡ä»¶è®°å¿†ï¼ˆEngramï¼‰â­ æ–°ï¼
  - åŠ¨æ€é€‰æ‹©è®°å¿†è®¿é—®
  - é™æ€æŸ¥æ‰¾è¡¨
  - é€‚åˆï¼šçŸ¥è¯†æ£€ç´¢ã€å±€éƒ¨æ¨¡å¼
```

### 3.2 Engram æ ¸å¿ƒæ¶æ„

#### æ•´ä½“æ¶æ„

```
è¾“å…¥ Token åºåˆ—
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Layer                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer 1-N (æµ…å±‚)    â”‚
â”‚ â”œâ”€ Multi-Head Attention         â”‚
â”‚ â””â”€ MoE Experts                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”
â•‘ ğŸ”¥ Engram è®°å¿†æ¨¡å—              â•‘
â•‘                                 â•‘
â•‘ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘ â”‚ 1. N-gram æå–               â”‚ â•‘
â•‘ â”‚    Token åºåˆ— â†’ N-gram æ¨¡å¼  â”‚ â•‘
â•‘ â”‚    "capital of France"       â”‚ â•‘
â•‘ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘         â†“                       â•‘
â•‘ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘ â”‚ 2. å“ˆå¸Œæ˜ å°„                  â”‚ â•‘
â•‘ â”‚    N-gram â†’ Slot Index       â”‚ â•‘
â•‘ â”‚    hash(...) % M â†’ #1234567  â”‚ â•‘
â•‘ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘         â†“                       â•‘
â•‘ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘ â”‚ 3. é™æ€è®°å¿†è¡¨                â”‚ â•‘
â•‘ â”‚    Memory[#1234567]          â”‚ â•‘
â•‘ â”‚    â†’ [0.23, -0.45, ...]      â”‚ â•‘
â•‘ â”‚    (10^7 æ§½ä½ Ã— 4096 ç»´)     â”‚ â•‘
â•‘ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘         â†“                       â•‘
â•‘ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘ â”‚ 4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§            â”‚ â•‘
â•‘ â”‚    Query = f(hidden_state)   â”‚ â•‘
â•‘ â”‚    Attention(Q, K, V)        â”‚ â•‘
â•‘ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘         â†“                       â•‘
â•‘ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘ â”‚ 5. èåˆåˆ°éšè—çŠ¶æ€            â”‚ â•‘
â•‘ â”‚    hidden' = hidden + memory â”‚ â•‘
â•‘ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â””â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer N+1-æœ«å±‚(æ·±å±‚) â”‚
â”‚ â”œâ”€ Multi-Head Attention         â”‚
â”‚ â””â”€ MoE Experts                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¾“å‡º
```

#### è¯¦ç»†å®ç°ï¼šä»¥ "capital of France" ä¸ºä¾‹

**å®Œæ•´çš„æ•°æ®æµ**ï¼š

```
è¾“å…¥ token åºåˆ—ï¼š["What", "is", "the", "capital", "of", "France", "?"]
ä½ç½® 5 (token "France") çš„å¤„ç†ï¼š

1ï¸âƒ£ æå– N-gramï¼š
   1-gram: ("France",)
   2-gram: ("of", "France")
   3-gram: ("capital", "of", "France") â† æœ€å…³é”®çš„æ¨¡å¼ï¼

2ï¸âƒ£ å“ˆå¸Œæ˜ å°„ï¼ˆå¤šå¤´ï¼‰ï¼š
   3-gram "capital of France":
     Head 0: hash(..., seed=0) % 10^7 â†’ æ§½ä½ #1234567
     Head 1: hash(..., seed=1) % 10^7 â†’ æ§½ä½ #5678901
     Head 2: hash(..., seed=2) % 10^7 â†’ æ§½ä½ #2345678
     Head 3: hash(..., seed=3) % 10^7 â†’ æ§½ä½ #8901234

3ï¸âƒ£ æŸ¥æ‰¾åµŒå…¥ï¼ˆO(1)ï¼‰ï¼š
   Memory[#1234567] â†’ [0.23, -0.45, 0.78, ..., 0.12]  â† 4096 ç»´å‘é‡
   Memory[#5678901] â†’ [0.34, -0.23, 0.56, ..., 0.34]
   Memory[#2345678] â†’ [0.12, -0.67, 0.89, ..., 0.56]
   Memory[#8901234] â†’ [0.45, -0.12, 0.34, ..., 0.78]
   
   è¿™ 4 ä¸ªå‘é‡å…±åŒç¼–ç äº† "capital of France â†’ Paris" çš„çŸ¥è¯†ï¼

4ï¸âƒ£ ä¸Šä¸‹æ–‡é—¨æ§ï¼š
   å½“å‰éšè—çŠ¶æ€ï¼šhidden[5] = [...]  â† "France" çš„è¡¨ç¤º
   
   Query = Linear_Q(hidden[5])
   Key = Linear_K([4 ä¸ªè®°å¿†å‘é‡])
   Value = Linear_V([4 ä¸ªè®°å¿†å‘é‡])
   
   Attention â†’ åŠ¨æ€é€‰æ‹©å’ŒåŠ æƒ
   â†’ å¾—åˆ°ä¸Šä¸‹æ–‡åŒ–çš„è®°å¿†è¡¨ç¤º

5ï¸âƒ£ èåˆï¼š
   enhanced_hidden[5] = hidden[5] + contextualized_memory
   
   â†’ "France" çš„è¡¨ç¤ºç°åœ¨åŒ…å«äº†"Paris"çš„å…ˆéªŒçŸ¥è¯†ï¼âœ¨
```

**ä»£ç å®ç°**ï¼š

```python
class EngramModule(nn.Module):
    """Engram æ¡ä»¶è®°å¿†æ¨¡å—"""
    
    def __init__(self, 
                 vocab_size=128000,
                 hidden_dim=4096,
                 num_slots=10_000_000,
                 num_hash_heads=4,
                 max_ngram_order=3):
        super().__init__()
        
        # 1. åˆ†è¯å™¨å‹ç¼©æ˜ å°„
        self.vocab_compression = build_compression_map(vocab_size)
        
        # 2. å¤šå¤´å“ˆå¸Œè®°å¿†è¡¨
        self.memory_tables = nn.ModuleList([
            nn.Embedding(num_slots, hidden_dim)
            for _ in range(num_hash_heads)
        ])
        
        # 3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§
        self.query_proj = nn.Linear(hidden_dim, hidden_dim)
        self.key_proj = nn.Linear(hidden_dim, hidden_dim)
        self.value_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # 4. è½»é‡çº§å·ç§¯
        self.conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
        
        self.num_slots = num_slots
        self.num_hash_heads = num_hash_heads
        self.max_ngram_order = max_ngram_order
    
    def forward(self, input_ids, hidden_states):
        """
        Args:
            input_ids: [batch, seq_len]
            hidden_states: [batch, seq_len, hidden_dim]
        
        Returns:
            memory_enhanced: [batch, seq_len, hidden_dim]
        """
        # === æ­¥éª¤ 1: æå– N-gram ===
        ngrams_list = []
        for n in range(1, self.max_ngram_order + 1):
            ngrams = self.extract_ngrams(input_ids, n)
            ngrams_list.extend(ngrams)
        
        # === æ­¥éª¤ 2: åˆ†è¯å™¨å‹ç¼© ===
        compressed_ngrams = [
            tuple(self.vocab_compression[t] for t in ng)
            for ng in ngrams_list
        ]
        
        # === æ­¥éª¤ 3: å¤šå¤´å“ˆå¸Œæ£€ç´¢ ===
        all_embeddings = []
        for head_id, memory_table in enumerate(self.memory_tables):
            # è®¡ç®—æ§½ä½ç´¢å¼•
            slot_indices = [
                self.hash_ngram(ng, seed=head_id) % self.num_slots
                for ng in compressed_ngrams
            ]
            
            # æ£€ç´¢åµŒå…¥
            embeddings = memory_table(torch.tensor(slot_indices))
            all_embeddings.append(embeddings)
        
        # å †å æ‰€æœ‰å¤´çš„ç»“æœ
        static_memory = torch.stack(all_embeddings, dim=1)  # [num_ngrams, num_heads, dim]
        
        # === æ­¥éª¤ 4: ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§ ===
        # ä¸ºæ¯ä¸ªä½ç½®è®¡ç®—æ³¨æ„åŠ›
        batch_size, seq_len, _ = hidden_states.shape
        memory_output = []
        
        for pos in range(seq_len):
            # å½“å‰ä½ç½®çš„éšè—çŠ¶æ€
            h = hidden_states[:, pos, :]  # [batch, dim]
            
            # è¯¥ä½ç½®å¯¹åº”çš„ N-gram åµŒå…¥
            pos_memory = static_memory[pos * self.max_ngram_order:(pos + 1) * self.max_ngram_order]
            pos_memory = pos_memory.view(-1, hidden_dim)  # [num_heads * max_ngram, dim]
            
            # è®¡ç®—æ³¨æ„åŠ›
            query = self.query_proj(h)
            key = self.key_proj(pos_memory)
            value = self.value_proj(pos_memory)
            
            attn_scores = torch.matmul(query, key.T) / math.sqrt(hidden_dim)
            attn_weights = F.softmax(attn_scores, dim=-1)
            
            # åŠ æƒæ±‚å’Œ
            context_memory = torch.matmul(attn_weights, value)
            memory_output.append(context_memory)
        
        memory_output = torch.stack(memory_output, dim=1)  # [batch, seq_len, dim]
        
        # === æ­¥éª¤ 5: è½»é‡çº§å·ç§¯ç²¾ç‚¼ ===
        memory_output = memory_output.transpose(1, 2)  # [batch, dim, seq_len]
        memory_output = self.conv(memory_output)
        memory_output = memory_output.transpose(1, 2)  # [batch, seq_len, dim]
        
        return memory_output
    
    def extract_ngrams(self, input_ids, n):
        """æå– N-gram"""
        ngrams = []
        for i in range(input_ids.size(1) - n + 1):
            ngram = tuple(input_ids[0, i:i+n].tolist())
            ngrams.append(ngram)
        return ngrams
    
    def hash_ngram(self, ngram, seed=0):
        """å“ˆå¸Œå‡½æ•°"""
        import hashlib
        ngram_str = f"{seed}:{ngram}"
        hash_value = int(hashlib.sha256(ngram_str.encode()).hexdigest(), 16)
        return hash_value
```

### 3.3 å…³é”®æŠ€æœ¯åˆ›æ–°

#### 1. å¤šå¤´å“ˆå¸Œï¼ˆMulti-Head Hashingï¼‰

**é—®é¢˜**ï¼šå“ˆå¸Œå†²çªä¸å¯é¿å…

```
å•å¤´å“ˆå¸Œï¼š
  "capital of France" â†’ hash â†’ æ§½ä½ #100
  "weather in Tokyo" â†’ hash â†’ æ§½ä½ #100  âŒ å†²çªï¼
  
å¤šå¤´å“ˆå¸Œï¼ˆK=4ï¼‰ï¼š
  "capital of France":
    Head 1 â†’ æ§½ä½ #100
    Head 2 â†’ æ§½ä½ #523
    Head 3 â†’ æ§½ä½ #891
    Head 4 â†’ æ§½ä½ #234
  
  "weather in Tokyo":
    Head 1 â†’ æ§½ä½ #100  (å†²çª)
    Head 2 â†’ æ§½ä½ #777  (ä¸å†²çª) âœ“
    Head 3 â†’ æ§½ä½ #445  (ä¸å†²çª) âœ“
    Head 4 â†’ æ§½ä½ #999  (ä¸å†²çª) âœ“
  
  â†’ 4 ä¸ªå¤´åŒæ—¶å†²çªçš„æ¦‚ç‡æä½ï¼
```

#### 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§ï¼ˆContext-Aware Gatingï¼‰

**é—®é¢˜**ï¼šé™æ€åµŒå…¥ç¼ºä¹ä¸Šä¸‹æ–‡é€‚åº”æ€§

```
ç¤ºä¾‹ï¼š"bank"

é™æ€åµŒå…¥å¯èƒ½æ··åˆäº†å¤šä¸ªå«ä¹‰ï¼š
  - æ²³å²¸ï¼ˆriver bankï¼‰
  - é“¶è¡Œï¼ˆmoney bankï¼‰

ä¸Šä¸‹æ–‡é—¨æ§è§£å†³ï¼š
  
  åœºæ™¯ Aï¼š"I went to the river bank"
    Query from "river" â†’ é«˜æƒé‡åˆ†é…ç»™"æ²³å²¸"å«ä¹‰
  
  åœºæ™¯ Bï¼š"I went to the money bank"
    Query from "money" â†’ é«˜æƒé‡åˆ†é…ç»™"é“¶è¡Œ"å«ä¹‰
  
  â†’ åŠ¨æ€æ¶ˆæ­§ï¼âœ¨
```

#### 3. å­˜å‚¨-è®¡ç®—è§£è€¦ï¼ˆStorage-Compute Decouplingï¼‰

**åˆ›æ–°**ï¼šç¡®å®šæ€§å“ˆå¸Œæ”¯æŒé¢„å–

```python
# æ¨ç†æ—¶çš„ä¼˜åŒ–

def prefetch_strategy(input_ids, memory_table):
    """
    å› ä¸ºå“ˆå¸Œæ˜¯ç¡®å®šæ€§çš„ï¼Œå¯ä»¥æå‰çŸ¥é“éœ€è¦å“ªäº›æ§½ä½ï¼
    """
    # 1. é¢„å…ˆè®¡ç®—æ‰€æœ‰æ§½ä½ç´¢å¼•ï¼ˆCPUï¼‰
    all_ngrams = extract_all_ngrams(input_ids)
    slot_indices = [hash(ng) % M for ng in all_ngrams]
    
    # 2. å¼‚æ­¥ä»ä¸»æœºå†…å­˜é¢„å–åˆ° GPUï¼ˆPCIeï¼‰
    async_prefetch(memory_table, slot_indices)
    
    # 3. åŒæ—¶è¿›è¡Œå‰å‡ å±‚ Transformer è®¡ç®—ï¼ˆGPUï¼‰
    hidden = layers_1_to_5(input_ids)
    
    # 4. åˆ° Engram å±‚æ—¶ï¼Œæ•°æ®å·²ç»åœ¨ GPU äº†ï¼
    memory = memory_table[slot_indices]  # æ— å»¶è¿Ÿ âœ¨
    
    return hidden, memory
```

**å¤šçº§ç¼“å­˜è®¾è®¡**ï¼š

```
åˆ©ç”¨ N-gram çš„ Zipfian åˆ†å¸ƒï¼š
  å°‘æ•°é«˜é¢‘ N-gram è´¡çŒ®äº†å¤§éƒ¨åˆ†è®¿é—®

L1 ç¼“å­˜ï¼ˆGPU HBMï¼‰ï¼š
  å­˜å‚¨ Top 1% çƒ­é—¨æ§½ä½
  å‘½ä¸­ç‡ ~80%
  è®¿é—®æ—¶é—´ï¼šçº³ç§’çº§

L2 ç¼“å­˜ï¼ˆä¸»æœº DRAMï¼‰ï¼š
  å­˜å‚¨ Top 10% å¸¸ç”¨æ§½ä½
  å‘½ä¸­ç‡ ~15%
  è®¿é—®æ—¶é—´ï¼šå¾®ç§’çº§

L3 å­˜å‚¨ï¼ˆNVMe SSDï¼‰ï¼š
  å­˜å‚¨å…¨éƒ¨æ§½ä½
  å‘½ä¸­ç‡ ~5%
  è®¿é—®æ—¶é—´ï¼šæ¯«ç§’çº§

â†’ å…¼é¡¾å®¹é‡å’Œé€Ÿåº¦ï¼
```

---

## å››ã€æ¶æ„å¯¹æ¯”ä¸ååŒ

### 4.1 æ ¸å¿ƒæ¡ˆä¾‹ï¼šä¸‰ä»£æ¶æ„å®Œæ•´å¯¹æ¯”

è®©æˆ‘ä»¬å®Œæ•´å¯¹æ¯”ä¸‰ä»£æ¶æ„å¦‚ä½•å¤„ç† **"What is the capital of France?"** è¿™ä¸ªé—®é¢˜ï¼š

#### Dense Transformer (2017)

```
è¾“å…¥ï¼š[What] [is] [the] [capital] [of] [France] [?]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Embedding + Attention + Dense FFN     â”‚
â”‚   æ‰€æœ‰ token ç”¨å…¨éƒ¨å‚æ•° (100%)                  â”‚
â”‚   ç†è§£ï¼šè¿™æ˜¯ä¸€ä¸ªå¥å­...                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2-3: ç»§ç»­å…¨å‚æ•°å¤„ç†                       â”‚
â”‚   ç†è§£ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äºåœ°ç†çš„é—®å¥                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 4-6: ä»ç„¶å…¨å‚æ•° (100%)                    â”‚
â”‚   FFN å¼€å§‹"å›å¿†"ï¼šFrance... æ¬§æ´²å›½å®¶...         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 7-9: å…¨å‚æ•°è®¡ç®—                          â”‚
â”‚   ç»§ç»­"å›å¿†"ï¼šcapital... é¦–éƒ½... Paris...      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 10-12: å…¨å‚æ•°å¤„ç†                        â”‚
â”‚   ç¡®å®šç­”æ¡ˆï¼šParisï¼ç»„ç»‡è¾“å‡º...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è®¡ç®—é‡ï¼š12 å±‚ Ã— 100% å‚æ•° = 1200% (åŸºçº¿)
æ—¶é—´ï¼šé•¿ï¼ˆæ¯å±‚éƒ½æ˜¯å®Œæ•´å‰å‘ä¼ æ’­ï¼‰
æµ…å±‚çŠ¶æ€ï¼šå¿™äº"å›å¿†"äº‹å®
æ·±å±‚çŠ¶æ€ï¼šå¯ç”¨å®¹é‡æœ‰é™
```

#### MoE Transformer (2022)

```
è¾“å…¥ï¼š[What] [is] [the] [capital] [of] [France] [?]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Embedding + Attention + MoE           â”‚
â”‚   Token "capital" â†’ è·¯ç”±åˆ°ã€åœ°ç†ä¸“å®¶ã€‘ã€æ”¿æ²»ä¸“å®¶ã€‘â”‚
â”‚   æ¿€æ´» 2/64 ä¸“å®¶ = 3.125%                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2-3: ç»§ç»­ MoE (3%)                        â”‚
â”‚   Token "France" â†’ è·¯ç”±åˆ°ã€æ¬§æ´²ä¸“å®¶ã€‘ã€æ³•å›½ä¸“å®¶ã€‘â”‚
â”‚   å¼€å§‹"å›å¿†"France ç›¸å…³ä¿¡æ¯                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 4-6: MoE ç¨€ç–æ¿€æ´» (3%)                    â”‚
â”‚   ä¸“å®¶ä»¬é€æ¸"æƒ³èµ·"Paris...                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 7-9: ç»§ç»­ MoE                             â”‚
â”‚   ç¡®å®šï¼šcapital of France â†’ Paris              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 10-12: MoE ç»„ç»‡è¾“å‡º                       â”‚
â”‚   ç”Ÿæˆç­”æ¡ˆ...                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è®¡ç®—é‡ï¼š12 å±‚ Ã— 3% å‚æ•° = 36%
æ—¶é—´ï¼šä¸­ç­‰ï¼ˆæ¯å±‚ä»éœ€å‰å‘ä¼ æ’­ï¼Œä½†å‚æ•°å°‘ï¼‰
æµ…å±‚çŠ¶æ€ï¼šä»åœ¨"å›å¿†"ï¼Œä½†æ•ˆç‡æå‡
æ·±å±‚çŠ¶æ€ï¼šå¯ç”¨å®¹é‡ä»å—é™
æ”¹è¿›ï¼šâœ… å‚æ•°æ•ˆç‡å¤§å¹…æå‡
ä»å­˜åœ¨ï¼šâŒ ä»éœ€å¤šå±‚"å›å¿†"äº‹å®
```

#### MoE + Engram (2026)

```
è¾“å…¥ï¼š[What] [is] [the] [capital] [of] [France] [?]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1-3: æµ…å±‚ MoE (3% Ã— 3 = 9%)               â”‚
â”‚   åŸºç¡€å¤„ç†ï¼šç†è§£é—®å¥ç»“æ„                         â”‚
â”‚   æ— éœ€"å›å¿†"çŸ¥è¯†ï¼Œåªåšåˆæ­¥ç†è§£                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 4: ğŸ”¥ Engram è®°å¿†æ¨¡å— (O(1) æŸ¥è¡¨)        â”‚
â”‚                                                 â”‚
â”‚   æå–ï¼š"capital of France"                     â”‚
â”‚   å“ˆå¸Œï¼šhash(...) % 10^7 â†’ #1234567            â”‚
â”‚   æŸ¥è¡¨ï¼šMemory[#1234567] â†’ [çŸ¥è¯†å‘é‡]          â”‚
â”‚   èåˆï¼šhidden + memory                        â”‚
â”‚                                                 â”‚
â”‚   âœ¨ ç›´æ¥è·å¾—"Paris"çš„å…ˆéªŒçŸ¥è¯†ï¼                â”‚
â”‚   âœ¨ æ— éœ€å±‚å±‚"å›å¿†"ï¼Œä¸€æ­¥åˆ°ä½ï¼                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 5-8: æ·±å±‚ MoE (3% Ã— 4 = 12%)             â”‚
â”‚   å·²æœ‰ Paris å…ˆéªŒï¼Œç°åœ¨ä¸“æ³¨äºï¼š                 â”‚
â”‚   - ç†è§£é—®å¥æ„å›¾                                â”‚
â”‚   - ç»„ç»‡ç­”æ¡ˆè¡¨è¾¾                                â”‚
â”‚   - ä¸Šä¸‹æ–‡æ•´åˆ                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 9-12: æ›´æ·±å±‚ MoE (3% Ã— 4 = 12%)          â”‚
â”‚   å¤§é‡å®¹é‡ç”¨äºå¤æ‚æ¨ç†ï¼                        â”‚
â”‚   å¯ä»¥å¤„ç†æ›´å¤æ‚çš„é€»è¾‘                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è®¡ç®—é‡ï¼š33% + O(1) æŸ¥è¡¨
æ—¶é—´ï¼šçŸ­ï¼ˆæµ…å±‚å¿«é€Ÿï¼ŒæŸ¥è¡¨ç¬é—´ï¼‰
æµ…å±‚çŠ¶æ€ï¼šè½»æ¾ï¼ˆç›´æ¥æŸ¥è¡¨ï¼‰
æ·±å±‚çŠ¶æ€ï¼šå……è£•ï¼ˆæœ‰æ•ˆæ·±åº¦ +5 å±‚ï¼‰
çªç ´ï¼šâœ… çŸ¥è¯†è®¿é—® O(1)
      âœ… æµ…å±‚è´Ÿæ‹…å¤§å‡
      âœ… æ·±å±‚å®¹é‡å¢åŠ 
```

### 4.2 é‡åŒ–å¯¹æ¯”è¡¨

| ç»´åº¦ | Dense Transformer | MoE Transformer | MoE + Engram |
|------|------------------|-----------------|--------------|
| **æå‡ºæ—¶é—´** | 2017 | 2017-2022 | 2026 |
| **å¤„ç† "capital of France"** | 12 å±‚é€æ­¥å›å¿† | 12 å±‚ç¨€ç–å›å¿† | æŸ¥è¡¨ + æ·±å±‚æ¨ç† |
| **ç¨€ç–ç»´åº¦** | æ—  | æ¡ä»¶è®¡ç®—ï¼ˆ1 ç»´ï¼‰ | æ¡ä»¶è®¡ç®— + æ¡ä»¶è®°å¿†ï¼ˆ2 ç»´ï¼‰ |
| **å‚æ•°æ•ˆç‡** | 1Ã— (baseline) | 10-20Ã— | 15-30Ã— |
| **çŸ¥è¯†å­˜å‚¨** | FFN éšå¼å­˜å‚¨ | MoE éšå¼å­˜å‚¨ | MoE + Engram æ˜¾å¼æŸ¥æ‰¾ |
| **çŸ¥è¯†è®¿é—®** | O(n) å‰å‘ä¼ æ’­ | O(n) å‰å‘ä¼ æ’­ | **O(1) æŸ¥è¡¨** ğŸ”¥ |
| **æµ…å±‚è´Ÿæ‹…** | é‡ï¼ˆå…¨å‚æ•°å›å¿†ï¼‰ | ä¸­ï¼ˆç¨€ç–å›å¿†ï¼‰ | **è½»ï¼ˆç›´æ¥æŸ¥è¡¨ï¼‰** âœ¨ |
| **æœ‰æ•ˆæ·±åº¦** | æ ‡å‡† | æ ‡å‡† | **+5 å±‚** ğŸš€ |
| **è®­ç»ƒç¨³å®šæ€§** | é«˜ | ä¸­ç­‰ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰ | ä¸­ç­‰ |
| **æ¨ç†ä¼˜åŒ–** | æ ‡å‡† | ä¸“å®¶å¹¶è¡Œ | é¢„å– + å¤šçº§ç¼“å­˜ |

### 4.2 çŸ¥è¯†åˆ†å¸ƒå˜åŒ–

#### Dense Transformer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ä½ç½®     â”‚  å æ¯”   â”‚ é™æ€/åŠ¨æ€â”‚       å­˜å‚¨çš„çŸ¥è¯†            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embedding   â”‚   ~3%   â”‚  é™æ€    â”‚ è¯çš„åŸºç¡€è¯­ä¹‰ã€è¯ç±»ä¿¡æ¯      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention   â”‚  ~15%   â”‚  åŠ¨æ€    â”‚ è¯­æ³•å…³ç³»ã€æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ç†è§£  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FFN         â”‚  ~65%   â”‚  åŠ¨æ€    â”‚ äº‹å®çŸ¥è¯†ã€å¸¸è¯†ã€ä¸–ç•Œæ¨¡å‹ â­ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output      â”‚   ~3%   â”‚  é™æ€    â”‚ å‘é‡åˆ°è¯çš„æ˜ å°„              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é—®é¢˜ï¼š
  âŒ FFN æ—¢å­˜å‚¨çŸ¥è¯†åˆåšæ¨ç†ï¼ˆèŒè´£æ··ä¹±ï¼‰
  âŒ æµ…å±‚èŠ±å¤§é‡ç®—åŠ›"å›å¿†"äº‹å®
```

#### MoE Transformer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ä½ç½®     â”‚  å æ¯”   â”‚ é™æ€/åŠ¨æ€â”‚       å­˜å‚¨çš„çŸ¥è¯†            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embedding   â”‚   ~3%   â”‚  é™æ€    â”‚ è¯çš„åŸºç¡€è¯­ä¹‰ã€è¯ç±»ä¿¡æ¯      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention   â”‚  ~15%   â”‚  åŠ¨æ€    â”‚ è¯­æ³•å…³ç³»ã€æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ç†è§£  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MoE Experts â”‚  ~65%   â”‚  åŠ¨æ€    â”‚ ä¸“å®¶ä¸“ä¸šåŒ–çŸ¥è¯† + æ¨ç† â­    â”‚
â”‚ (ç¨€ç–æ¿€æ´»)  â”‚         â”‚          â”‚ (ä½†ä»éœ€å‰å‘ä¼ æ’­è®¿é—®)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output      â”‚   ~3%   â”‚  é™æ€    â”‚ å‘é‡åˆ°è¯çš„æ˜ å°„              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ”¹è¿›ï¼š
  âœ… å‚æ•°æ•ˆç‡æå‡ï¼ˆç¨€ç–æ¿€æ´»ï¼‰
  âœ… ä¸“å®¶è‡ªåŠ¨ä¸“ä¸šåŒ–

ä»å­˜åœ¨ï¼š
  âŒ çŸ¥è¯†ä»éœ€å‰å‘ä¼ æ’­è®¿é—®
  âŒ æµ…å±‚ä»æœ‰å›å¿†è´Ÿæ‹…
```

#### MoE + Engram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ä½ç½®         â”‚  å æ¯”   â”‚ é™æ€/åŠ¨æ€â”‚       å­˜å‚¨çš„çŸ¥è¯†            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Embedding       â”‚   ~3%   â”‚  é™æ€    â”‚ è¯çš„åŸºç¡€è¯­ä¹‰ã€è¯ç±»ä¿¡æ¯      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Attention       â”‚  ~12%   â”‚  åŠ¨æ€    â”‚ å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆè´Ÿæ‹…å‡è½»ï¼‰âœ¨    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MoE Experts     â”‚  ~50%   â”‚  åŠ¨æ€    â”‚ å¤æ‚æ¨ç†ã€åŠ¨æ€è®¡ç®— ğŸ§        â”‚
â”‚                 â”‚         â”‚          â”‚ (ä¸“æ³¨æ¨ç†ï¼Œä¸å­˜çŸ¥è¯†)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”¥ Engram è®°å¿†   â”‚  ~20%   â”‚  é™æ€    â”‚ äº‹å®çŸ¥è¯†ã€å±€éƒ¨æ¨¡å¼ ğŸ“š      â”‚
â”‚                 â”‚         â”‚          â”‚ (O(1) æŸ¥è¡¨è®¿é—®)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Output          â”‚   ~3%   â”‚  é™æ€    â”‚ å‘é‡åˆ°è¯çš„æ˜ å°„              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

çªç ´ï¼š
  âœ… èŒè´£æ˜ç¡®åˆ†ç¦»ï¼ˆçŸ¥è¯† vs æ¨ç†ï¼‰
  âœ… æµ…å±‚è´Ÿæ‹…å¤§å¹…å‡è½»
  âœ… æ·±å±‚æœ‰æ•ˆæ·±åº¦å¢åŠ 
  âœ… æ³¨æ„åŠ›å®¹é‡é‡Šæ”¾
```

### 4.3 U å‹æ‰©å±•è§„å¾‹ ğŸ”¥

**æ ¸å¿ƒå‘ç°**ï¼šMoE ä¸ Engram çš„æœ€ä¼˜åˆ†é…æ¯”ä¾‹

```
å®éªŒè®¾ç½®ï¼š
  å›ºå®šæ¡ä»¶ï¼š
    - æ€»å‚æ•°é‡ P_total = 27B
    - è®­ç»ƒ FLOP = å¸¸æ•°
    - è®­ç»ƒæ•°æ® = 262B tokens
  
  å˜é‡ï¼š
    - Ï = MoE å ç¨€ç–å‚æ•°é¢„ç®—çš„æ¯”ä¾‹
    - èŒƒå›´ï¼š0% ~ 100%

ç»“æœï¼ˆéªŒè¯æŸå¤±ï¼‰ï¼š

  Loss
    â†‘
1.75â”‚     â—  â† Ï=0% (çº¯ Engramï¼Œå¾ˆå·®)
    â”‚      \
1.73â”‚       \
    â”‚        \
1.72â”‚         â— â† Ï=50% (å‡åˆ†ï¼Œæ¬¡ä¼˜)
    â”‚        /
1.71â”‚       /
    â”‚      / â— â† Ï=80% (æœ€ä¼˜ï¼)
1.70â”‚     /
    â”‚    â— â† Ï=100% (çº¯ MoEï¼Œæ¬¡ä¼˜)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ï
         0%   50%   80%  100%

å…³é”®ç»“è®ºï¼š
  1. çº¯ MoE (Ï=100%) æ˜¯æ¬¡ä¼˜çš„ ğŸ”¥
  2. æœ€ä¼˜ç‚¹åœ¨ Ï â‰ˆ 75-80%
  3. åˆ†é… 20-25% ç»™ Engram è·å¾—æœ€ä½³æ€§èƒ½
  4. ä¸¤ä¸ªç»´åº¦æ˜¯äº’è¡¥çš„ï¼Œä¸æ˜¯ç«äº‰çš„
```

**æ•°å­¦è§£é‡Š**ï¼š

```
æ€»æ€§èƒ½ âˆ f(æ¨ç†èƒ½åŠ›, çŸ¥è¯†å®¹é‡)

æ¨ç†èƒ½åŠ› âˆ MoE å‚æ•°é‡ âˆ Ï
çŸ¥è¯†å®¹é‡ âˆ Engram å‚æ•°é‡ âˆ (1-Ï)

æç«¯æƒ…å†µï¼š
  Ï â†’ 0%:   æ¨ç†â†’0ï¼ŒçŸ¥è¯†â†’æœ€å¤§  â†’ æ€»æ€§èƒ½ä½
  Ï = 80%:  æ¨ç†=é«˜ï¼ŒçŸ¥è¯†=å……è¶³  â†’ æ€»æ€§èƒ½æœ€é«˜ âœ¨
  Ï â†’ 100%: æ¨ç†â†’å—é™ï¼ŒçŸ¥è¯†â†’ä½æ•ˆ â†’ æ€»æ€§èƒ½æ¬¡ä¼˜

â†’ U å‹æ›²çº¿åæ˜ äº†ä¸¤ç§èƒ½åŠ›çš„ååŒä¼˜åŒ–
```

---

## äº”ã€æ€§èƒ½æå‡ä¸æœªæ¥å±•æœ›

### 5.1 å®éªŒéªŒè¯

**å®éªŒè®¾ç½®**ï¼ˆDeepSeek Engram è®ºæ–‡ï¼‰ï¼š

- **ç­‰å‚æ•°å¯¹æ¯”**ï¼šæ‰€æœ‰æ¨¡å‹ ~27B æ€»å‚æ•°
- **ç­‰ FLOP å¯¹æ¯”**ï¼šæ¯ä¸ª token æ¿€æ´»å‚æ•°ç›¸åŒ
- **è®­ç»ƒæ•°æ®**ï¼š262B tokensï¼ˆç›¸åŒæ•°æ®ã€ç›¸åŒé¡ºåºï¼‰

#### æ¨¡å‹é…ç½®

| æ¨¡å‹ | æ€»å‚æ•° | æ¿€æ´»å‚æ•° | MoE é…ç½® | Engram é…ç½® |
|------|--------|---------|---------|------------|
| Dense-4B | 4.1B | 4.1B | - | - |
| MoE-27B | 26.7B | ~2B | 64 ä¸“å®¶ Ã— top-2 | - |
| **Engram-27B** | **26.7B** | **~2B** | **50 ä¸“å®¶ Ã— top-2** | **æ§½ä½ 10^7** |
| Engram-40B | 39.5B | ~2B | 50 ä¸“å®¶ Ã— top-2 | æ§½ä½ 2Ã—10^7 |

#### æ€§èƒ½å¯¹æ¯”ï¼šEngram-27B vs MoE-27B

**çŸ¥è¯†å¯†é›†ä»»åŠ¡**ï¼š

| åŸºå‡† | MoE-27B | Engram-27B | æå‡ |
|------|---------|------------|------|
| MMLU | - | - | **+3.4** â­ |
| MMLU-Pro | - | - | **+1.8** |
| CMMLU | - | - | **+4.0** â­ |

**æ¨ç†ä»»åŠ¡**ï¼ˆæå‡æœ€æ˜¾è‘—ï¼ï¼‰ï¼š

| åŸºå‡† | MoE-27B | Engram-27B | æå‡ |
|------|---------|------------|------|
| BBH | - | - | **+5.0** ğŸ”¥ |
| ARC-Challenge | - | - | **+3.7** â­ |
| DROP | - | - | **+3.x** |

**ä»£ç ä¸æ•°å­¦**ï¼š

| åŸºå‡† | MoE-27B | Engram-27B | æå‡ |
|------|---------|------------|------|
| HumanEval | - | - | **+3.0** |
| MATH | - | - | **+2.4** |

**é•¿ä¸Šä¸‹æ–‡**ï¼ˆå·¨å¤§çªç ´ï¼ï¼‰ï¼š

| åŸºå‡† | MoE-27B | Engram-27B | æå‡ |
|------|---------|------------|------|
| Multi-Query NIAH | 84.2 | **97.0** | **+12.8** ğŸš€ |
| Variable Tracking | 71.x | **87.2** | **+15+** ğŸš€ |

### 5.2 æœºåˆ¶åˆ†æï¼šä¸ºä»€ä¹ˆ Engram æ›´æœ‰æ•ˆï¼Ÿ

è®©æˆ‘ä»¬ç»§ç»­ç”¨ **"capital of France"** ä¾‹å­åˆ†æå†…åœ¨æœºåˆ¶ã€‚

#### LogitLens åˆ†æï¼šæµ…å±‚æ”¶æ•›é€Ÿåº¦

LogitLens æŠ€æœ¯å¯ä»¥çœ‹åˆ°æ¯ä¸€å±‚çš„"é¢„æµ‹å€¾å‘"ï¼Œç”¨ KL æ•£åº¦è¡¡é‡ä¸æœ€ç»ˆç­”æ¡ˆçš„è·ç¦»ã€‚

**MoE æ¨¡å‹**ï¼š
```
è¾“å…¥ï¼š"What is the capital of France?"

Layer 1:  KL æ•£åº¦ = 8.5
  æ¨¡å‹çŠ¶æ€ï¼šåˆšå¼€å§‹å¤„ç†ï¼Œè¿˜ä¸çŸ¥é“ç­”æ¡ˆ
  
Layer 3:  KL æ•£åº¦ = 6.2
  æ¨¡å‹çŠ¶æ€ï¼šçŸ¥é“æ˜¯åœ°ç†é—®é¢˜ï¼Œä½†è¿˜åœ¨"æ‘¸ç´¢"
  
Layer 5:  KL æ•£åº¦ = 4.8
  æ¨¡å‹çŠ¶æ€ï¼šå¼€å§‹"æƒ³èµ·"Parisï¼Œä½†ä¸ç¡®å®š
  
Layer 8:  KL æ•£åº¦ = 2.1
  æ¨¡å‹çŠ¶æ€ï¼šé€æ¸ç¡®å®šæ˜¯ Paris
  
Layer 12: KL æ•£åº¦ = 0.3
  æ¨¡å‹çŠ¶æ€ï¼šç»ˆäºç¡®å®šç­”æ¡ˆæ˜¯ Paris
  
â†’ éœ€è¦ 12 å±‚æ‰èƒ½ç¡®å®šç®€å•äº‹å®ï¼
```

**Engram æ¨¡å‹**ï¼š
```
è¾“å…¥ï¼š"What is the capital of France?"

Layer 1-3: KL æ•£åº¦ = 6.0
  æµ…å±‚ MoEï¼šåŸºç¡€å¤„ç†
  
Layer 4: ğŸ”¥ Engram æŸ¥è¡¨
  Memory["capital of France"] â†’ [Paris çš„å…ˆéªŒ]
  
Layer 5:  KL æ•£åº¦ = 1.2  â† éª¤é™ï¼
  æ¨¡å‹çŠ¶æ€ï¼šå·²ç»"çŸ¥é“"æ˜¯ Parisï¼ˆé€šè¿‡æŸ¥è¡¨ï¼‰
  
Layer 7:  KL æ•£åº¦ = 0.4
  æ¨¡å‹çŠ¶æ€ï¼šè¿›ä¸€æ­¥ç¡®è®¤
  
Layer 12: KL æ•£åº¦ = 0.1
  æ¨¡å‹çŠ¶æ€ï¼šå®Œç¾è¾“å‡º
  
â†’ ç¬¬ 5 å±‚å°±ç¡®å®šäº†ç­”æ¡ˆï¼
â†’ åç»­å±‚å¯ä»¥åšæ›´å¤æ‚çš„æ¨ç†ï¼âœ¨
```

**å¯¹æ¯”æ›²çº¿**ï¼š
```
KL æ•£åº¦
  â†‘
9 â”‚ â—  MoE (Layer 1)
  â”‚  \
7 â”‚   \â—
  â”‚    \
5 â”‚     \â—  Engram (Layer 1-3)
  â”‚      \
3 â”‚       â—  MoE (Layer 8)
  â”‚    â— â† Engram (Layer 5)
1 â”‚   /  \
  â”‚  /    â—  MoE (Layer 12)
0 â”‚ â—â”€â”€â”€â”€â”€â”€â”€â”€â— Engram (Layer 12)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Layer
      1  3  5  8    12

å…³é”®ï¼š
  Engram åœ¨ç¬¬ 5 å±‚å°±è¾¾åˆ°äº† MoE ç¬¬ 8 å±‚çš„ç¡®å®šæ€§
  â†’ æœ‰æ•ˆæ·±åº¦å¢åŠ äº† ~3-5 å±‚ï¼
```

#### CKA ç›¸ä¼¼åº¦åˆ†æï¼šåŠŸèƒ½æ·±åº¦å¢åŠ 

CKAï¼ˆCentered Kernel Alignmentï¼‰å¯ä»¥è¡¡é‡ä¸åŒæ¨¡å‹å±‚çš„åŠŸèƒ½ç›¸ä¼¼åº¦ã€‚

```
å‘ç°ï¼šEngram æ¨¡å‹çš„ç¬¬ 5 å±‚ â‰ˆ MoE æ¨¡å‹çš„ç¬¬ 10 å±‚

å«ä¹‰ï¼š
  å¤„ç† "capital of France" æ—¶ï¼š
  
  Engram Layer 5 çš„è¡¨ç¤ºè´¨é‡ = MoE Layer 10 çš„è¡¨ç¤ºè´¨é‡
  
  â†’ Engram ç”¨æ›´å°‘çš„å±‚è¾¾åˆ°ç›¸åŒçš„ç†è§£æ·±åº¦
  â†’ ç­‰æ•ˆäºå¢åŠ äº† 5 å±‚çš„"åŠŸèƒ½æ·±åº¦"
  â†’ åç»­å±‚æœ‰æ›´å¤šå®¹é‡å¤„ç†å¤æ‚æ¨ç†ï¼âœ¨
```

**ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ**

```
MoE æ¨¡å‹çš„æµ…å±‚ï¼ˆLayer 1-10ï¼‰ï¼š
  å¿™äº"å›å¿†"Paris è¿™ä¸ªäº‹å®
  â†’ å ç”¨å¤§é‡è®¡ç®—èµ„æº
  â†’ æ·±å±‚å¯ç”¨å®¹é‡å—é™

Engram æ¨¡å‹çš„æµ…å±‚ï¼ˆLayer 1-5ï¼‰ï¼š
  ç›´æ¥æŸ¥è¡¨è·å¾— Paris
  â†’ å‡ ä¹ä¸å ç”¨è®¡ç®—èµ„æº
  â†’ æ·±å±‚æœ‰å……è¶³å®¹é‡

ç±»æ¯”ï¼š
  MoE: 
    å‰ 10 å±‚åœ¨"æƒ³"ç­”æ¡ˆ
    å 2 å±‚åšæ¨ç†
  
  Engram:
    å‰ 5 å±‚å°±"çŸ¥é“"ç­”æ¡ˆï¼ˆæŸ¥è¡¨ï¼‰
    å 7 å±‚éƒ½åœ¨åšæ¨ç†
    
  â†’ æœ‰æ•ˆæ¨ç†æ·±åº¦ï¼š2 å±‚ vs 7 å±‚ï¼
```

### 5.3 æ”¹è¿›æ€»ç»“

#### ç›¸æ¯” Dense Transformer

```
æ”¹è¿›ç‚¹ï¼š
  1. å‚æ•°æ•ˆç‡ï¼š27B å‚æ•°ï¼Œ2B æ¿€æ´» â†’ 13Ã— æ•ˆç‡
  2. çŸ¥è¯†å­˜å‚¨ï¼šæ˜¾å¼æŸ¥è¡¨ vs éšå¼åˆ†å¸ƒ
  3. è®¿é—®é€Ÿåº¦ï¼šO(1) vs O(n)
  4. å¯æ‰©å±•æ€§ï¼šå¯æ‰©å±•åˆ°åƒäº¿æ§½ä½
  5. ç³»ç»Ÿä¼˜åŒ–ï¼šæ”¯æŒé¢„å–ã€å¤šçº§ç¼“å­˜

æ€§èƒ½æå‡ï¼š
  å…¨ä»»åŠ¡å¹³å‡ +3~5 åˆ†
  é•¿ä¸Šä¸‹æ–‡ +12.8 åˆ†ï¼ˆçªç ´æ€§ï¼‰
```

#### ç›¸æ¯” MoE Transformer

```
æ”¹è¿›ç‚¹ï¼š
  1. æ–°å¢ç¨€ç–ç»´åº¦ï¼šæ¡ä»¶è®°å¿†ï¼ˆç¬¬äºŒç»´åº¦ï¼‰
  2. èŒè´£åˆ†ç¦»ï¼šçŸ¥è¯†å­˜å‚¨ vs æ¨ç†è®¡ç®—
  3. æµ…å±‚ä¼˜åŒ–ï¼šç›´æ¥æŸ¥è¡¨ï¼Œæ— éœ€å‰å‘ä¼ æ’­
  4. æ·±å±‚å¢å¼ºï¼šæœ‰æ•ˆæ·±åº¦ +5 å±‚
  5. æ³¨æ„åŠ›é‡Šæ”¾ï¼šä¸éœ€è¦å¤„ç†å±€éƒ¨æ¨¡å¼

æ€§èƒ½æå‡ï¼š
  åœ¨ç­‰å‚æ•°ã€ç­‰ FLOP æ¡ä»¶ä¸‹
  å…¨é¢è¶…è¶Šçº¯ MoE baseline
  è¯æ˜äº† U å‹æ‰©å±•è§„å¾‹
```

### 5.4 æœªæ¥å±•æœ›

#### 1. æ›´å¤šç¨€ç–åŒ–ç»´åº¦

```
å·²æœ‰ç»´åº¦ï¼š
  ç»´åº¦ 1: æ¡ä»¶è®¡ç®—ï¼ˆMoEï¼‰
  ç»´åº¦ 2: æ¡ä»¶è®°å¿†ï¼ˆEngramï¼‰

æ½œåœ¨ç»´åº¦ï¼š
  ç»´åº¦ 3: æ¡ä»¶æ³¨æ„åŠ›ï¼ˆç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼‰
  ç»´åº¦ 4: æ¡ä»¶æ·±åº¦ï¼ˆåŠ¨æ€å±‚æ•°è°ƒæ•´ï¼‰
  ç»´åº¦ 5: æ¡ä»¶æ¨¡æ€ï¼ˆå¤šæ¨¡æ€é€‰æ‹©æ€§æ¿€æ´»ï¼‰

â†’ å¤šç»´åº¦ååŒä¼˜åŒ–çš„ç©ºé—´å·¨å¤§ï¼
```

#### 2. ç¡¬ä»¶ååŒè®¾è®¡

```
ä¸“ç”¨ç¡¬ä»¶ï¼š
  - MoE åŠ é€Ÿå™¨ï¼ˆä¸“å®¶è·¯ç”±ä¼˜åŒ–ï¼‰
  - Engram æŸ¥æ‰¾å¼•æ“ï¼ˆå“ˆå¸ŒæŸ¥è¡¨åŠ é€Ÿï¼‰
  - å¤šçº§ç¼“å­˜å±‚æ¬¡ï¼ˆHBM + DRAM + SSDï¼‰

è½¯ç¡¬ååŒï¼š
  - é¢„æµ‹æ€§é¢„å–ï¼ˆæå‰åŠ è½½æ§½ä½ï¼‰
  - åŠ¨æ€ç¼“å­˜ç®¡ç†ï¼ˆçƒ­æ§½ä½ä¿ç•™ï¼‰
  - å¼‚æ„è®¡ç®—ç¼–æ’ï¼ˆCPU + GPU + ä¸“ç”¨èŠ¯ç‰‡ï¼‰
```

#### 3. è®­ç»ƒæ–¹æ³•åˆ›æ–°

```
æŒ‘æˆ˜ï¼š
  - MoE è´Ÿè½½å‡è¡¡
  - Engram å†·æ§½ä½åˆ©ç”¨
  - ä¸¤è€…ååŒè®­ç»ƒ

æ–¹å‘ï¼š
  - æ¸è¿›å¼è®­ç»ƒï¼ˆå…ˆ MoEï¼Œå†åŠ  Engramï¼‰
  - è”åˆä¼˜åŒ–ç›®æ ‡ï¼ˆå¹³è¡¡ä¸¤ä¸ªç»´åº¦ï¼‰
  - çŸ¥è¯†è’¸é¦ï¼ˆä» Dense åˆ° Sparseï¼‰
```

#### 4. åº”ç”¨åœºæ™¯æ‰©å±•

```
ç‰¹åˆ«é€‚åˆçš„åœºæ™¯ï¼š
  âœ“ çŸ¥è¯†å¯†é›†å‹åº”ç”¨ï¼ˆQAã€æœç´¢ï¼‰
  âœ“ é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆæ–‡æ¡£åˆ†æï¼‰
  âœ“ å¤šè¯­è¨€æ¨¡å‹ï¼ˆè¯­è¨€-ç‰¹å®šæ¨¡å¼ï¼‰
  âœ“ é¢†åŸŸä¸“å®¶ç³»ç»Ÿï¼ˆåŒ»ç–—ã€æ³•å¾‹ï¼‰

æ½œåœ¨åº”ç”¨ï¼š
  - çŸ¥è¯†å›¾è°±èåˆ
  - æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ›¿ä»£
  - æŒç»­å­¦ä¹ ï¼ˆå¢é‡æ›´æ–°æ§½ä½ï¼‰
```

---

## æ€»ç»“ï¼šä¸‰ä»£æ¶æ„çš„æ¼”è¿›é€»è¾‘

### æ ¸å¿ƒæ¡ˆä¾‹å›é¡¾

è®©æˆ‘ä»¬æœ€åå›é¡¾ä¸€ä¸‹ **"What is the capital of France?"** åœ¨ä¸‰ä»£æ¶æ„ä¸­çš„å®Œæ•´æ¼”è¿›ï¼š

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜ï¼š"What is the capital of France?"
æœŸæœ›ï¼š"Paris"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“… 2017: Dense Transformer

  å¤„ç†æ–¹å¼ï¼š
    Layer 1-12: æ¯å±‚ç”¨å…¨éƒ¨å‚æ•°é€æ­¥"å›å¿†"
    
  è®¡ç®—æˆæœ¬ï¼š
    12 å±‚ Ã— 100% å‚æ•° = 1200%
    
  é—®é¢˜ï¼š
    âŒ æ‰€æœ‰å‚æ•°éƒ½å‚ä¸ï¼ˆæµªè´¹ï¼‰
    âŒ éœ€è¦ 12 å±‚æ‰èƒ½"æƒ³èµ·"Paris
    âŒ çŸ¥è¯†éšå¼åˆ†æ•£åœ¨æƒé‡ä¸­
    
  ç»“æœï¼š
    âœ“ èƒ½å›ç­”ï¼Œä½†æ•ˆç‡ä½
    
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“… 2022: MoE Transformer

  å¤„ç†æ–¹å¼ï¼š
    Layer 1-12: æ¯å±‚é€‰æ‹©ä¸“å®¶ï¼Œç¨€ç–æ¿€æ´»
    "capital" â†’ åœ°ç†ä¸“å®¶
    "France" â†’ æ¬§æ´²ä¸“å®¶
    
  è®¡ç®—æˆæœ¬ï¼š
    12 å±‚ Ã— 3% å‚æ•° = 36%
    
  æ”¹è¿›ï¼š
    âœ… å‚æ•°æ•ˆç‡å¤§å¹…æå‡ï¼ˆ30Ã— æ•ˆç‡ï¼‰
    âœ… ä¸“å®¶è‡ªåŠ¨ä¸“ä¸šåŒ–
    
  ä»å­˜åœ¨ï¼š
    âŒ ä»éœ€ 12 å±‚"å›å¿†"äº‹å®
    âŒ çŸ¥è¯†ä»éšå¼å­˜å‚¨
    
  ç»“æœï¼š
    âœ“ æ›´é«˜æ•ˆï¼Œä½†æµ…å±‚ä»å¿™
    
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“… 2026: MoE + Engram

  å¤„ç†æ–¹å¼ï¼š
    Layer 1-3: æµ…å±‚ MoE åŸºç¡€å¤„ç†
    Layer 4: ğŸ”¥ Engram æŸ¥è¡¨
             hash("capital of France") â†’ #1234567
             Memory[#1234567] â†’ [Paris å…ˆéªŒ]
    Layer 5-12: æ·±å±‚ MoE ä¸“æ³¨æ¨ç†
    
  è®¡ç®—æˆæœ¬ï¼š
    33% + O(1) æŸ¥è¡¨
    
  çªç ´ï¼š
    âœ… O(1) çŸ¥è¯†è®¿é—®ï¼ˆæŸ¥è¡¨ï¼‰
    âœ… æµ…å±‚è´Ÿæ‹…å¤§å‡
    âœ… æ·±å±‚å®¹é‡å¢åŠ  (+5 å±‚)
    âœ… çŸ¥è¯†-æ¨ç†åˆ†ç¦»
    
  ç»“æœï¼š
    âœ“ ç¬¬ 5 å±‚å°±ç¡®å®šç­”æ¡ˆ
    âœ“ åç»­ 7 å±‚åšå¤æ‚æ¨ç†
    âœ“ å…¨é¢è¶…è¶Šå‰ä¸¤ä»£ï¼âœ¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### æ¼”è¿›è„‰ç»œ

```
2017: Dense Transformer
  æ ¸å¿ƒï¼šå…¨å‚æ•°å‰å‘ä¼ æ’­
  é—®é¢˜ï¼šå‚æ•°-è®¡ç®—çº¿æ€§è€¦åˆï¼Œæ— æ³•æŒç»­æ‰©å±•
  ä¾‹å­ï¼š12 å±‚é€æ­¥"å›å¿†"Paris
  
  â†“ çªç ´ï¼šç¨€ç–æ¿€æ´»
  
2022: MoE Transformer
  æ ¸å¿ƒï¼šæ¡ä»¶è®¡ç®—ï¼ˆç¬¬ä¸€ä¸ªç¨€ç–ç»´åº¦ï¼‰
  çªç ´ï¼šå‚æ•°-è®¡ç®—è§£è€¦ï¼Œ10-20Ã— æ•ˆç‡æå‡
  ä¾‹å­ï¼š12 å±‚ä¸“å®¶ç¨€ç–"å›å¿†"
  å±€é™ï¼šçŸ¥è¯†ä»éœ€å‰å‘ä¼ æ’­è®¿é—®ï¼Œæµ…å±‚è´Ÿæ‹…é‡
  
  â†“ çªç ´ï¼šçŸ¥è¯†æŸ¥æ‰¾
  
2026: MoE + Engram
  æ ¸å¿ƒï¼šæ¡ä»¶è®¡ç®— + æ¡ä»¶è®°å¿†ï¼ˆä¸¤ä¸ªç»´åº¦ï¼‰
  çªç ´ï¼šçŸ¥è¯†-æ¨ç†åˆ†ç¦»ï¼ŒO(1) æŸ¥è¡¨è®¿é—®
  ä¾‹å­ï¼šæŸ¥è¡¨è·å¾—çŸ¥è¯†ï¼Œ7 å±‚æ·±åº¦æ¨ç†
  åˆ›æ–°ï¼šU å‹æ‰©å±•è§„å¾‹ï¼Œè¯æ˜ä¸¤ç»´åº¦äº’è¡¥
  
  â†’ ä¸‹ä¸€ä»£ç¨€ç–å¤§æ¨¡å‹çš„æ ‡å‡†æ¶æ„ï¼âœ¨
```

### å…³é”®æ´å¯Ÿ

#### 1. ç¨€ç–åŒ–æ˜¯å¤šç»´åº¦çš„

```
é—®ï¼š"capital of France"

ä¸€ç»´ç¨€ç–ï¼ˆMoEï¼‰ï¼š
  åªé€‰æ‹©ç›¸å…³ä¸“å®¶è®¡ç®—
  â†’ 3% vs 100% å‚æ•°
  â†’ ä½†ä»éœ€ 12 å±‚"å›å¿†"

äºŒç»´ç¨€ç–ï¼ˆMoE + Engramï¼‰ï¼š
  è®¡ç®—ï¼šé€‰æ‹©ç›¸å…³ä¸“å®¶
  è®°å¿†ï¼šç›´æ¥æŸ¥è¡¨è·å–
  â†’ 33% å‚æ•° + O(1) æŸ¥è¡¨
  â†’ 5 å±‚å°±ç¡®å®šç­”æ¡ˆï¼âœ¨

æœªæ¥ï¼šå¯èƒ½æœ‰æ›´å¤šç»´åº¦
  - æ¡ä»¶æ³¨æ„åŠ›ï¼ˆç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼‰
  - æ¡ä»¶æ·±åº¦ï¼ˆåŠ¨æ€å±‚æ•°ï¼‰
  - æ¡ä»¶æ¨¡æ€ï¼ˆå¤šæ¨¡æ€é€‰æ‹©ï¼‰
```

#### 2. çŸ¥è¯†ä¸æ¨ç†åº”åˆ†ç¦»

```
ä¼ ç»Ÿæ–¹å¼ï¼ˆDense/MoEï¼‰ï¼š
  çŸ¥è¯†å’Œæ¨ç†æ··åœ¨ä¸€èµ·
  
  "capital of France" çš„å¤„ç†ï¼š
    Layer 1-10: åŠªåŠ›"å›å¿†"Parisï¼ˆçŸ¥è¯†ï¼‰
    Layer 11-12: ç»„ç»‡è¾“å‡ºï¼ˆæ¨ç†ï¼‰
    â†’ çŸ¥è¯†å ç”¨ 83% çš„å±‚ï¼

Engram æ–¹å¼ï¼š
  çŸ¥è¯†å’Œæ¨ç†æ˜ç¡®åˆ†ç¦»
  
  "capital of France" çš„å¤„ç†ï¼š
    Layer 4: æŸ¥è¡¨è·å¾— Parisï¼ˆçŸ¥è¯†ï¼‰
    Layer 5-12: æ¨ç†ã€ç»„ç»‡ï¼ˆæ¨ç†ï¼‰
    â†’ æ¨ç†å ç”¨ 67% çš„å±‚ï¼
    
  â†’ èŒè´£æ¸…æ™°ï¼Œå„å¸å…¶èŒï¼âœ¨
```

#### 3. O(1) è®¿é—®æ˜¯å…³é”®

```
é—®ï¼š"capital of France"

å‰å‘ä¼ æ’­ï¼ˆDense/MoEï¼‰ï¼š
  è¾“å…¥ â†’ Layer 1 â†’ Layer 2 â†’ ... â†’ Layer 12 â†’ ç­”æ¡ˆ
  å¤æ‚åº¦ï¼šO(n)ï¼Œn = å±‚æ•°
  æ—¶é—´ï¼šéœ€è¦ 12 æ¬¡çŸ©é˜µä¹˜æ³•
  
å“ˆå¸ŒæŸ¥è¡¨ï¼ˆEngramï¼‰ï¼š
  è¾“å…¥ â†’ hash â†’ æ§½ä½ç´¢å¼• â†’ ç›´æ¥è¯»å– â†’ ç­”æ¡ˆ
  å¤æ‚åº¦ï¼šO(1)ï¼Œä¸€æ¬¡æŸ¥æ‰¾
  æ—¶é—´ï¼šä¸€æ¬¡å“ˆå¸Œ + ä¸€æ¬¡å†…å­˜è¯»å–
  
å·®å¼‚ï¼š
  12 æ¬¡çŸ©é˜µä¹˜æ³• vs 1 æ¬¡å“ˆå¸ŒæŸ¥æ‰¾
  â†’ æ•°é‡çº§çš„æ•ˆç‡æå‡ï¼ğŸš€
```

#### 4. ç³»ç»Ÿä¼˜åŒ–åŒæ ·é‡è¦

```
"capital of France" çš„ç³»ç»Ÿä¼˜åŒ–ï¼š

ç®—æ³•å±‚é¢ï¼š
  âœ“ å¤šå¤´å“ˆå¸Œï¼šå‡å°‘å†²çª
  âœ“ ä¸Šä¸‹æ–‡é—¨æ§ï¼šåŠ¨æ€æ¶ˆæ­§
  âœ“ è½»é‡å·ç§¯ï¼šç²¾ç‚¼è¡¨ç¤º

ç³»ç»Ÿå±‚é¢ï¼š
  âœ“ é¢„å–ï¼šæå‰çŸ¥é“è¦è®¿é—® #1234567
  âœ“ ç¼“å­˜ï¼šé«˜é¢‘æ§½ä½æ”¾åœ¨ GPU
  âœ“ å¼‚æ„ï¼šMoE åœ¨ GPUï¼ŒEngram å¯åœ¨ DRAM

ç¡¬ä»¶å±‚é¢ï¼š
  âœ“ ä¸“ç”¨æŸ¥æ‰¾å¼•æ“
  âœ“ å¤šçº§ç¼“å­˜å±‚æ¬¡
  âœ“ CPU+GPU ååŒ

â†’ è½¯ç¡¬ä»¶ååŒä¼˜åŒ–ï¼âœ¨
```

### æœ€ç»ˆå¯¹æ¯”å›¾ï¼šä¸‰ä»£æ¶æ„å¤„ç† "capital of France"

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¾“å…¥ï¼š"What is the capital of France?"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ã€Dense Transformer - 2017ã€‘

Layer 1:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (è¿˜ä¸çŸ¥é“...)
Layer 2:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (åœ¨æ‘¸ç´¢...)
Layer 3:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (ç»§ç»­æƒ³...)
Layer 4:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (å¥½åƒæ˜¯...)
Layer 5:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (Paris?...)
...
Layer 12: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% å‚æ•° (ç¡®å®šäº†!)

è®¡ç®—é‡ï¼š1200%  |  ç­”æ¡ˆç¡®å®šå±‚ï¼šç¬¬ 12 å±‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ã€MoE Transformer - 2022ã€‘

Layer 1:  â–ˆâ–ˆ 3% ä¸“å®¶ (åœ°ç†ç›¸å…³...)
Layer 2:  â–ˆâ–ˆ 3% ä¸“å®¶ (æ³•å›½ç›¸å…³...)
Layer 3:  â–ˆâ–ˆ 3% ä¸“å®¶ (ç»§ç»­æƒ³...)
Layer 4:  â–ˆâ–ˆ 3% ä¸“å®¶ (Paris?...)
...
Layer 12: â–ˆâ–ˆ 3% ä¸“å®¶ (ç¡®å®šäº†!)

è®¡ç®—é‡ï¼š36%   |  ç­”æ¡ˆç¡®å®šå±‚ï¼šç¬¬ 12 å±‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ã€MoE + Engram - 2026ã€‘

Layer 1:  â–ˆâ–ˆ 3% ä¸“å®¶ (åŸºç¡€å¤„ç†)
Layer 2:  â–ˆâ–ˆ 3% ä¸“å®¶ (ç†è§£é—®å¥)
Layer 3:  â–ˆâ–ˆ 3% ä¸“å®¶ (å‡†å¤‡æŸ¥è¯¢)
Layer 4:  ğŸ” Engram æŸ¥è¡¨ï¼
          hash("capital of France") â†’ Memory[#1234567]
          â†’ [Paris å…ˆéªŒçŸ¥è¯†]  âœ¨ O(1)!
Layer 5:  â–ˆâ–ˆ 3% ä¸“å®¶ (å·²çŸ¥é“ Paris!)
Layer 6:  â–ˆâ–ˆ 3% ä¸“å®¶ (ç»„ç»‡ç­”æ¡ˆ)
...
Layer 12: â–ˆâ–ˆ 3% ä¸“å®¶ (å®Œç¾è¾“å‡º)

è®¡ç®—é‡ï¼š33% + O(1)  |  ç­”æ¡ˆç¡®å®šå±‚ï¼šç¬¬ 5 å±‚ âš¡

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å¯¹æ¯”ï¼š
  Dense:  12 å±‚å›å¿†ï¼Œ1200% è®¡ç®—
  MoE:    12 å±‚å›å¿†ï¼Œ36% è®¡ç®—
  Engram: 1 æ¬¡æŸ¥è¡¨ + 8 å±‚æ¨ç†ï¼Œ33% è®¡ç®—

å…³é”®ï¼š
  Engram ç”¨ 1 æ¬¡æŸ¥è¡¨æ›¿ä»£äº† 10 å±‚"å›å¿†"
  â†’ é‡Šæ”¾å‡ºçš„æ·±åº¦ç”¨äºå¤æ‚æ¨ç†ï¼âœ¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### DeepSeek çš„è´¡çŒ® â­

> "æ¡ä»¶è®°å¿†å°†æˆä¸ºä¸‹ä¸€ä»£ç¨€ç–å¤§æ¨¡å‹ä¸­ä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒå»ºæ¨¡åŸè¯­ã€‚"

**ä¸ºä»€ä¹ˆé‡è¦**ï¼ˆç”¨ "capital of France" ä¾‹å­è¯´æ˜ï¼‰ï¼š

1. **ç†è®ºè´¡çŒ®**ï¼š
   - æå‡ºç¬¬äºŒä¸ªç¨€ç–åŒ–ç»´åº¦ï¼ˆæ¡ä»¶è®°å¿†ï¼‰
   - å‘ç° U å‹æ‰©å±•è§„å¾‹ï¼ˆ80:20 é»„é‡‘æ¯”ä¾‹ï¼‰
   - è¯æ˜ä¸¤ç»´åº¦äº’è¡¥æ€§ï¼ˆçŸ¥è¯†æŸ¥æ‰¾ + æ¨ç†è®¡ç®—ï¼‰
   
   å®ä¾‹ï¼šå¤„ç† "capital of France" æ—¶ï¼ŒæŸ¥è¡¨ï¼ˆEngramï¼‰æ¯”è®¡ç®—ï¼ˆMoEï¼‰æ›´é«˜æ•ˆ

2. **å·¥ç¨‹è´¡çŒ®**ï¼š
   - å¼€æºå®Œæ•´å®ç°ï¼ˆGitHub ä»£ç å¯è¿è¡Œï¼‰
   - æä¾›å®è·µæŒ‡å¯¼ï¼ˆ80% MoE + 20% Engramï¼‰
   - éªŒè¯ç³»ç»Ÿå¯è¡Œæ€§ï¼ˆé¢„å–ã€ç¼“å­˜ç­‰ä¼˜åŒ–ï¼‰
   
   å®ä¾‹ï¼šæ§½ä½ #1234567 å¯ä»¥è¢«é¢„å–åˆ° GPUï¼ŒæŸ¥æ‰¾å‡ ä¹æ— å»¶è¿Ÿ

3. **äº§ä¸šå½±å“**ï¼š
   - æŒ‘æˆ˜çº¯ MoE èŒƒå¼ï¼ˆè¯æ˜çº¯ MoE æ¬¡ä¼˜ï¼‰
   - æŒ‡æ˜æœªæ¥æ¼”è¿›æ–¹å‘ï¼ˆå¤šç»´åº¦ç¨€ç–åŒ–ï¼‰
   - é™ä½è®­ç»ƒæˆæœ¬ï¼ˆ557ä¸‡ç¾å…ƒè®­ç»ƒ DeepSeek-V3ï¼‰
   
   å®ä¾‹ï¼šç›¸åŒé¢„ç®—ä¸‹ï¼ŒEngram æ¯”çº¯ MoE æ€§èƒ½æ›´å¥½ï¼ˆBBH +5.0ï¼‰

---

## å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. **Transformer**
   - [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., NeurIPS 2017)

2. **MoE ç³»åˆ—**
   - [Outrageously Large Neural Networks](https://arxiv.org/abs/1701.06538) (Shazeer et al., 2017)
   - [GShard: Scaling Giant Models](https://arxiv.org/abs/2006.16668) (Lepikhin et al., 2020)
   - [Switch Transformers](https://arxiv.org/abs/2101.03961) (Fedus et al., 2021)

3. **Engram**
   - [Conditional Memory via Scalable Lookup](https://github.com/deepseek-ai/Engram) (DeepSeek & åŒ—äº¬å¤§å­¦, 2026)
   - è®ºæ–‡ PDF: [Engram_paper.pdf](https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf)

4. **FFN çŸ¥è¯†å­˜å‚¨**
   - [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) (Geva et al., 2021)

### å¼€æºå®ç°

- DeepSeek Engram: [https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)
- Mixtral 8Ã—7B: [https://huggingface.co/mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)

### æŠ€æœ¯åšå®¢

- DeepSeek-V3 æŠ€æœ¯æŠ¥å‘Š: [å®˜æ–¹é“¾æ¥å¾…è¡¥å……]
- Mixtral å®˜æ–¹åšå®¢: [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/)

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**åˆ›å»ºæ—¶é—´**ï¼š2026-01-13  
**æ›´æ–°æ—¶é—´**ï¼š2026-01-13  
**ä½œè€…**ï¼šç’‡ç‘ âœ¨
