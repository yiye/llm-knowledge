# LLM 训练知识体系 🌳

> 📘 从数学基础到工程实践，**以模型为中心的双向探索**

[![更新时间](https://img.shields.io/badge/最后更新-2026年1月-blue.svg)]()
[![版本](https://img.shields.io/badge/版本-v3.0-brightgreen.svg)]()

---

## 🎯 项目特色

本知识库采用**创新的双向探索结构**：

```
                【向上：训练方法】
                       ↑
          工程实践（成本、场景、趋势）
                       ↑
          对齐与推理（DPO、推理强化）
                       ↑
    ┌─────────────────────────────────┐
    │   🏗️ LLM 模型本质（中心）        │
    │   先理解模型是什么！              │
    └─────────────────────────────────┘
                       ↓
          表示学习（词向量、分布式表示）
                       ↓
          点积与相似度（Attention 基础）
                       ↓
          数学基础（概率、线性代数）
                       
              【向下：数学原理】
```

**核心理念**：
- ✅ 先理解 LLM 是什么（中心层 L0）
- ✅ 向下探索：为什么这样设计？（数学原理）
- ✅ 向上探索：如何训练模型？（训练方法）
- ✅ 每个概念都讲清楚**本质**和**与 LLM 的关系**

---

## 使用方法

将仓库 clone 下来使用 cursor、qcoder 打开， 阅读文章， 遇到不动的疑惑的地方，选择，直接问题 LLM .

## 🗺️ 完整知识树

### 📋 知识库结构

本知识库以 **LLM 模型本质**为核心：
- **向下探索**：深入数学与理论基础（为什么这样设计？）
- **向上探索**：各种训练方法（如何训练模型？）

**学习建议**：
- 🎯 **初学者**：从 L0 开始 → 根据需要向下补充基础 → 向上学习训练
- 🚀 **实践者**：快速浏览 L0 → 直接跳到 L+1/L+2 学习训练方法
- 🔬 **研究者**：从 L0 → 向下深挖数学原理 → 理解训练方法的本质

---

## 🏗️ L0：LLM 模型本质（中心层，必读）⭐⭐⭐

**核心问题**：LLM 到底是什么？Transformer 如何工作？参数存储了什么？

**核心内容**：
```
L0-1. 数学本质
  └─ P(next_token | context) - 概率模型
  
L0-2. Transformer 架构
  └─ Attention(Q,K,V) - 核心机制
  
L0-3. 参数存储
  └─ 压缩的语言知识和世界知识
  
L0-4. 能力演进
  └─ PreTrain → SFT → DPO → Reasoning
```

**对应文档**：
- 📄 [L0-llm-model-essence.md](./L0-llm-model-essence.md)（938 行，详细讲解）

**学习要点**：
- 🔥 理解 L0 是理解整个知识树的关键
- 🔥 先建立整体认知，再向下深挖或向上学习
- 🔥 LLM = 架构 + 参数，能力来自训练

---

## 📐 向下探索：为什么这样设计？

### L-1：数学基础 📊

**核心问题**：理解 LLM 需要哪些数学知识？

**核心内容**：
```
L-1-1. 概率论
  ├─ 条件概率、期望、方差
  └─ 信息论：交叉熵、KL 散度

L-1-2. 马尔科夫链
  └─ 与语言模型的关系

L-1-3. 线性代数
  ├─ 向量、矩阵
  └─ 点积、转置、乘法

L-1-4. 与 LLM 的关系
  └─ 每个概念都有应用场景
```

**对应文档**：
- 📄 [L-1-math-foundations.md](./L-1-math-foundations.md)（802 行）

**依赖关系**：
- ⬆️ 支撑所有上层（数学基础）

**学习建议**：不必全部记住，遇到不懂的公式再回来查

---

### L-2：点积与相似度 🔍

**核心问题**：Attention 为什么用点积？

**核心内容**：
```
L-2-1. 点积的几何意义 ⭐
  └─ 代数定义 = 几何定义

L-2-2. 余弦相似度
  └─ cos(θ) = A·B / (|A||B|)

L-2-3. 为什么 Attention 用点积 ⭐⭐⭐
  ├─ 计算高效
  ├─ 几何意义清晰
  ├─ 完美并行化
  ├─ 梯度特性好
  └─ 实践验证

L-2-4. 相似度应用
  └─ Attention、检索、聚类
```

**对应文档**：
- 📄 [L-2-dot-product-similarity.md](./L-2-dot-product-similarity.md)（772 行）

**依赖关系**：
- ⬆️ 支撑 L0（Attention(Q,K,V) 公式）
- ⬇️ 依赖 L-1（向量、余弦）

---

### L-3：表示学习基础 🎯

**核心问题**：为什么要把文字变成向量？

**核心内容**：
```
L-3-1. 三种方案对比
  ├─ 简单编号：有问题
  ├─ One-Hot：维度爆炸
  └─ 词向量：编码语义 ✅

L-3-2. 词向量学习
  ├─ Word2Vec（2013）
  ├─ GloVe（2014）
  └─ Transformer Embedding

L-3-3. 神奇特性
  ├─ 向量算术：king - man + woman ≈ queen
  ├─ 聚类现象
  └─ 降维可视化

L-3-4. 分布式表示 ⭐
  └─ 性别分散在多个维度（维度纠缠）
```

**对应文档**：
- 📄 [L-3-representation-learning.md](./L-3-representation-learning.md)（700 行）

**依赖关系**：
- ⬆️ 支撑 L0（Embedding 层为什么这样设计）
- ⬇️ 依赖 L-1、L-2（数学基础、点积相似度）

---

## 🚀 向上探索：如何训练模型？

### L+1：预训练与微调 🔧

**核心问题**：各种训练到底在训练什么？

#### L+1-1. 预训练（PreTrain）⭐

**训练的是什么？**
```python
# 训练目标
Loss = -log P(token_t | token_1, ..., token_t-1)

# 在训练什么？
✅ 语言规律：语法、句法、语义
✅ 世界知识：事实、常识、关系  
✅ 基础推理：简单的逻辑和因果

# 怎么训练？
输入："猫在___"
模型预测："睡觉" (0.6), "玩耍" (0.3), ...
实际答案："睡觉" ✅
→ 调整参数，让 P("睡觉") 更大
```

**数据规模**：万亿 tokens（TB 级）  
**训练成本**：557.6万美元（[DeepSeek-V3 技术报告](https://arxiv.org/abs/2412.19437), 2024.12）  
**学到什么**：从零到"会说话"

#### L+1-2. 中期训练（Mid-Training）🆕

**训练的是什么？**
```
在 PreTrain 基础上：
  ✅ 领域适应：医疗、法律、金融专业知识
  ✅ 上下文扩展：32K → 128K → 1M tokens
  ✅ 知识更新：最新的时效性数据

训练目标：还是 Next Token Prediction
但数据不同：精选的领域数据
```

**数据规模**：百万到数十亿 tokens  
**训练成本**：几万到十几万美元  
**学到什么**：从"通用"到"专业"

#### L+1-3. 指令微调（SFT）⭐

**训练的是什么？**
```python
# 训练目标
Loss = -log P(output | instruction)

# 在训练什么？
✅ 理解人类意图（指令理解）
✅ 遵循指令格式（结构化输出）
✅ 任务泛化能力（举一反三）

# 怎么训练？
输入："请用一句话总结以下文章：..."
期望输出："文章主要讲述了..."
→ 调整参数，让模型生成期望输出
```

**数据格式**：
```json
{
  "instruction": "将以下句子翻译成英文",
  "input": "猫在睡觉",
  "output": "The cat is sleeping"
}
```

**数据规模**：几万到几十万条  
**训练成本**：几千到几万美元  
**学到什么**：从"续写机器"到"对话助手"

**对应文档**：
- 📄 [L+1-pretraining-and-finetuning.md](./L+1-pretraining-and-finetuning.md)

**依赖关系**：
- ⬇️ 依赖 L0（需要先有 Transformer 模型）
- ⬆️ 支撑 L+2（SFT 模型是对齐的起点）

---

### L+2：对齐与推理 ⚡

**核心问题**：如何让模型符合人类偏好？如何让模型会思考？

#### L+2-1. 偏好对齐的本质

**训练的是什么？**
```
问题：SFT 模型"能用"但不一定"好用"
  同一个问题可能有多种回答
  哪个回答更好？→ 需要符合人类偏好

训练目标：
  ✅ 有帮助（Helpful）
  ✅ 无害（Harmless）
  ✅ 诚实（Honest）
```

#### L+2-2. RLHF 方法（历史背景）

**训练的是什么？**
```python
# 分两阶段训练

阶段1：训练奖励模型 r(x, y)
  输入：(x, y_好, y_差)
  目标：r(x, y_好) > r(x, y_差)
  
阶段2：用 PPO 优化策略 π
  目标：max E[r(x, y)] - β·KL(π || π_ref)
  意思：最大化奖励，但不要偏离 SFT 模型太远
```

**问题**：复杂、不稳定、需要四个模型同时运行 ❌

#### L+2-3. DPO 方法（当前主流）⭐⭐⭐

**训练的是什么？**
```python
# 核心思想：跳过奖励模型，直接优化偏好

# DPO 损失函数（闭式解）
L_DPO = -E[log σ(β log π(y_w|x)/π_ref - β log π(y_l|x)/π_ref)]

# 直观理解
拉高好回答的概率：↑ π(y_w | x)
降低差回答的概率：↓ π(y_l | x)
保持接近参考模型：KL(π || π_ref) 隐式约束
```

**为什么 DPO 更好？**
```
✅ 简单：只需一个模型训练
✅ 稳定：没有 RL 的不稳定性
✅ 高效：计算成本大幅降低
✅ 效果相当：性能接近 RLHF
✅ 工业验证：Claude、Llama 3 都在用
```

**数据格式**：
```json
{
  "prompt": "如何学习编程？",
  "chosen": "循序渐进，多动手实践，从简单项目开始...",
  "rejected": "直接看源码，硬啃就行了"
}
```

#### L+2-4. 推理强化（Reasoning）

**训练的是什么？**
```
问题：SFT + DPO 适合日常对话
     但复杂推理任务（数学、代码）表现不佳

训练目标：
  ✅ 多步推理能力
  ✅ 中间步骤的正确性
  ✅ 自我纠错能力

核心方法：过程监督（Process Supervision）
  不只奖励最终答案 ❌
  而是奖励每一步推理 ✅
```

**训练数据**：
```json
{
  "question": "求 ∫ x² dx",
  "reasoning_chain": [
    "步骤1：使用幂函数积分公式",
    "步骤2：∫ x^n dx = x^(n+1)/(n+1) + C",
    "步骤3：代入 n=2，得 x³/3 + C"
  ],
  "answer": "x³/3 + C"
}
```

**训练成本**：极高（专家标注，$25-200/条）  
**适用场景**：数学、代码、逻辑推理（不适合日常对话）

**对应文档**：
- 📄 [L+2-alignment-rlhf-dpo.md](./L+2-alignment-rlhf-dpo.md)
- 📄 [L+2-dpo-practice.md](./L+2-dpo-practice.md)
- 📄 [L+2-reasoning.md](./L+2-reasoning.md)

**依赖关系**：
- ⬇️ 强依赖 L+1（需要先有 SFT 模型作为 π_ref）
- ⬇️ 依赖 L-3（DPO 推导需要概率论、KL 散度）
- ⬆️ 支撑 L+3（工程实践需要理解各种训练方法）

---

### L+3：工程实践与优化 🚀

**核心问题**：实际训练中如何选方案？如何降低成本？

#### L+3-1. 场景化训练方案 ⭐

```
场景1：大公司从零训练
  PreTrain (MoE + FP8) → Mid-Training → SFT → DPO
  成本：500万 - 2000万美元

场景2：小团队微调开源模型
  选开源模型 → SFT (高质量数据) → DPO (可选)
  成本：5万 - 20万美元

场景3：垂直领域（医疗/法律）
  选通用模型 → Mid-Training (必须) → 领域SFT → DPO
  成本：10万 - 50万美元

场景4：对话机器人
  SFT + DPO (必须，提升体验)
  不需要推理强化

场景5：数学/代码助手
  SFT + 推理强化 (核心差异化)
  DPO 可选
```

#### L+3-2. 成本优化技术

```
架构优化：
  ✅ MoE（混合专家）：节省 90% 成本
  ✅ FP8 训练：降低显存和计算
  ✅ Flash Attention：加速 2-4x

参数高效微调：
  ✅ LoRA：只训练低秩矩阵
  ✅ QLoRA：量化 + LoRA
  ✅ Prefix Tuning

数据策略：
  ✅ 质量 > 数量（2024最重要发现）
  ✅ 数据清洗与去重
  ✅ 主动学习（选最有价值的数据标注）
```

#### L+3-3. 训练阶段的本质差异

```
为什么不能混用数据格式？

PreTrain：无监督
  数据：原始文本
  目标：P(token_t | context)
  
SFT：监督学习
  数据：(指令, 输出) 对
  目标：P(output | instruction)
  
DPO：偏好学习
  数据：(指令, 好, 差) 三元组
  目标：P(好 ≻ 差 | 指令)

→ 数据结构由训练目标决定！
```

**对应文档**：
- 📄 [L+3-practical-guide.md](./L+3-practical-guide.md)
- 📄 [L+3-training-differences.md](./L+3-training-differences.md)
- 📄 [L+3-milestones.md](./L+3-milestones.md)
- 📄 [L+3-future-trends.md](./L+3-future-trends.md)

**依赖关系**：
- ⬇️ 需要理解 L+1, L+2 的所有训练方法
- ⬇️ 需要对整个体系（L-3 到 L+2）有全局认知

---

## 🔗 双向依赖关系图

### 这个图如何帮助你学习 LLM 训练？

**学习 LLM 训练最大的挑战**：知识点太多，不知道从哪里开始，不知道各个概念之间的关系。

**这个依赖图的作用**：
- ✅ **快速定位**：看到任意概念，知道它在整个体系中的位置
- ✅ **按需学习**：想学某个技术（如 DPO），立即知道需要哪些前置知识
- ✅ **追溯原理**：遇到不懂的概念，顺着依赖链向下查找
- ✅ **规划路径**：根据自己的目标选择学习方向（向下补基础 or 向上学训练）

**使用建议**：
1. **初学者**：从 L0（中心）开始，建立整体认知，再根据需要向下或向上探索
2. **实践者**：快速浏览 L0，直接跳到 L+1/L+2，遇到不懂的概念回头补 L-1/L-2/L-3
3. **研究者**：按照依赖链完整学习，理解每个设计背后的数学原理

**举例：想学 DPO 需要什么？**
- 向下追溯：L-3 的 KL 散度 → L-2 的点积 → L-1 的词向量 → L0 的 Transformer
- 向上追溯：需要先有 L+1 的 SFT 模型作为参考模型 π_ref

### 以 LLM 模型为中心的知识依赖

```
┌──────────────────────────────────────────────────────────────────────┐
│                     【以 L0 为中心的双向依赖】                         │
└──────────────────────────────────────────────────────────────────────┘

                        【向上：训练方法】
                               ↑
                  L+3: 工程实践（成本、场景、趋势）
                               ↑
                  L+2: 推理强化（思考能力）
                               ↑
                  L+2: DPO（偏好对齐）✨
                        ↑         ↑
                        |         └─ 需要 π_ref
                        |         └─ 需要 Bradley-Terry
                        |         └─ 需要 KL 散度（来自 L-3）
                        ↑
                  L+1: SFT（学会听话）
                        ↑
                  L+1: Mid-Training（领域适应）
                        ↑
                  L+1: PreTrain（学会语言）
                        ↑
    ┌────────────────────────────────────────────────────┐
    │         L0: LLM 模型本质（中心）                    │
    │                                                    │
    │  ├─ 模型 = P(next_token | context)                │
    │  ├─ Transformer 架构                               │
    │  │   └─ Attention(Q,K,V) = softmax(QK^T/√d_k)V   │
    │  │       ↑                                        │
    │  │       └─ 为什么用点积？（来自 L-2）             │
    │  ├─ Embedding 层                                   │
    │  │   └─ 文本 → 向量（来自 L-1）                   │
    │  └─ 参数 = 压缩的知识                              │
    └────────────────────────────────────────────────────┘
                        ↓
                  L-1: 表示学习
                        ├─ 为什么用词向量？
                        ├─ 词向量如何学习？
                        └─ 分布式表示的本质
                        ↓
                  L-2: 点积与相似度 ⭐
                        ├─ 点积的几何意义
                        ├─ 余弦相似度
                        └─ 为什么 Attention 用点积？
                        ↓    ↑
                        |    └─ 支撑 L0 的 Attention 设计
                        ↓
                  L-3: 数学基础
                        ├─ 概率论（P, E, Var）
                        ├─ 马尔科夫链
                        ├─ 线性代数（向量、矩阵）
                        └─ 信息论（KL 散度）
                        
                     【向下：数学与理论】

```

### 关键依赖路径：从 L0 到 DPO

```
┌──────────────────────────────────────────────────────────────────┐
│                  【理解 DPO 的最小依赖路径】                       │
└──────────────────────────────────────────────────────────────────┘

向下依赖（理论基础）：
  L-3: 条件概率 P(y|x)
       ↓
  L-3: 期望 E[...]、KL 散度
       ↓
  L-2: 点积 = 相似度
       ↓
  L-1: 词向量表示
       ↓
  L0: Transformer 模型 P(token | context)

向上依赖（训练过程）：
  L0: 有了模型
       ↓
  L+1: PreTrain → 学会语言
       ↓
  L+1: SFT → 学会听话 → 得到 π_ref
       ↓
  L+2: 收集偏好数据 (x, y_w, y_l)
       ↓
  L+2: Bradley-Terry 模型
       P(y_w ≻ y_l) = σ(r(y_w) - r(y_l))
       ↓
  L+2: RLHF 最优解（闭式）
       π*(y|x) ∝ π_ref(y|x) exp(r(x,y)/β)
       ↓
  L+2: 反解隐式奖励
       r(x,y) = β log(π*/π_ref)
       ↓
  L+2: DPO 损失函数 ✨
       L_DPO = -E[log σ(β log π/π_ref(y_w) - β log π/π_ref(y_l))]
       ↓
  L+2: 训练实践（反向传播、超参数）
       ↓
  L+3: 工程落地（成本优化、场景选择）
```

---

## 📊 知识点层级统计

| 层级 | 核心文档数 | 预计学习时间 | 难度 | 重要性 | 说明 |
|------|----------|------------|------|--------|------|
| **L0** | 1 | 2h | ⭐⭐⭐ | 🔥🔥🔥🔥🔥 | **中心层**，理解模型本质 |
| L-1 | 1 | 1.5h | ⭐⭐ | 🔥🔥🔥🔥 | 表示学习基础 |
| L-2 | 1 | 1.5h | ⭐⭐⭐ | 🔥🔥🔥🔥 | 点积与相似度 |
| L-3 | 1 | 1h（按需） | ⭐⭐ | 🔥🔥🔥 | 数学基础（按需查阅）|
| L+1 | 1 | 3h | ⭐⭐⭐ | 🔥🔥🔥🔥🔥 | 预训练与微调 |
| L+2 | 3 | 6-8h | ⭐⭐⭐⭐ | 🔥🔥🔥🔥🔥 | 对齐与推理（DPO核心）|
| L+3 | 4 | 2-3h | ⭐⭐ | 🔥🔥🔥🔥 | 工程实践 |

**总计**：12 个核心文档，17-21 小时完整学习  
**核心路径**：L0 → L+1 → L+2 → L+3（6-11 小时快速上手）

---

## 🔥 2024-2025 重大突破

| 时间 | 模型 | 突破点 | 意义 | 来源 |
|------|------|--------|------|------|
| 2024.12 | **DeepSeek-V3** | 训练成本仅 **557.6 万美元** | 证明高效训练的可能性 | [技术报告](https://arxiv.org/abs/2412.19437) |
| 2025.01 | **DeepSeek-R1** | 开源推理模型（MIT 许可） | 推理能力民主化 | [TechCrunch](https://techcrunch.com/2025/01/27/deepseek-claims-its-reasoning-model-beats-openais-o1-on-certain-benchmarks/) |
| 2025.09 | **Claude Sonnet 4.5** | 编码能力全球领先，SWE-bench SOTA | 代码 Agent 新标杆 | [Anthropic](https://www.anthropic.com/news/claude-sonnet-4-5) |
| 2025.11 | **Gemini 3 Pro** | 增强推理与多模态理解 | AI 深度集成搜索与应用 | [Google Blog](https://blog.google/technology/ai/google-ai-updates-november-2025/) |

---

## ⚠️ 常见误区

❌ **误区 1**："PreTrain 数据越多越好"  
✅ **正确**：质量 > 数量（DeepSeek-V3 验证）

❌ **误区 2**："必须先学数学才能学 LLM"  
✅ **正确**：先理解模型是什么（L0），按需补数学

❌ **误区 3**："训练就是调参"  
✅ **正确**：不同训练阶段训练的是**不同能力**

❌ **误区 4**："RLHF 一定比 DPO 好"  
✅ **正确**：DPO 更简单高效，已成主流

---

### ✅ 建议做的事

1. **从 L0 开始**：先理解模型本质，建立整体认知 🔥
2. **按需选择方向**：
   - 想理解原理 → 向下探索（L-1, L-2, L-3）
   - 想学习训练 → 向上探索（L+1, L+2, L+3）
3. **动手实践**：至少跑通一个 DPO 训练脚本
4. **画依赖图**：自己画一遍"L0 → DPO"的依赖链
5. **推导公式**：DPO 的数学推导至少手推一遍（研究者）
6. **场景对照**：对照自己的实际场景选择训练方案

### ❌ 避免的误区

1. **跳过 L0**：不知道模型是什么，后面都会很迷茫 🔥
2. **追求全面覆盖**：根据目标选择学习路径，不必学完所有层级
3. **只看结论不看推导**：理解"为什么"比记住"是什么"更重要
4. **忽视工程实践**：理论再好也要能落地
5. **混淆训练目标**：PreTrain、SFT、DPO 训练的是不同的能力

### 💡 核心洞察

```
最重要的认知转变：

❌ 旧认知："LLM 训练就是调参数"
✅ 新认知："不同训练阶段训练的是不同能力"

PreTrain    训练语言能力（语法、知识）
SFT         训练听话能力（指令理解）
DPO         训练价值观（符合人类偏好）
Reasoning   训练思考能力（多步推理）

→ 理解"训练什么"比"怎么训练"更重要
```

---

## 🤝 贡献指南

### 三大核心原则

1. **内容必须有依据** 🔥
   - 来自官方文档、论文、技术博客
   - 明确标注来源和时间
   - 禁止"据说"、"听说"等模糊表述

2. **技术方案确保新鲜度** 🔥
   - 2024-2026：✅ 可作为"当前主流"
   - 2023：⚠️ 需标注时间
   - 2022 及之前：⚠️ 标注为"历史背景"

3. **不要靠猜想生成** 🔥
   - 不确定就搜索验证
   - 找不到就承认未知
   - 宁可缺失也不要编造

详见 **[AGENTS.md](./AGENTS.md)** 中的完整工作规范。

### 知识树特有原则

4. **保持依赖关系清晰**
   - 每个层级的前置知识必须明确
   - 跨层引用需要标注依赖

5. **学习路径可验证**
   - 推荐的学习时间要基于实际测试
   - 难度评级要符合大多数学习者的反馈

6. **实战导向**
   - 理论必须能指导实践
   - 每个方法都要有适用场景说明

---

## 📮 反馈与贡献

欢迎提出改进建议！特别是：

- 📝 学习路径是否合理
- 🔗 依赖关系是否清晰
- ⏰ 学习时间估计是否准确
- 💡 是否有遗漏的重要知识点
- 🎯 实战方案是否有效

---

**记住**：理解"LLM 是什么"比"怎么训练"更重要。

先建立整体认知，再深挖细节，最后回到实践。✨

从**向量**到 **DPO**，每一步都有明确的依赖关系。掌握这个知识树，你就掌握了 LLM 训练的完整流程！

---

*最后更新：2026 年 1 月*
