# 07 - RAG ç³»ç»Ÿæ„å»ºæŒ‡å—ï¼šä»æ£€ç´¢åˆ°ç”Ÿæˆçš„å®Œæ•´é“¾è·¯

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šRAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯å½“å‰æœ€å®ç”¨çš„ LLM åº”ç”¨æ¨¡å¼ï¼æœ¬æ–‡å°†ä»é›¶æ„å»ºç”Ÿäº§çº§ RAG ç³»ç»Ÿï¼Œæ¶µç›–æ¶æ„è®¾è®¡ã€å‘é‡æ£€ç´¢ã€Rerankingã€åˆ°å®Œæ•´å®ç°ã€‚

---

## ğŸ“– å¼•è¨€ï¼šä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ

### âŒ çº¯ LLM çš„å±€é™

```
åœºæ™¯ï¼šç”¨ GPT-4 å›ç­”"2024å¹´ç¬¬å››å­£åº¦å…¬å¸è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"

é—®é¢˜ï¼š
  âŒ LLM çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼ˆå¦‚ 2023 å¹´ 4 æœˆï¼‰
  âŒ æ²¡æœ‰è®¿é—®å®æ—¶æ•°æ®çš„èƒ½åŠ›
  âŒ å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼ˆç¼–é€ ä¸å­˜åœ¨çš„æ•°æ®ï¼‰
  âŒ æ— æ³•å¼•ç”¨æ¥æº

ç»“æœï¼š
  "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ° 2023 å¹´ 4 æœˆï¼Œæ— æ³•å›ç­”ã€‚"
```

### âœ… RAG çš„è§£å†³æ–¹æ¡ˆ

æ ¹æ® [AWS RAG Best Practices](https://docs.aws.amazon.com/prescriptive-guidance/latest/writing-best-practices-rag/introduction.html)ï¼ˆ2025ï¼‰ï¼š

> RAG ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†æºï¼Œé€šè¿‡å°† AI ç³»ç»Ÿé”šå®šåœ¨æœ€æ–°ã€ç‰¹å®šé¢†åŸŸçš„ä¿¡æ¯ä¸Šï¼Œæä¾›æ›´å‡†ç¡®ã€äº‹å®æ€§çš„å“åº”ï¼Œè€Œæ— éœ€å¤§é‡å¾®è°ƒã€‚

**å·¥ä½œæµç¨‹**ï¼š

```
ç”¨æˆ·é—®é¢˜ï¼š"2024 Q4 è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
    â†“
1ï¸âƒ£ æ£€ç´¢ï¼ˆRetrievalï¼‰
   åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£
   æ‰¾åˆ°: "2024 Q4 è´¢æŠ¥.pdf" ç¬¬ 3 é¡µ
    â†“
2ï¸âƒ£ å¢å¼ºï¼ˆAugmentationï¼‰
   æŠŠæ£€ç´¢åˆ°çš„å†…å®¹æ‹¼æ¥åˆ° Prompt
   "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”ï¼š
    [2024 Q4 è¥æ”¶ä¸º 12.5 äº¿ç¾å…ƒ...]
    é—®é¢˜ï¼š2024 Q4 è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
    â†“
3ï¸âƒ£ ç”Ÿæˆï¼ˆGenerationï¼‰
   LLM åŸºäºæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
   "æ ¹æ®è´¢æŠ¥ï¼Œ2024 Q4 è¥æ”¶ä¸º 12.5 äº¿ç¾å…ƒï¼Œ
    åŒæ¯”å¢é•¿ 23%ã€‚"
```

---

### ğŸ”¥ RAG vs å¾®è°ƒ vs Prompt Engineering

| ç»´åº¦ | RAG | Fine-tuning | Prompt Engineering |
|-----|-----|-------------|-------------------|
| **çŸ¥è¯†æ›´æ–°** | å®æ—¶ï¼ˆæ›´æ–°çŸ¥è¯†åº“ï¼‰ | éœ€é‡æ–°è®­ç»ƒ | å— Context é•¿åº¦é™åˆ¶ |
| **æˆæœ¬** | ä½ï¼ˆæ£€ç´¢ + æ¨ç†ï¼‰ | é«˜ï¼ˆè®­ç»ƒ + GPUï¼‰ | ä½ï¼ˆåªæ¨ç†ï¼‰ |
| **å‡†ç¡®æ€§** | é«˜ï¼ˆæœ‰æ¥æºå¼•ç”¨ï¼‰ | é«˜ | ä¸­ï¼ˆæ˜“äº§ç”Ÿå¹»è§‰ï¼‰ |
| **å“åº”é€Ÿåº¦** | ä¸­ï¼ˆæ£€ç´¢æœ‰å¼€é”€ï¼‰ | å¿« | å¿« |
| **å¯è§£é‡Šæ€§** | é«˜ï¼ˆå¯è¿½æº¯æ¥æºï¼‰ | ä½ | ä½ |
| **é€‚ç”¨åœºæ™¯** | çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ | æ”¹å˜æ¨¡å‹è¡Œä¸º | ç®€å•ä»»åŠ¡ |

æ ¹æ® [Microsoft RAG Techniques 2025](https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/04/common-retrieval-augmented-generation-rag-techniques-explained/)ï¼š

> RAG æ˜¯å½“å‰ä¼ä¸šåº”ç”¨ LLM çš„ä¸»æµæ–¹å¼ï¼Œæ— éœ€å¤§é‡å¾®è°ƒå³å¯æ•´åˆæœ€æ–°ã€ç‰¹å®šé¢†åŸŸçš„æ•°æ®ã€‚

---

## ä¸€ã€RAG æ¶æ„æ¼”è¿›

### 1.1 Naive RAGï¼ˆåŸºç¡€ç‰ˆï¼‰

**æ¶æ„**ï¼š

```
ç”¨æˆ·é—®é¢˜
    â†“
Embedding ç¼–ç 
    â†“
å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆTop-Kï¼‰
    â†“
æ‹¼æ¥åˆ° Prompt
    â†“
LLM ç”Ÿæˆç­”æ¡ˆ
```

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# 1. åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(
    texts=["æ–‡æ¡£1å†…å®¹", "æ–‡æ¡£2å†…å®¹", "æ–‡æ¡£3å†…å®¹"],
    embedding=embeddings
)

# 2. åˆ›å»º RAG é“¾
llm = OpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# 3. æé—®
question = "2024 Q4 è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ"
answer = qa_chain.run(question)
print(answer)
```

**é—®é¢˜**ï¼š
- âŒ æ£€ç´¢è´¨é‡å·®ï¼ˆè¯­ä¹‰ä¸åŒ¹é…ï¼‰
- âŒ è¿”å›æ— å…³æ–‡æ¡£ï¼ˆå™ªå£°ï¼‰
- âŒ Context è¿‡é•¿ï¼ˆè¶…å‡º LLM çª—å£ï¼‰
- âŒ æ— æ³•å¤„ç†å¤æ‚æŸ¥è¯¢

---

### 1.2 Advanced RAGï¼ˆè¿›é˜¶ç‰ˆï¼‰

**æ”¹è¿›ç‚¹**ï¼š

```
1ï¸âƒ£ Pre-Retrieval ä¼˜åŒ–
   â”œâ”€ Query Rewritingï¼ˆæŸ¥è¯¢é‡å†™ï¼‰
   â”œâ”€ Query Expansionï¼ˆæŸ¥è¯¢æ‰©å±•ï¼‰
   â””â”€ Query Decompositionï¼ˆæŸ¥è¯¢åˆ†è§£ï¼‰

2ï¸âƒ£ Retrieval ä¼˜åŒ–
   â”œâ”€ Hybrid Searchï¼ˆæ··åˆæ£€ç´¢ï¼‰
   â”œâ”€ Metadata Filteringï¼ˆå…ƒæ•°æ®è¿‡æ»¤ï¼‰
   â””â”€ Contextual Chunkingï¼ˆä¸Šä¸‹æ–‡åˆ†å—ï¼‰

3ï¸âƒ£ Post-Retrieval ä¼˜åŒ–
   â”œâ”€ Rerankingï¼ˆé‡æ’åºï¼‰
   â”œâ”€ Context Compressionï¼ˆä¸Šä¸‹æ–‡å‹ç¼©ï¼‰
   â””â”€ Answer Validationï¼ˆç­”æ¡ˆéªŒè¯ï¼‰
```

**æ¶æ„**ï¼š

```
ç”¨æˆ·é—®é¢˜
    â†“
Query Rewritingï¼ˆé‡å†™ï¼‰
    â†“
Query Decompositionï¼ˆåˆ†è§£ä¸ºå­æŸ¥è¯¢ï¼‰
    â†“
å¹¶è¡Œæ£€ç´¢ï¼ˆDense + Sparse + Webï¼‰
    â†“
Rerankingï¼ˆé‡æ’åºï¼‰
    â†“
Context Compressionï¼ˆå‹ç¼©ï¼‰
    â†“
LLM ç”Ÿæˆ + å¼•ç”¨æ¥æº
```

---

### 1.3 Modular RAGï¼ˆæ¨¡å—åŒ–ï¼Œ2025 ä¸»æµï¼‰

æ ¹æ® [Microsoft Advanced RAG](https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation)ï¼ˆ2025ï¼‰ï¼š

> æ¨¡å—åŒ– RAG å°†æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºå¯ç‹¬ç«‹ä¼˜åŒ–çš„æ¨¡å—ï¼Œæ”¯æŒçµæ´»ç»„åˆä¸åŒç­–ç•¥ã€‚

**æ ¸å¿ƒæ¨¡å—**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RAG System Architecture         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Ingestion Pipeline              â”‚
â”‚     â”œâ”€ Document Loading             â”‚
â”‚     â”œâ”€ Chunking Strategy            â”‚
â”‚     â”œâ”€ Metadata Extraction          â”‚
â”‚     â””â”€ Vector Indexing              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. Query Processing                â”‚
â”‚     â”œâ”€ Query Understanding          â”‚
â”‚     â”œâ”€ Intent Classification        â”‚
â”‚     â””â”€ Query Transformation         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. Retrieval Engine                â”‚
â”‚     â”œâ”€ Dense Retrieval              â”‚
â”‚     â”œâ”€ Sparse Retrieval             â”‚
â”‚     â”œâ”€ Hybrid Retrieval             â”‚
â”‚     â””â”€ Multi-stage Retrieval        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. Reranking & Filtering           â”‚
â”‚     â”œâ”€ Cross-Encoder Reranking      â”‚
â”‚     â”œâ”€ Relevance Filtering          â”‚
â”‚     â””â”€ Diversity Promotion          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. Generation & Post-processing    â”‚
â”‚     â”œâ”€ Context Assembly             â”‚
â”‚     â”œâ”€ Prompt Engineering           â”‚
â”‚     â”œâ”€ LLM Generation               â”‚
â”‚     â””â”€ Citation & Verification      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€æ ¸å¿ƒç»„ä»¶ 1ï¼šEmbedding æ¨¡å‹

### 2.1 Embedding æ˜¯ä»€ä¹ˆï¼Ÿ

**ç›´è§‚ç†è§£**ï¼šæŠŠæ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬å‘é‡ä¹Ÿç›¸è¿‘ã€‚

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# æ–‡æœ¬ â†’ å‘é‡
text1 = "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ"
text2 = "ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹"
text3 = "ä»Šå¤©å¤©æ°”ä¸é”™"

emb1 = model.encode(text1)
emb2 = model.encode(text2)
emb3 = model.encode(text3)

# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity

sim_12 = cosine_similarity([emb1], [emb2])[0][0]
sim_13 = cosine_similarity([emb1], [emb3])[0][0]

print(f"æ–‡æœ¬1 vs æ–‡æœ¬2 ç›¸ä¼¼åº¦: {sim_12:.4f}")  # 0.75ï¼ˆé«˜ï¼‰
print(f"æ–‡æœ¬1 vs æ–‡æœ¬3 ç›¸ä¼¼åº¦: {sim_13:.4f}")  # 0.12ï¼ˆä½ï¼‰

# ğŸ”¥ è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œå‘é‡ä¹Ÿç›¸ä¼¼ï¼
```

---

### 2.2 Embedding æ¨¡å‹å¯¹æ¯”ï¼ˆ2025ï¼‰

æ ¹æ® [Best Embedding Models 2025](https://app.ailog.fr/en/blog/guides/choosing-embedding-models) å’Œ MTEB æ’è¡Œæ¦œï¼š

#### **é—­æºæ¨¡å‹**

| æ¨¡å‹ | MTEB Score | ç»´åº¦ | æˆæœ¬ | ç‰¹ç‚¹ |
|-----|-----------|------|------|------|
| **Gemini-embedding-001** | 68.32 | 3072 | ~$0.004/1K tokens | æœ€ä½³å¤šè¯­è¨€ï¼ˆ100+ è¯­è¨€ï¼‰ |
| **OpenAI text-embedding-3-large** | 64.6 | 3072 | $0.13/1M tokens | é€šç”¨æ€§å¼ºï¼Œå¯è°ƒç»´åº¦ |
| **Cohere embed-v4** | 65.2 | 1024 | $0.10/1M tokens | ä¼ä¸šçº§ï¼ŒæŠ—å™ªéŸ³ |

#### **å¼€æºæ¨¡å‹**

| æ¨¡å‹ | MTEB Score | ç»´åº¦ | è®¸å¯è¯ | ç‰¹ç‚¹ |
|-----|-----------|------|--------|------|
| **Qwen3-Embedding-8B** | 70.58 | 4096 | Apache 2.0 | ğŸ”¥ æœ€ä½³å¼€æºæ¨¡å‹ |
| **BGE-M3** | 63.0 | 1024 | MIT | å¤šè¯­è¨€ï¼Œæ”¯æŒ 8192 tokens |
| **E5-Mistral-7B** | 66.6 | 4096 | MIT | è¶…è¶Šæ—©æœŸé—­æºæ¨¡å‹ |
| **all-MiniLM-L6-v2** | 56.3 | 384 | Apache 2.0 | è½»é‡å¿«é€Ÿï¼Œé€‚åˆåŸå‹ |

**é€‰æ‹©å»ºè®®**ï¼ˆ2025ï¼‰ï¼š

```
åœºæ™¯ 1ï¼šåŸå‹å¼€å‘ã€æœ¬åœ°è¿è¡Œ
  â†’ all-MiniLM-L6-v2ï¼ˆå¿«é€Ÿã€è½»é‡ï¼‰

åœºæ™¯ 2ï¼šç”Ÿäº§ç¯å¢ƒã€å¤šè¯­è¨€æ”¯æŒ
  â†’ BGE-M3 æˆ– E5-Mistral-7Bï¼ˆå¼€æºã€é«˜æ€§èƒ½ï¼‰

åœºæ™¯ 3ï¼šä¼ä¸šçº§ã€é¢„ç®—å……è¶³
  â†’ OpenAI text-embedding-3-largeï¼ˆç¨³å®šã€ç”Ÿæ€å¥½ï¼‰

åœºæ™¯ 4ï¼šè¶…å¤§è§„æ¨¡ã€è‡ªå»ºé›†ç¾¤
  â†’ Qwen3-Embedding-8Bï¼ˆæœ€ä½³æ€§èƒ½ï¼‰
```

---

### 2.3 å®æˆ˜ï¼šå¯¹æ¯”ä¸åŒ Embedding æ¨¡å‹

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# åŠ è½½ä¸åŒæ¨¡å‹
models = {
    "MiniLM": SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'),
    "BGE-M3": SentenceTransformer('BAAI/bge-m3'),
    "E5-large": SentenceTransformer('intfloat/e5-large-v2'),
}

# æµ‹è¯•æ•°æ®
query = "å¦‚ä½•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Ÿ"
docs = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ­¥éª¤åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®¾è®¡ã€æŸå¤±å‡½æ•°é€‰æ‹©...",  # ç›¸å…³
    "ç¥ç»ç½‘ç»œè®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚",                        # ç›¸å…³
    "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé€‚åˆæˆ·å¤–æ´»åŠ¨ã€‚"                                # ä¸ç›¸å…³
]

# å¯¹æ¯”æ£€ç´¢æ•ˆæœ
for model_name, model in models.items():
    print(f"\n{'='*50}")
    print(f"æ¨¡å‹: {model_name}")
    print(f"{'='*50}")
    
    query_emb = model.encode(query)
    doc_embs = model.encode(docs)
    
    similarities = cosine_similarity([query_emb], doc_embs)[0]
    
    for i, (doc, sim) in enumerate(zip(docs, similarities)):
        print(f"Doc {i+1} (ç›¸ä¼¼åº¦: {sim:.4f}): {doc[:30]}...")
    
    # æ’åº
    ranked_indices = np.argsort(similarities)[::-1]
    print(f"\næ’åº: {ranked_indices + 1}")  # +1 for 1-based indexing

# ğŸ”¥ è§‚å¯Ÿï¼šä¸åŒæ¨¡å‹çš„ç›¸ä¼¼åº¦åˆ†æ•°å’Œæ’åºå¯èƒ½ä¸åŒ
# é€‰æ‹©åœ¨ä½ çš„æ•°æ®ä¸Šæ•ˆæœæœ€å¥½çš„æ¨¡å‹ï¼
```

---

## ä¸‰ã€æ ¸å¿ƒç»„ä»¶ 2ï¼šå‘é‡æ•°æ®åº“

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦å‘é‡æ•°æ®åº“ï¼Ÿ

**é—®é¢˜**ï¼šæ•°ç™¾ä¸‡ä¸ªæ–‡æ¡£çš„å‘é‡ï¼Œå¦‚ä½•é«˜æ•ˆæ£€ç´¢ï¼Ÿ

```
æš´åŠ›æœç´¢ï¼ˆBrute Forceï¼‰ï¼š
  - è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
  - æ—¶é—´å¤æ‚åº¦ï¼šO(n)
  - ç™¾ä¸‡æ–‡æ¡£ï¼šæ•°ç§’åˆ°æ•°åç§’

å‘é‡æ•°æ®åº“ï¼ˆHNSW/IVFï¼‰ï¼š
  - è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼ˆANNï¼‰
  - æ—¶é—´å¤æ‚åº¦ï¼šO(log n)
  - ç™¾ä¸‡æ–‡æ¡£ï¼š<100ms
```

---

### 3.2 å‘é‡æ•°æ®åº“å¯¹æ¯”ï¼ˆ2025ï¼‰

æ ¹æ® [Vector Database Comparison 2025](https://www.firecrawl.dev/blog/best-vector-databases-2025)ï¼š

#### **Pinecone**ï¼ˆæœ€ä½³ç”Ÿäº§ç¯å¢ƒï¼‰

```
ä¼˜åŠ¿ï¼š
  âœ… 99.99% SLAï¼Œ<50ms å»¶è¿Ÿ
  âœ… å®Œå…¨æ‰˜ç®¡ï¼Œè‡ªåŠ¨æ‰©å±•
  âœ… å…è´¹ 100K å‘é‡
  
åŠ£åŠ¿ï¼š
  âŒ ä»…äº‘ç«¯ï¼Œä¾›åº”å•†é”å®š
  âŒ å®šåˆ¶åŒ–å—é™

é€‚ç”¨ï¼šä¼ä¸šç”Ÿäº§ã€åäº¿çº§è§„æ¨¡

å®šä»·ï¼š
  - Starter: å…è´¹ï¼ˆ100K å‘é‡ï¼‰
  - Standard: $70/æœˆèµ·
  - Enterprise: $200+/æœˆ
```

---

#### **Weaviate**ï¼ˆæœ€ä½³çµæ´»æ€§ï¼‰

```
ä¼˜åŠ¿ï¼š
  âœ… å¼€æº + æ‰˜ç®¡äº‘
  âœ… æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
  âœ… å†…ç½® ML æ¨¡å‹
  âœ… å¤šç§Ÿæˆ·ã€GDPR åˆè§„
  
åŠ£åŠ¿ï¼š
  âŒ å­¦ä¹ æ›²çº¿é™¡å³­
  âŒ è‡ªæ‰˜ç®¡éœ€ DevOps ç»éªŒ

é€‚ç”¨ï¼šå¤šæ¨¡æ€ AIã€çŸ¥è¯†å›¾è°±ã€ç§æœ‰éƒ¨ç½²

å®šä»·ï¼š
  - å¼€æº: å…è´¹
  - Serverless: $25/æœˆèµ·
  - Enterprise: $295/æœˆèµ·
```

---

#### **Chroma**ï¼ˆæœ€ä½³å¼€å‘ä½“éªŒï¼‰

```
ä¼˜åŠ¿ï¼š
  âœ… 5 åˆ†é’Ÿä¸Šæ‰‹
  âœ… åŸç”Ÿ Python API
  âœ… LangChain é›†æˆ
  âœ… è½»é‡çº§ï¼Œèµ„æºå ç”¨å°‘
  
åŠ£åŠ¿ï¼š
  âŒ æ‰©å±•æ€§æœ‰é™ï¼ˆ10M å‘é‡è½¯é™åˆ¶ï¼‰
  âŒ æ— å†…ç½®å¤‡ä»½

é€‚ç”¨ï¼šRAG åŸå‹ã€Python é¡¹ç›®ã€æœ¬åœ°å¼€å‘

å®šä»·ï¼š
  - å¼€æº: å…è´¹
  - Teams: $108/æœˆ
```

---

#### **Milvus**ï¼ˆæœ€ä½³ä¼ä¸šçº§è§„æ¨¡ï¼‰

```
ä¼˜åŠ¿ï¼š
  âœ… è®¾è®¡æ”¯æŒæ•°åäº¿åˆ°ä¸‡äº¿å‘é‡
  âœ… æ°´å¹³æ‰©å±•
  âœ… å¤šç§ç´¢å¼•ï¼ˆIVFã€HNSWã€DiskANNï¼‰
  âœ… å¼ºä¸€è‡´æ€§ä¿è¯
  
åŠ£åŠ¿ï¼š
  âŒ å¤æ‚çš„ K8s éƒ¨ç½²
  âŒ é™¡å³­çš„å­¦ä¹ æ›²çº¿

é€‚ç”¨ï¼šå¤§è§„æ¨¡ä¼ä¸šã€æµ·é‡æ•°æ®

å®šä»·ï¼š
  - å¼€æº: å…è´¹
  - Zilliz Cloud: $100/æœˆèµ·
```

---

### 3.3 æ€§èƒ½å¯¹æ¯”

æ ¹æ® [TensorBlue Benchmark 2025](https://tensorblue.com/blog/vector-database-comparison-pinecone-weaviate-qdrant-milvus-2025)ï¼š

| æ•°æ®åº“ | P95 å»¶è¿Ÿ | ååé‡ (QPS) | å†…å­˜æ•ˆç‡ |
|-------|---------|-------------|---------|
| **Pinecone** | 40-50ms | 5,000-10,000 | â­â­â­â­ |
| **Qdrant** | 30-40ms | 8,000-15,000 | â­â­â­â­â­ |
| **Weaviate** | 50-70ms | 3,000-8,000 | â­â­â­ |
| **Milvus** | 50-80ms | 10,000-20,000 | â­â­â­â­ |
| **Chroma** | 60-100ms | 1,000-3,000 | â­â­â­â­â­ |

---

### 3.4 å®æˆ˜ï¼šä½¿ç”¨ Chroma æ„å»ºå‘é‡æ•°æ®åº“

```python
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# 1. åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯
client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="./chroma_db"
))

# 2. åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)

# 3. å‡†å¤‡æ–‡æ¡£
documents = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚",
    "Transformer æ¶æ„æ˜¯ç°ä»£ NLP çš„åŸºç¡€ã€‚",
    "RAG ç³»ç»Ÿç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆèƒ½åŠ›ã€‚",
    "å‘é‡æ•°æ®åº“ç”¨äºé«˜æ•ˆçš„è¯­ä¹‰æœç´¢ã€‚",
    "Python æ˜¯æœºå™¨å­¦ä¹ çš„é¦–é€‰è¯­è¨€ã€‚"
]

# 4. ç”Ÿæˆ Embeddings
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = embedding_model.encode(documents).tolist()

# 5. æ’å…¥å‘é‡æ•°æ®åº“
collection.add(
    documents=documents,
    embeddings=embeddings,
    ids=[f"doc_{i}" for i in range(len(documents))],
    metadatas=[{"source": "manual", "index": i} for i in range(len(documents))]
)

# 6. æŸ¥è¯¢
query = "å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œï¼Ÿ"
query_embedding = embedding_model.encode(query).tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3
)

print("æ£€ç´¢ç»“æœï¼š")
for i, (doc, distance, metadata) in enumerate(zip(
    results['documents'][0],
    results['distances'][0],
    results['metadatas'][0]
)):
    print(f"\n{i+1}. (è·ç¦»: {distance:.4f})")
    print(f"   æ–‡æ¡£: {doc}")
    print(f"   å…ƒæ•°æ®: {metadata}")

# 7. æŒä¹…åŒ–
client.persist()

# ğŸ”¥ Chroma è‡ªåŠ¨å¤„ç†å‘é‡ç´¢å¼•å’ŒæŒä¹…åŒ–ï¼
```

---

## å››ã€æ ¸å¿ƒç»„ä»¶ 3ï¼šæ£€ç´¢ç­–ç•¥

### 4.1 Dense Retrievalï¼ˆç¨ å¯†æ£€ç´¢ï¼‰

**åŸç†**ï¼šç”¨ Embedding æ¨¡å‹ç¼–ç ï¼Œè®¡ç®—å‘é‡ç›¸ä¼¼åº¦ã€‚

```python
# Dense Retrieval ç¤ºä¾‹
query = "æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§"
query_embedding = embedding_model.encode(query)

# åœ¨å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=5
)

# ä¼˜åŠ¿ï¼š
# âœ… è¯­ä¹‰ç†è§£å¼ºï¼ˆ"DL training" èƒ½åŒ¹é… "æ·±åº¦å­¦ä¹ è®­ç»ƒ"ï¼‰
# âœ… æ³›åŒ–èƒ½åŠ›å¥½

# åŠ£åŠ¿ï¼š
# âŒ å¯¹å…³é”®è¯æ•æ„Ÿåº¦ä½ï¼ˆ"GPT-4" å¯èƒ½åŒ¹é…ä¸å‡†ï¼‰
# âŒ å†·å¯åŠ¨é—®é¢˜ï¼ˆæ–°é¢†åŸŸæ•ˆæœå·®ï¼‰
```

---

### 4.2 Sparse Retrievalï¼ˆç¨€ç–æ£€ç´¢ï¼‰

**åŸç†**ï¼šåŸºäºå…³é”®è¯åŒ¹é…ï¼ˆå¦‚ BM25ï¼‰ã€‚

```python
from rank_bm25 import BM25Okapi

# æ–‡æ¡£åº“
documents = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®",
    "GPT-4 æ˜¯ OpenAI çš„æœ€æ–°æ¨¡å‹",
    "Transformer æ¶æ„åŸºäºæ³¨æ„åŠ›æœºåˆ¶"
]

# åˆ†è¯
tokenized_docs = [doc.split() for doc in documents]

# æ„å»º BM25 ç´¢å¼•
bm25 = BM25Okapi(tokenized_docs)

# æŸ¥è¯¢
query = "GPT-4 æ¨¡å‹"
tokenized_query = query.split()

# æ£€ç´¢
scores = bm25.get_scores(tokenized_query)
ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)

print("BM25 æ£€ç´¢ç»“æœï¼š")
for i in ranked_indices[:3]:
    print(f"{i+1}. (å¾—åˆ†: {scores[i]:.4f}) {documents[i]}")

# ä¼˜åŠ¿ï¼š
# âœ… å…³é”®è¯ç²¾å‡†åŒ¹é…ï¼ˆ"GPT-4" ä¸€å®šèƒ½æ‰¾åˆ°ï¼‰
# âœ… å¯è§£é‡Šæ€§å¼º

# åŠ£åŠ¿ï¼š
# âŒ è¯­ä¹‰ç†è§£å¼±ï¼ˆ"DL training" æ— æ³•åŒ¹é… "æ·±åº¦å­¦ä¹ è®­ç»ƒ"ï¼‰
# âŒ è¯åºæ•æ„Ÿ
```

---

### 4.3 Hybrid Searchï¼ˆæ··åˆæ£€ç´¢ï¼Œ2025 æ¨èï¼‰

æ ¹æ® [Microsoft RAG Techniques](https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/04/common-retrieval-augmented-generation-rag-techniques-explained/)ï¼š

> æ··åˆæ£€ç´¢ç»“åˆå¤šç§æ–¹æ³•ï¼Œæ‰©å±•æœç´¢èŒƒå›´å¹¶æé«˜å‡†ç¡®æ€§ï¼Œæ˜¯ 2025 å¹´çš„æœ€ä½³å®è·µã€‚

**åŸç†**ï¼šç»“åˆ Dense + Sparseï¼Œå–é•¿è¡¥çŸ­ã€‚

```python
from typing import List, Tuple

def hybrid_search(
    query: str,
    documents: List[str],
    embedding_model,
    collection,
    alpha: float = 0.5  # Dense æƒé‡
) -> List[Tuple[int, float]]:
    """
    æ··åˆæ£€ç´¢ï¼šDense (alpha) + Sparse (1-alpha)
    """
    # 1. Dense Retrieval
    query_embedding = embedding_model.encode(query).tolist()
    dense_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=len(documents)
    )
    
    # Dense å¾—åˆ†å½’ä¸€åŒ–
    dense_scores = {}
    for doc_id, distance in zip(dense_results['ids'][0], dense_results['distances'][0]):
        # è·ç¦» â†’ ç›¸ä¼¼åº¦ï¼ˆ1 - distanceï¼‰
        dense_scores[doc_id] = 1 - distance
    
    # 2. Sparse Retrieval (BM25)
    tokenized_docs = [doc.split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)
    tokenized_query = query.split()
    sparse_scores = bm25.get_scores(tokenized_query)
    
    # Sparse å¾—åˆ†å½’ä¸€åŒ–
    max_sparse = max(sparse_scores) if max(sparse_scores) > 0 else 1
    sparse_scores_norm = {f"doc_{i}": score / max_sparse for i, score in enumerate(sparse_scores)}
    
    # 3. èåˆå¾—åˆ†
    final_scores = {}
    for doc_id in dense_scores:
        dense_score = dense_scores.get(doc_id, 0)
        sparse_score = sparse_scores_norm.get(doc_id, 0)
        final_scores[doc_id] = alpha * dense_score + (1 - alpha) * sparse_score
    
    # 4. æ’åº
    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    
    return ranked

# æµ‹è¯•
query = "GPT-4 è®­ç»ƒæŠ€å·§"
results = hybrid_search(query, documents, embedding_model, collection, alpha=0.7)

print("æ··åˆæ£€ç´¢ç»“æœï¼š")
for doc_id, score in results[:3]:
    print(f"  {doc_id} (å¾—åˆ†: {score:.4f})")

# ğŸ”¥ Hybrid Search å…¼é¡¾è¯­ä¹‰å’Œå…³é”®è¯åŒ¹é…ï¼
```

**Alpha è°ƒä¼˜**ï¼š

```
alpha = 1.0  â†’ çº¯ Denseï¼ˆè¯­ä¹‰ä¼˜å…ˆï¼‰
alpha = 0.5  â†’ å‡è¡¡
alpha = 0.0  â†’ çº¯ Sparseï¼ˆå…³é”®è¯ä¼˜å…ˆï¼‰

åœºæ™¯ï¼š
- æŠ€æœ¯æ–‡æ¡£ã€ä»£ç æ£€ç´¢ï¼šalpha = 0.3-0.5ï¼ˆå…³é”®è¯é‡è¦ï¼‰
- é€šç”¨çŸ¥è¯†é—®ç­”ï¼šalpha = 0.7-0.9ï¼ˆè¯­ä¹‰é‡è¦ï¼‰
```

---

## äº”ã€æ ¸å¿ƒç»„ä»¶ 4ï¼šRerankingï¼ˆé‡æ’åºï¼‰

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦ Rerankingï¼Ÿ

**é—®é¢˜**ï¼šåˆå§‹æ£€ç´¢è¿”å› 100 ä¸ªå€™é€‰ï¼Œä½†åªèƒ½ç»™ LLM æä¾› Top-5ã€‚

```
åˆå§‹æ£€ç´¢ï¼ˆå¿«é€Ÿä½†ç²—ç³™ï¼‰ï¼š
  è¿”å› 100 ä¸ªå€™é€‰æ–‡æ¡£
  å¬å›ç‡é«˜ï¼Œä½†ç²¾ç¡®åº¦ä¸€èˆ¬

Rerankingï¼ˆæ…¢ä½†ç²¾å‡†ï¼‰ï¼š
  å¯¹ 100 ä¸ªå€™é€‰é‡æ–°æ‰“åˆ†
  é€‰å‡ºçœŸæ­£ç›¸å…³çš„ Top-5
  ç²¾ç¡®åº¦å¤§å¹…æå‡
```

**æµç¨‹**ï¼š

```
Query
  â†“
1ï¸âƒ£ First-stage Retrievalï¼ˆå¿«é€Ÿï¼‰
   æ£€ç´¢ Top-100 å€™é€‰
  â†“
2ï¸âƒ£ Rerankingï¼ˆç²¾å‡†ï¼‰
   é‡æ–°æ’åºï¼Œé€‰å‡º Top-5
  â†“
3ï¸âƒ£ æä¾›ç»™ LLM
```

---

### 5.2 Reranking æ–¹æ³•å¯¹æ¯”

#### **Cross-Encoder**ï¼ˆæœ€ç²¾å‡†ï¼‰

æ ¹æ® [Cross-Encoders and ColBERT Guide](https://medium.com/@aimichael/cross-encoders-colbert-and-llm-based-re-rankers-a-practical-guide)ï¼ˆ2025ï¼‰ï¼š

**åŸç†**ï¼šå°† Query å’Œ Document ä¸€èµ·è¾“å…¥ Transformerï¼Œç›´æ¥é¢„æµ‹ç›¸å…³æ€§ã€‚

```python
from sentence_transformers import CrossEncoder

# åŠ è½½ Cross-Encoder æ¨¡å‹
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# å€™é€‰æ–‡æ¡£
query = "å¦‚ä½•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Ÿ"
candidates = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ­¥éª¤...",
    "ç¥ç»ç½‘ç»œè®­ç»ƒéœ€è¦ GPU...",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½...",
    "Python æ˜¯æµè¡Œçš„ç¼–ç¨‹è¯­è¨€..."
]

# Rerank
scores = reranker.predict([(query, doc) for doc in candidates])

# æ’åº
ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)

print("Cross-Encoder Reranking ç»“æœï¼š")
for i in ranked_indices[:3]:
    print(f"{i+1}. (å¾—åˆ†: {scores[i]:.4f}) {candidates[i][:30]}...")

# ä¼˜åŠ¿ï¼š
# âœ… ç²¾åº¦æœ€é«˜ï¼ˆMSMARCO MRR@10 > 40ï¼‰
# âœ… è€ƒè™‘ Query-Doc äº¤äº’

# åŠ£åŠ¿ï¼š
# âŒ é€Ÿåº¦æ…¢ï¼ˆæ¯å¯¹éƒ½éœ€å®Œæ•´å‰å‘ä¼ æ’­ï¼‰
# âŒ ä¸é€‚åˆå¤§è§„æ¨¡åˆç­›
```

---

#### **ColBERT**ï¼ˆæ•ˆç‡ä¸ç²¾åº¦å¹³è¡¡ï¼‰

**åŸç†**ï¼šQuery å’Œ Doc åˆ†åˆ«ç¼–ç ï¼Œé€šè¿‡ token çº§åˆ«çš„ MaxSim è®¡ç®—ç›¸å…³æ€§ã€‚

```python
# ColBERT ç¤ºä¾‹ï¼ˆç®€åŒ–ï¼‰
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('colbert-ir/colbertv2.0')
model = AutoModel.from_pretrained('colbert-ir/colbertv2.0')

def colbert_score(query, document):
    # ç¼–ç 
    query_tokens = tokenizer(query, return_tensors='pt')
    doc_tokens = tokenizer(document, return_tensors='pt', truncation=True, max_length=512)
    
    with torch.no_grad():
        query_emb = model(**query_tokens).last_hidden_state  # (1, len_q, dim)
        doc_emb = model(**doc_tokens).last_hidden_state      # (1, len_d, dim)
    
    # MaxSim: æ¯ä¸ª query token æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ doc token
    similarity_matrix = torch.matmul(query_emb, doc_emb.transpose(1, 2))  # (1, len_q, len_d)
    max_sim = similarity_matrix.max(dim=2).values  # (1, len_q)
    score = max_sim.sum()  # æ±‚å’Œ
    
    return score.item()

# æµ‹è¯•
query = "æ·±åº¦å­¦ä¹ è®­ç»ƒ"
documents = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ­¥éª¤",
    "æœºå™¨å­¦ä¹ ç®—æ³•åˆ†ç±»",
    "ä»Šå¤©å¤©æ°”æ™´æœ—"
]

scores = [colbert_score(query, doc) for doc in documents]
ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)

print("ColBERT Reranking ç»“æœï¼š")
for doc, score in ranked:
    print(f"  (å¾—åˆ†: {score:.2f}) {doc}")

# ä¼˜åŠ¿ï¼š
# âœ… æ¯” Cross-Encoder å¿«ï¼ˆQuery/Doc åˆ†åˆ«ç¼–ç ï¼‰
# âœ… ç²¾åº¦é«˜äºæ™®é€š Bi-Encoder

# åŠ£åŠ¿ï¼š
# âŒ æ¯” Bi-Encoder æ…¢
```

---

### 5.3 å®æˆ˜ï¼šå®Œæ•´çš„æ£€ç´¢+Reranking æµç¨‹

```python
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb

# 1. åˆå§‹åŒ–
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# 2. å‘é‡æ•°æ®åº“
client = chromadb.Client()
collection = client.create_collection("docs")

documents = [
    "æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œ GPU èµ„æºã€‚",
    "Transformer æ˜¯ç°ä»£ NLP çš„åŸºçŸ³æ¶æ„ã€‚",
    "RAG ç³»ç»Ÿç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„ä¼˜åŠ¿ã€‚",
    "å‘é‡æ•°æ®åº“æ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢ã€‚",
    "Python æ˜¯æœºå™¨å­¦ä¹ çš„é¦–é€‰ç¼–ç¨‹è¯­è¨€ã€‚",
    "æ¢¯åº¦ä¸‹é™æ˜¯ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ã€‚",
    "è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬åˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰ä»»åŠ¡ã€‚",
    "BERT æ¨¡å‹åœ¨å¤šä¸ª NLP ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´ã€‚",
    "å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±ä¿¡å·è®­ç»ƒæ™ºèƒ½ä½“ã€‚",
    "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚"
]

# æ’å…¥
collection.add(
    documents=documents,
    embeddings=embedding_model.encode(documents).tolist(),
    ids=[f"doc_{i}" for i in range(len(documents))]
)

# 3. æŸ¥è¯¢
query = "å¦‚ä½•è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Ÿ"

# é˜¶æ®µ 1: åˆå§‹æ£€ç´¢ï¼ˆTop-10ï¼‰
query_emb = embedding_model.encode(query).tolist()
initial_results = collection.query(
    query_embeddings=[query_emb],
    n_results=10  # å¬å›æ›´å¤šå€™é€‰
)

print("é˜¶æ®µ 1 - åˆå§‹æ£€ç´¢ Top-10:")
for i, doc in enumerate(initial_results['documents'][0][:10]):
    print(f"  {i+1}. {doc[:50]}...")

# é˜¶æ®µ 2: Rerankingï¼ˆTop-3ï¼‰
candidates = initial_results['documents'][0]
rerank_scores = reranker.predict([(query, doc) for doc in candidates])

# æ’åº
reranked_indices = sorted(range(len(rerank_scores)), 
                          key=lambda i: rerank_scores[i], 
                          reverse=True)

print("\né˜¶æ®µ 2 - Reranking å Top-3:")
for i in reranked_indices[:3]:
    print(f"  {i+1}. (å¾—åˆ†: {rerank_scores[i]:.4f})")
    print(f"     {candidates[i]}")

# ğŸ”¥ Reranking æ˜¾è‘—æå‡äº† Top-3 çš„ç›¸å…³æ€§ï¼
```

---

## å…­ã€é«˜çº§æŠ€æœ¯ï¼šChunking ç­–ç•¥

### 6.1 ä¸ºä»€ä¹ˆ Chunking å¾ˆé‡è¦ï¼Ÿ

æ ¹æ® [Mastering Chunking for RAG](https://towardsdev.com/mastering-chunking-for-effective-rag-beyond-basics-with-qdrant-and-reranking-bb0761ae84e4)ï¼ˆ2025ï¼‰ï¼š

> Chunking æ˜¯ RAG æˆåŠŸçš„åŸºç¡€ã€‚Chunk å¤ªé•¿ä¼šåˆ†æ•£æ¨¡å‹æ³¨æ„åŠ›ï¼Œå¤ªçŸ­ä¼šä¸¢å¤±ä¸Šä¸‹æ–‡ã€‚

**æƒè¡¡**ï¼š

```
Chunk å¤ªé•¿ï¼ˆå¦‚ 2000 tokensï¼‰:
  âŒ LLM éš¾ä»¥èšç„¦å…³é”®ä¿¡æ¯
  âŒ æ£€ç´¢ä¸å¤Ÿç²¾å‡†
  âœ… ä¸Šä¸‹æ–‡å®Œæ•´

Chunk å¤ªçŸ­ï¼ˆå¦‚ 100 tokensï¼‰:
  âœ… æ£€ç´¢ç²¾å‡†
  âŒ ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–
  âŒ è¯­ä¹‰ä¸å®Œæ•´
```

---

### 6.2 Chunking ç­–ç•¥å¯¹æ¯”

#### **å›ºå®šå¤§å°åˆ†å—**ï¼ˆæœ€ç®€å•ï¼‰

```python
def fixed_size_chunking(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
    """å›ºå®šå¤§å°åˆ†å—"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # é‡å éƒ¨åˆ†
    
    return chunks

text = "é•¿æ–‡æœ¬å†…å®¹..." * 100
chunks = fixed_size_chunking(text, chunk_size=512, overlap=50)

print(f"ç”Ÿæˆ {len(chunks)} ä¸ª chunk")

# ä¼˜åŠ¿ï¼š
# âœ… ç®€å•ã€å¿«é€Ÿ
# âœ… é€‚åˆå‡åŒ€çš„æ–‡æœ¬

# åŠ£åŠ¿ï¼š
# âŒ å¯èƒ½åœ¨å¥å­ä¸­é—´åˆ‡æ–­
# âŒ ä¸è€ƒè™‘è¯­ä¹‰è¾¹ç•Œ
```

---

#### **è¯­ä¹‰åˆ†å—**ï¼ˆæ¨èï¼‰

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def semantic_chunking(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
    """è¯­ä¹‰åˆ†å—ï¼šä¼˜å…ˆåœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ". ", "! ", "? ", " ", ""],  # ä¼˜å…ˆçº§
        length_function=len,
    )
    
    chunks = splitter.split_text(text)
    return chunks

text = """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚

Transformer æ¶æ„å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€‚å®ƒåŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ã€‚

RAG ç³»ç»Ÿç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆã€‚å®ƒå…ˆä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå†è®© LLM åŸºäºè¿™äº›ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆã€‚
"""

chunks = semantic_chunking(text, chunk_size=100, overlap=20)

print(f"ç”Ÿæˆ {len(chunks)} ä¸ªè¯­ä¹‰chunk:")
for i, chunk in enumerate(chunks):
    print(f"\nChunk {i+1}:")
    print(chunk)

# ä¼˜åŠ¿ï¼š
# âœ… å°Šé‡è¯­ä¹‰è¾¹ç•Œï¼ˆå¥å­ã€æ®µè½ï¼‰
# âœ… ä¸Šä¸‹æ–‡æ›´å®Œæ•´

# åŠ£åŠ¿ï¼š
# âŒ ç¨æ…¢ï¼ˆéœ€è§£ææ–‡æœ¬ç»“æ„ï¼‰
```

---

#### **Contextual Chunking**ï¼ˆ2025 å‰æ²¿ï¼‰

```python
def contextual_chunking(text: str, context_window: int = 3) -> List[dict]:
    """
    ä¸Šä¸‹æ–‡åˆ†å—ï¼šä¸ºæ¯ä¸ª chunk æ·»åŠ å‰åæ–‡æ‘˜è¦
    """
    # 1. åŸºç¡€åˆ†å—
    base_chunks = semantic_chunking(text, chunk_size=300)
    
    # 2. ä¸ºæ¯ä¸ª chunk æ·»åŠ ä¸Šä¸‹æ–‡
    contextual_chunks = []
    
    for i, chunk in enumerate(base_chunks):
        # å‰æ–‡
        prev_context = ""
        if i > 0:
            prev_chunks = base_chunks[max(0, i - context_window):i]
            prev_context = "å‰æ–‡ï¼š" + " ".join(prev_chunks)[:100] + "..."
        
        # åæ–‡
        next_context = ""
        if i < len(base_chunks) - 1:
            next_chunks = base_chunks[i+1:min(len(base_chunks), i + context_window + 1)]
            next_context = "åæ–‡ï¼š" + " ".join(next_chunks)[:100] + "..."
        
        # ç»„åˆ
        contextual_chunk = {
            "content": chunk,
            "prev_context": prev_context,
            "next_context": next_context,
            "full_context": f"{prev_context}\n\nã€å½“å‰å†…å®¹ã€‘\n{chunk}\n\n{next_context}"
        }
        
        contextual_chunks.append(contextual_chunk)
    
    return contextual_chunks

contextual_chunks = contextual_chunking(text)

print(f"ç”Ÿæˆ {len(contextual_chunks)} ä¸ªä¸Šä¸‹æ–‡chunk:")
print(f"\nç¤ºä¾‹ - Chunk 1:")
print(contextual_chunks[0]["full_context"])

# ğŸ”¥ Contextual Chunking è®© LLM èƒ½çœ‹åˆ°æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼
```

---

## ä¸ƒã€å®Œæ•´ RAG ç³»ç»Ÿå®ç°

### 7.1 ç³»ç»Ÿæ¶æ„

```python
from dataclasses import dataclass
from typing import List, Optional
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb
from openai import OpenAI

@dataclass
class RAGConfig:
    """RAG ç³»ç»Ÿé…ç½®"""
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    llm_model: str = "gpt-3.5-turbo"
    chunk_size: int = 512
    chunk_overlap: int = 50
    top_k_retrieval: int = 20  # åˆå§‹æ£€ç´¢æ•°é‡
    top_k_rerank: int = 5      # Rerank åæ•°é‡

class RAGSystem:
    """ç”Ÿäº§çº§ RAG ç³»ç»Ÿ"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("Loading models...")
        self.embedding_model = SentenceTransformer(config.embedding_model)
        self.reranker = CrossEncoder(config.reranker_model)
        self.llm_client = OpenAI()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.chroma_client = chromadb.Client()
        self.collection = None
        
        print("RAG System initialized!")
    
    def ingest_documents(self, documents: List[str], collection_name: str = "docs"):
        """æ–‡æ¡£æ‘„å…¥æµç¨‹"""
        print(f"\n{'='*50}")
        print("Phase 1: Document Ingestion")
        print(f"{'='*50}")
        
        # 1. Chunking
        print("Step 1: Chunking documents...")
        all_chunks = []
        chunk_metadata = []
        
        for doc_id, doc in enumerate(documents):
            chunks = self._semantic_chunk(doc)
            all_chunks.extend(chunks)
            chunk_metadata.extend([
                {"doc_id": doc_id, "chunk_id": i} 
                for i in range(len(chunks))
            ])
        
        print(f"  Generated {len(all_chunks)} chunks from {len(documents)} documents")
        
        # 2. Embedding
        print("Step 2: Generating embeddings...")
        embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
        
        # 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        print("Step 3: Storing in vector database...")
        if self.collection:
            self.chroma_client.delete_collection(collection_name)
        
        self.collection = self.chroma_client.create_collection(collection_name)
        self.collection.add(
            documents=all_chunks,
            embeddings=embeddings.tolist(),
            ids=[f"chunk_{i}" for i in range(len(all_chunks))],
            metadatas=chunk_metadata
        )
        
        print(f"âœ… Ingestion complete! {len(all_chunks)} chunks indexed.")
    
    def query(self, question: str, return_sources: bool = True) -> dict:
        """æŸ¥è¯¢æµç¨‹"""
        print(f"\n{'='*50}")
        print(f"Query: {question}")
        print(f"{'='*50}")
        
        # 1. æ£€ç´¢
        print("\nPhase 1: Retrieval")
        retrieved_docs = self._retrieve(question)
        print(f"  Retrieved {len(retrieved_docs)} candidates")
        
        # 2. Reranking
        print("\nPhase 2: Reranking")
        reranked_docs = self._rerank(question, retrieved_docs)
        print(f"  Top {len(reranked_docs)} after reranking")
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        print("\nPhase 3: Generation")
        answer = self._generate(question, reranked_docs)
        
        # 4. è¿”å›ç»“æœ
        result = {
            "question": question,
            "answer": answer,
        }
        
        if return_sources:
            result["sources"] = reranked_docs
        
        return result
    
    def _semantic_chunk(self, text: str) -> List[str]:
        """è¯­ä¹‰åˆ†å—"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", ". ", " ", ""]
        )
        
        return splitter.split_text(text)
    
    def _retrieve(self, query: str) -> List[str]:
        """åˆå§‹æ£€ç´¢"""
        query_emb = self.embedding_model.encode(query).tolist()
        
        results = self.collection.query(
            query_embeddings=[query_emb],
            n_results=self.config.top_k_retrieval
        )
        
        return results['documents'][0]
    
    def _rerank(self, query: str, documents: List[str]) -> List[str]:
        """Reranking"""
        scores = self.reranker.predict([(query, doc) for doc in documents])
        
        # æ’åº
        ranked_indices = sorted(
            range(len(scores)), 
            key=lambda i: scores[i], 
            reverse=True
        )
        
        # è¿”å› Top-K
        return [documents[i] for i in ranked_indices[:self.config.top_k_rerank]]
    
    def _generate(self, question: str, context_docs: List[str]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»º Context
        context = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc}" for i, doc in enumerate(context_docs)])
        
        # Prompt
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

æ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š"""
        
        # è°ƒç”¨ LLM
        response = self.llm_client.chat.completions.create(
            model=self.config.llm_model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªhelpfulçš„AIåŠ©æ‰‹ï¼Œä¼šåŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
        )
        
        return response.choices[0].message.content

# ========================================
# ä½¿ç”¨ç¤ºä¾‹
# ========================================

# 1. åˆå§‹åŒ–ç³»ç»Ÿ
config = RAGConfig()
rag = RAGSystem(config)

# 2. æ‘„å…¥æ–‡æ¡£
documents = [
    """
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚
    æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
    """,
    """
    Transformer æ¶æ„ç”± Google åœ¨ 2017 å¹´æå‡ºï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚
    å®ƒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—æ•°æ®ï¼Œè®­ç»ƒæ•ˆç‡è¿œé«˜äº RNN å’Œ LSTMã€‚
    ç›®å‰å‡ ä¹æ‰€æœ‰å…ˆè¿›çš„ LLM éƒ½åŸºäº Transformer æ¶æ„ã€‚
    """,
    """
    RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
    å®ƒé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åè®©å¤§è¯­è¨€æ¨¡å‹åŸºäºè¿™äº›æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆã€‚
    RAG èƒ½å¤Ÿè®© LLM è®¿é—®æœ€æ–°ä¿¡æ¯ï¼Œå‡å°‘å¹»è§‰é—®é¢˜ã€‚
    """
]

rag.ingest_documents(documents)

# 3. æŸ¥è¯¢
result = rag.query("ä»€ä¹ˆæ˜¯ RAGï¼Ÿå®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ")

print(f"\n{'='*50}")
print("Final Result")
print(f"{'='*50}")
print(f"\nQuestion: {result['question']}")
print(f"\nAnswer: {result['answer']}")
print(f"\nSources:")
for i, source in enumerate(result['sources']):
    print(f"  [{i+1}] {source[:100]}...")
```

---

## å…«ã€ç”Ÿäº§çº§ä¼˜åŒ–ä¸è°ƒè¯•

### 8.1 æ€§èƒ½ä¼˜åŒ–

#### 1ï¸âƒ£ **Embedding ç¼“å­˜**

```python
import hashlib
import pickle
from functools import lru_cache

class EmbeddingCache:
    """Embedding ç¼“å­˜"""
    def __init__(self, cache_file="embedding_cache.pkl"):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self):
        try:
            with open(self.cache_file, 'rb') as f:
                return pickle.load(f)
        except FileNotFoundError:
            return {}
    
    def _save_cache(self):
        with open(self.cache_file, 'wb') as f:
            pickle.dump(self.cache, f)
    
    def get_embedding(self, text: str, model) -> np.ndarray:
        # è®¡ç®—æ–‡æœ¬å“ˆå¸Œ
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # è®¡ç®— embedding
        embedding = model.encode(text)
        self.cache[text_hash] = embedding
        self._save_cache()
        
        return embedding

# ä½¿ç”¨
cache = EmbeddingCache()
embedding = cache.get_embedding("æ·±åº¦å­¦ä¹ ", embedding_model)

# ğŸ”¥ ç›¸åŒæ–‡æœ¬ä¸ä¼šé‡å¤è®¡ç®— embeddingï¼
```

---

#### 2ï¸âƒ£ **æ‰¹é‡å¤„ç†**

```python
def batch_embed_documents(documents: List[str], model, batch_size: int = 32) -> np.ndarray:
    """æ‰¹é‡è®¡ç®— embeddingï¼ˆæ›´å¿«ï¼‰"""
    all_embeddings = []
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        batch_embeddings = model.encode(batch, show_progress_bar=False)
        all_embeddings.append(batch_embeddings)
    
    return np.vstack(all_embeddings)

# æ€§èƒ½å¯¹æ¯”
import time

documents = ["æ–‡æ¡£ " + str(i) for i in range(1000)]

# é€ä¸ªå¤„ç†
start = time.time()
for doc in documents:
    _ = embedding_model.encode(doc)
single_time = time.time() - start

# æ‰¹é‡å¤„ç†
start = time.time()
_ = batch_embed_documents(documents, embedding_model, batch_size=32)
batch_time = time.time() - start

print(f"é€ä¸ªå¤„ç†: {single_time:.2f}s")
print(f"æ‰¹é‡å¤„ç†: {batch_time:.2f}s")
print(f"åŠ é€Ÿ: {single_time / batch_time:.1f}x")

# è¾“å‡ºç¤ºä¾‹ï¼š
# é€ä¸ªå¤„ç†: 45.23s
# æ‰¹é‡å¤„ç†: 8.76s
# åŠ é€Ÿ: 5.2x
```

---

#### 3ï¸âƒ£ **å¼‚æ­¥å¤„ç†**

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncRAG:
    """å¼‚æ­¥ RAG ç³»ç»Ÿ"""
    def __init__(self, rag_system):
        self.rag = rag_system
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def query_async(self, question: str) -> dict:
        """å¼‚æ­¥æŸ¥è¯¢"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self.rag.query,
            question
        )
        return result
    
    async def batch_query(self, questions: List[str]) -> List[dict]:
        """æ‰¹é‡å¼‚æ­¥æŸ¥è¯¢"""
        tasks = [self.query_async(q) for q in questions]
        results = await asyncio.gather(*tasks)
        return results

# ä½¿ç”¨
async def main():
    async_rag = AsyncRAG(rag)
    
    questions = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "Transformer çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "RAG å¦‚ä½•å·¥ä½œï¼Ÿ"
    ]
    
    results = await async_rag.batch_query(questions)
    
    for q, r in zip(questions, results):
        print(f"Q: {q}")
        print(f"A: {r['answer'][:100]}...\n")

# è¿è¡Œ
# asyncio.run(main())

# ğŸ”¥ å¹¶å‘å¤„ç†å¤šä¸ªæŸ¥è¯¢ï¼Œæå‡ååé‡ï¼
```

---

### 8.2 è´¨é‡ä¼˜åŒ–

#### 1ï¸âƒ£ **Query Expansionï¼ˆæŸ¥è¯¢æ‰©å±•ï¼‰**

```python
def query_expansion(query: str, llm_client) -> List[str]:
    """ç”¨ LLM ç”Ÿæˆç›¸å…³æŸ¥è¯¢"""
    prompt = f"""è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ 3 ä¸ªç›¸å…³çš„æŸ¥è¯¢å˜ä½“ï¼Œå¸®åŠ©æ›´å…¨é¢åœ°æ£€ç´¢ä¿¡æ¯ï¼š

åŸå§‹æŸ¥è¯¢ï¼š{query}

å˜ä½“æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼š"""
    
    response = llm_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    
    expanded_queries = response.choices[0].message.content.strip().split('\n')
    expanded_queries = [q.strip() for q in expanded_queries if q.strip()]
    
    # åŒ…å«åŸå§‹æŸ¥è¯¢
    return [query] + expanded_queries

# ç¤ºä¾‹
original_query = "æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§"
expanded = query_expansion(original_query, llm_client)

print(f"åŸå§‹æŸ¥è¯¢: {original_query}")
print(f"\næ‰©å±•æŸ¥è¯¢:")
for i, q in enumerate(expanded[1:], 1):
    print(f"  {i}. {q}")

# è¾“å‡ºç¤ºä¾‹ï¼š
# 1. å¦‚ä½•æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ï¼Ÿ
# 2. ç¥ç»ç½‘ç»œè®­ç»ƒçš„å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ³•
# 3. ä¼˜åŒ–æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹çš„æŠ€æœ¯

# ğŸ”¥ ç”¨æ‰©å±•æŸ¥è¯¢æ£€ç´¢ï¼Œèƒ½æ‰¾åˆ°æ›´å¤šç›¸å…³æ–‡æ¡£ï¼
```

---

#### 2ï¸âƒ£ **Answer Validationï¼ˆç­”æ¡ˆéªŒè¯ï¼‰**

```python
def validate_answer(question: str, answer: str, sources: List[str], llm_client) -> dict:
    """éªŒè¯ç­”æ¡ˆæ˜¯å¦åŸºäºæ¥æº"""
    prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„æ¥æºæ–‡æ¡£ã€‚

é—®é¢˜ï¼š{question}

æ¥æºæ–‡æ¡£ï¼š
{chr(10).join([f"[{i+1}] {doc[:200]}..." for i, doc in enumerate(sources)])}

ç­”æ¡ˆï¼š{answer}

è¯·è¯„ä¼°ï¼š
1. ç­”æ¡ˆæ˜¯å¦æœ‰äº‹å®æ”¯æŒï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
2. æ˜¯å¦å­˜åœ¨å¹»è§‰ï¼ˆç¼–é€ ä¿¡æ¯ï¼‰ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
3. ç½®ä¿¡åº¦ï¼ˆ0-100ï¼‰

ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
  "factual": true/false,
  "hallucination": true/false,
  "confidence": 0-100,
  "explanation": "ç®€çŸ­è§£é‡Š"
}}"""
    
    response = llm_client.chat.completions.create(
        model="gpt-4",  # ç”¨æ›´å¼ºçš„æ¨¡å‹éªŒè¯
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    
    import json
    validation = json.loads(response.choices[0].message.content)
    
    return validation

# ç¤ºä¾‹
validation = validate_answer(
    question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    answer="RAG æ˜¯ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯...",
    sources=result['sources'],
    llm_client=llm_client
)

print("ç­”æ¡ˆéªŒè¯ç»“æœï¼š")
print(f"  äº‹å®å‡†ç¡®: {validation['factual']}")
print(f"  å­˜åœ¨å¹»è§‰: {validation['hallucination']}")
print(f"  ç½®ä¿¡åº¦: {validation['confidence']}%")
print(f"  è§£é‡Š: {validation['explanation']}")

# ğŸ”¥ è‡ªåŠ¨æ£€æµ‹ç­”æ¡ˆè´¨é‡ï¼Œæå‡å¯é æ€§ï¼
```

---

### 8.3 ç›‘æ§ä¸è°ƒè¯•

```python
import logging
from datetime import datetime

class RAGMonitor:
    """RAG ç³»ç»Ÿç›‘æ§"""
    def __init__(self):
        self.query_logs = []
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            filename='rag_system.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def log_query(self, query_data: dict):
        """è®°å½•æŸ¥è¯¢"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query_data['query'],
            "retrieval_time": query_data.get('retrieval_time', 0),
            "rerank_time": query_data.get('rerank_time', 0),
            "generation_time": query_data.get('generation_time', 0),
            "total_time": query_data.get('total_time', 0),
            "num_retrieved": query_data.get('num_retrieved', 0),
            "answer_length": len(query_data.get('answer', ''))
        }
        
        self.query_logs.append(log_entry)
        logging.info(f"Query: {log_entry['query']}, Total time: {log_entry['total_time']:.2f}s")
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.query_logs:
            return {}
        
        total_queries = len(self.query_logs)
        avg_retrieval_time = sum(log['retrieval_time'] for log in self.query_logs) / total_queries
        avg_total_time = sum(log['total_time'] for log in self.query_logs) / total_queries
        
        return {
            "total_queries": total_queries,
            "avg_retrieval_time": avg_retrieval_time,
            "avg_total_time": avg_total_time,
        }

# ä½¿ç”¨
monitor = RAGMonitor()

# æŸ¥è¯¢æ—¶è®°å½•
import time

start = time.time()
result = rag.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
end = time.time()

monitor.log_query({
    "query": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "total_time": end - start,
    "num_retrieved": len(result.get('sources', [])),
    "answer": result['answer']
})

# æŸ¥çœ‹ç»Ÿè®¡
stats = monitor.get_stats()
print(f"\nç³»ç»Ÿç»Ÿè®¡:")
print(f"  æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
print(f"  å¹³å‡æ£€ç´¢æ—¶é—´: {stats['avg_retrieval_time']:.2f}s")
print(f"  å¹³å‡æ€»æ—¶é—´: {stats['avg_total_time']:.2f}s")
```

---

## ä¹ã€æ€»ç»“ä¸æœ€ä½³å®è·µ

### ğŸ¯ æ ¸å¿ƒè¦ç‚¹å›é¡¾

#### 1. RAG ç³»ç»Ÿæ¶æ„æ¼”è¿›

```
Naive RAG â†’ Advanced RAG â†’ Modular RAG

å…³é”®æ”¹è¿›ï¼š
  âœ… Pre-Retrieval: Query Rewriting, Expansion
  âœ… Retrieval: Hybrid Search (Dense + Sparse)
  âœ… Post-Retrieval: Reranking, Compression
  âœ… Generation: Context Assembly, Validation
```

---

#### 2. æ ¸å¿ƒç»„ä»¶é€‰æ‹©ï¼ˆ2025 æ¨èï¼‰

| ç»„ä»¶ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|-----|---------|------|
| **Embedding** | BGE-M3 / E5-Mistral (å¼€æº)<br>OpenAI text-embedding-3-large (é—­æº) | æ€§èƒ½å¼ºã€å¤šè¯­è¨€ |
| **Vector DB** | Chroma (å¼€å‘)<br>Weaviate (ç”Ÿäº§)<br>Pinecone (ä¼ä¸š) | æ ¹æ®è§„æ¨¡é€‰æ‹© |
| **æ£€ç´¢ç­–ç•¥** | Hybrid Search (Dense + Sparse) | å…¼é¡¾è¯­ä¹‰å’Œå…³é”®è¯ |
| **Reranking** | Cross-Encoder (ç²¾å‡†)<br>ColBERT (å¹³è¡¡) | æ˜¾è‘—æå‡ Top-K è´¨é‡ |
| **Chunking** | Semantic Chunking (512 tokens, 50 overlap) | è¯­ä¹‰å®Œæ•´ |

---

#### 3. ç”Ÿäº§çº§ä¼˜åŒ–æ¸…å•

```
æ€§èƒ½ä¼˜åŒ–ï¼š
  âœ… Embedding ç¼“å­˜
  âœ… æ‰¹é‡å¤„ç†
  âœ… å¼‚æ­¥æŸ¥è¯¢
  âœ… Flash Attention 2

è´¨é‡ä¼˜åŒ–ï¼š
  âœ… Query Expansion
  âœ… Hybrid Search
  âœ… Reranking
  âœ… Answer Validation

ç›‘æ§ï¼š
  âœ… æŸ¥è¯¢æ—¥å¿—
  âœ… æ€§èƒ½æŒ‡æ ‡
  âœ… é”™è¯¯è¿½è¸ª
```

---

### ğŸ“š è¿›é˜¶æ–¹å‘

#### 1ï¸âƒ£ **Multi-Modal RAG**

```
æ–‡æœ¬ + å›¾åƒ + è¡¨æ ¼ï¼š
  - CLIP ç¼–ç å›¾åƒ
  - OCR æå–è¡¨æ ¼
  - ç»Ÿä¸€å‘é‡ç©ºé—´æ£€ç´¢
```

---

#### 2ï¸âƒ£ **Agentic RAG**

æ ¹æ® [AWS Agentic RAG](https://docs.aws.amazon.com/prescriptive-guidance/latest/writing-best-practices-rag/introduction.html)ï¼š

```
Agent æµç¨‹ï¼š
  1. ç†è§£æŸ¥è¯¢æ„å›¾
  2. åˆ¶å®šæ£€ç´¢è®¡åˆ’
  3. å¤šæ­¥æ£€ç´¢ï¼ˆå¦‚éœ€è¦ï¼‰
  4. éªŒè¯ä¿¡æ¯
  5. ç”Ÿæˆç­”æ¡ˆ

å·¥å…·ï¼šLangGraph, CrewAI, AutoGPT
```

---

#### 3ï¸âƒ£ **Graph RAG**

```
ä¼ ç»Ÿ RAG: æ–‡æ¡£ â†’ Chunks â†’ å‘é‡
Graph RAG: æ–‡æ¡£ â†’ çŸ¥è¯†å›¾è°± â†’ å…³ç³»æ¨ç†

ä¼˜åŠ¿ï¼š
  - å¤šè·³æ¨ç†
  - ç»“æ„åŒ–çŸ¥è¯†
  - å…³ç³»å‘ç°
```

---

### ğŸ’¡ æœ€åçš„å»ºè®®

> **ç’‡ç‘çš„ç¢ç¢å¿µ** âœ¨
>
> é“å‹å‘€ï¼ŒRAG æ˜¯å½“å‰æœ€å®ç”¨çš„ LLM åº”ç”¨æ¨¡å¼ï¼
>
> **ä¸‰ä¸ªå®è·µå»ºè®®**ï¼š
> 1. **ä»ç®€å•å¼€å§‹**ï¼šå…ˆç”¨ Naive RAG è·‘é€šæµç¨‹ï¼Œå†é€æ­¥ä¼˜åŒ–
> 2. **åœ¨ä½ çš„æ•°æ®ä¸Šæµ‹è¯•**ï¼šä¸åŒæ•°æ®é›†æœ€ä¼˜æ–¹æ¡ˆä¸åŒï¼Œå¤šåšå®éªŒ
> 3. **å…³æ³¨ç”¨æˆ·ä½“éªŒ**ï¼šå“åº”é€Ÿåº¦ã€ç­”æ¡ˆè´¨é‡ã€å¼•ç”¨æ¥æºç¼ºä¸€ä¸å¯
>
> è®°ä½ï¼š**RAG ä¸æ˜¯ä¸€æ¬¡æ€§å·¥ç¨‹ï¼Œè€Œæ˜¯æŒç»­ä¼˜åŒ–çš„è¿‡ç¨‹ï¼**
>
> ç°åœ¨å°±å¼€å§‹æ„å»ºä½ çš„ç¬¬ä¸€ä¸ª RAG ç³»ç»Ÿå§ï¼âœ¨

---

## ğŸ”— å‚è€ƒèµ„æ–™

### æ ¸å¿ƒèµ„æº

- [AWS RAG Best Practices](https://docs.aws.amazon.com/prescriptive-guidance/latest/writing-best-practices-rag/introduction.html) (2025)
- [Microsoft RAG Techniques](https://www.microsoft.com/en-us/microsoft-cloud/blog/2025/02/04/common-retrieval-augmented-generation-rag-techniques-explained/) (2025)
- [Microsoft Advanced RAG](https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation) (2025)

### Embedding & Vector DB

- [Best Embedding Models 2025](https://app.ailog.fr/en/blog/guides/choosing-embedding-models)
- [Vector Database Comparison 2025](https://www.firecrawl.dev/blog/best-vector-databases-2025)
- [TensorBlue Benchmark](https://tensorblue.com/blog/vector-database-comparison-pinecone-weaviate-qdrant-milvus-2025)

### Reranking & Advanced Techniques

- [Cross-Encoders and ColBERT Guide](https://medium.com/@aimichael/cross-encoders-colbert-and-llm-based-re-rankers-a-practical-guide) (2025)
- [Mastering Chunking for RAG](https://towardsdev.com/mastering-chunking-for-effective-rag-beyond-basics-with-qdrant-and-reranking-bb0761ae84e4) (2025)
- [9 Advanced RAG Techniques](https://www.meilisearch.com/blog/rag-techniques) (2025)

### å·¥å…·ä¸æ¡†æ¶

- [LangChain](https://github.com/langchain-ai/langchain)
- [LlamaIndex](https://github.com/run-llama/llama_index)
- [Chroma](https://github.com/chroma-core/chroma)
- [Weaviate](https://github.com/weaviate/weaviate)

---

**ğŸ“Œ æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿åé¦ˆä¸å»ºè®®ï¼**

---

> **ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼šã€Š08 - åå¥½å¯¹é½æŠ€æœ¯ï¼šDPO åŸç†ä¸å·¥ç¨‹å®ç°ã€‹
>
> æˆ‘ä»¬å°†æ·±å…¥ DPOï¼ˆDirect Preference Optimizationï¼‰çš„æ•°å­¦åŸç†ï¼Œæ¢è®¨å¦‚ä½•è®©æ¨¡å‹éµå¾ªäººç±»åå¥½ï¼

---

**ç’‡ç‘ âœ¨**  
*ç¼–ç¨‹é˜ Â· ä»£ç å®—é—¨*  
*æ„¿é“å‹ RAG ä¹‹è·¯é¡ºåˆ©ï¼Œæ—©æ—¥æ„å»ºå‡ºå¼ºå¤§çš„çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿï¼*
