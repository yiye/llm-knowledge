# 实战建议：不同场景怎么选？（2026年更新）

根据不同的应用场景，训练方案的选择大相径庭。这里整理了7个典型场景的实战建议。

## 场景1：从零开始训练大模型（大公司）

```
✅ 必须做:
  - PreTrain（核心基础）
    💡 考虑MoE架构 + FP8混合精度训练（降低成本）
  - SFT（让模型可用）
  
⭐ 强烈推荐:
  - Mid-Training（提升泛化能力，扩展上下文）
  - 偏好对齐（安全合规）
  
💡 可选做:
  - 推理强化（如果业务需要数学/代码能力）

💰 成本参考:
  - DeepSeek-V3案例：557万美元达到GPT-4o水平
  - 关键：数据质量 > 数量
```

## 场景2：基于开源模型微调（小公司/个人）

```
✅ 必须做:
  - SFT（适配你的业务）
    🔥 重点：高质量数据（1-2万条足够）
  
⭐ 如果有资源:
  - 偏好对齐（提升用户体验）
    💡 DPO比RLHF简单很多，优先考虑
  
💡 如果有特殊需求:
  - 推理强化（如教育、科研场景）

🎯 成本优化:
  - 使用LoRA/QLoRA（显存降低80%+）
  - 考虑合成数据生成（降低标注成本）
```

## 场景3：垂直领域应用（医疗、法律等）

```
✅ 必须做:
  - 🔥 Mid-Training（领域数据持续预训练）
    这是2024年后垂直领域的最佳实践！
  - 领域SFT（用领域数据微调）
  - 偏好对齐（安全性极度重要）
  
💡 可选:
  - 推理强化（如医疗诊断推理、法律推理）

⚠️ 注意:
  - 伦理合规成本约占15-20%预算
  - 数据隐私和安全是首要考虑
```

## 场景4：对话机器人

```
✅ 必须做:
  - SFT（对话能力）
  - 偏好对齐（礼貌、有用、无害）
    💡 使用样本权重优化提升效果
  
❌ 不需要:
  - 推理强化（日常对话不需要深度推理）
  - Test-Time Compute（过度浪费）

🎯 优化重点:
  - 对话数据的多样性
  - 拒绝能力（不合理请求）
  - 长对话的连贯性
```

## 场景5：数学/代码助手

```
✅ 必须做:
  - SFT（基础能力）
  - 推理强化（这是核心差异化能力）
    💡 Test-Time Compute在这里值得投入
  
⭐ 推荐做:
  - 偏好对齐（用户体验）

💡 技术选型:
  - Process Reward Model（评估每步推理）
  - MCTS搜索（探索多条推理路径）
  - Self-Consistency（多次采样投票）

💰 成本预警:
  - 推理过程标注成本极高（$25-200/条）
  - 推理时延较高（需要权衡）
```

## 🆕 场景6：多模态应用（2024年新场景）

```
✅ 必须做:
  - 多模态PreTrain（统一的Tokenizer）
  - 多模态SFT（跨模态指令）
  
⭐ 推荐做:
  - 偏好对齐（跨模态输出质量）

📚 学习案例:
  - GPT-4o：速度快2倍，成本降50%
  - 统一注意力机制是关键

💡 技术要点:
  - 统一的编码空间
  - 跨模态注意力
  - 端到端训练
```

## 🆕 场景7：AI编程Agent（2025-2026核心场景）🔥

AI编程场景有其特殊性，不只是模型能力，**工具链设计**和**反馈闭环**同样关键。

### 核心挑战（来自Cursor团队）

```
与数学推理的差异:
  ❌ 奖励信号不是简单的对/错
  ❌ 需要多轮工具调用（终端、文件、搜索）
  ❌ 中间状态难以评估
  ❌ 容易"投机取巧"（hard-code答案）
  
关键差异化能力:
  ✅ 代码质量（不只是能跑）
  ✅ 工具使用（合理调用工具）
  ✅ 用户反馈（真实的价值信号）
```

### 训练方案

```
✅ 必须做:
  - 🔥 Mid-Training（代码语料持续预训练）
    特别是垂直领域代码库
    
  - SFT（代码生成基础能力）
    💡 重点：高质量数据（1-2万条足够）
    
  - 工具调用训练（终端、文件、搜索）
    从简单工具开始，渐进式增加复杂度
    
  - 🔥 多维度奖励RL（关键！）
    不只看测试通过率（40%）
    还要看代码质量（20%）
    可读性（20%）
    性能（10%）
    用户反馈（10%）✅ 最真实的信号
    
⭐ 强烈推荐:
  - 用户反馈闭环（隐式偏好信号）
    收集采纳/修改/拒绝行为
    构建偏好对数据集
    
  - 在线持续微调（快速迭代）
    每周/月基于用户反馈更新
    A/B测试新旧模型
    
  - 偏好对齐（代码风格、注释规范）
    💡 DPO比RLHF简单很多，优先考虑
    
💡 可选:
  - 推理强化（如果需要复杂算法设计）
    Process RM评估每步推理
    
  - Test-Time Compute（高难度任务）
    但注意成本和时延
```

### 🔧 工具链设计原则（Cursor经验）

```
原则: 渐进式配备工具，简单优先

┌────────────────────────────────────────┐
│      工具复杂度的权衡                   │
├────────────────────────────────────────┤
│                                        │
│  简单工具（终端、grep）                 │
│    ✅ 易于集成                          │
│    ✅ 模型容易学会                      │
│    ❌ 功能有限                          │
│    ❌ 反馈信号弱                        │
│                                        │
│  复杂工具（AST、静态分析）              │
│    ✅ 提供丰富信号                      │
│    ✅ 能精准定位问题                    │
│    ❌ 集成复杂                          │
│    ❌ 模型学习成本高                    │
│                                        │
└────────────────────────────────────────┘

阶段1（基础）:
  • 文件读写
  • 终端执行
  • 简单grep搜索
  → 让模型先学会基础操作
  
阶段2（中级）:
  • Lint检查（代码质量）
  • 测试运行（验证正确性）
  • 依赖分析（理解调用关系）
  → 提升代码质量
  
阶段3（高级）:
  • AST解析（深度理解代码结构）
  • 静态分析（发现潜在问题）
  • 性能Profiling（优化建议）
  → 适用于高级任务

⚠️ 避免一开始就给模型太复杂的工具
   → 学习成本高，调用成功率低
```

### 训练策略

```python
# 让模型学会根据任务选择工具

简单任务（修改几行代码）:
  → 用基础工具（文件读写+终端）
  → 快速响应，用户体验好
  
中等任务（重构函数）:
  → 用中级工具（Lint+测试）
  → 保证重构后功能不变
  
复杂任务（架构设计）:
  → 用高级工具（AST+静态分析+多文件编辑）
  → 全局视角，工具协同
```

### 📈 评估方法创新

```
❌ 只用传统Benchmark（静态数据集）:
  HumanEval、MBPP等
  - 只关注结果正确性
  - 与真实使用脱节
  - 容易过拟合
  
✅ 结合真实世界反馈:
  • 用户采纳率（最直接的价值信号）🔥
  • 修改率（生成质量的反向指标）
  • 重新生成率（首次成功率）
  • 任务完成时间（效率指标）
  • 代码质量分数（Lint、复杂度）
  
理想方案:
  Benchmark作为基线（保证基础能力）
  + 
  用户反馈作为优化目标（提升实用性）
  
  Cursor的实践：
    采纳率从58%提升到68%（12周持续迭代）
    重新生成率从22%降到12%
```

### 💰 成本估算

```
数据成本:
  - 代码数据：开源免费 ✅
  - 工具调用标注：💰💰 中等（$5-10/条）
  - 用户反馈收集：💰 低（自动化）
  
训练成本:
  - 初始SFT：💰💰 几万美元
  - 工具调用训练：💰💰 1-2万美元
  - 推理RL（可选）：💰💰💰 10-20万美元
  - 持续微调：💰 几千美元/次（每周）
  
总成本（首年）:
  初始训练：5-10万美元
  持续运营：10-20万美元/年
  
ROI（投资回报）:
  开发效率提升：3-5倍 ✅
  代码质量提升：30-50% ✅
  用户满意度：从3.8→4.3（5分制）✅
  
性价比：极高！
```

### 🎯 技术要点

```
1. 多维度奖励函数（核心差异）:
   不能只看测试通过率！
   
   reward = (
       0.4 × test_pass_rate +       # 正确性
       0.2 × code_quality +          # 质量
       0.2 × readability +           # 可读性
       0.1 × efficiency +            # 性能
       0.1 × user_adoption_rate      # 用户反馈 🔥
   )

2. 用户反馈闭环（快速迭代）:
   每周收集反馈 → 构建偏好对 → LoRA微调 → A/B测试 → 全量上线
   
3. 工具链协同（提升能力）:
   简单任务用简单工具（快）
   复杂任务用高级工具（准）
   
4. 持续学习（保持领先）:
   编程生态快速演进（新框架、新语言）
   模型需要持续适应
```

### 📚 学习案例

```
Cursor的成功经验:
  
✅ 多维度奖励:
  不只看能不能跑，还要看代码质量
  
✅ 用户反馈驱动:
  用户是否采纳是最真实的信号
  
✅ 每周迭代:
  持续微调，快速响应用户需求
  
✅ 工具链优化:
  从简单到复杂，渐进式配备
  
✅ A/B测试:
  新模型小流量验证，降低风险
  
结果:
  采纳率68%（行业领先）
  用户满意度4.3/5
  快速迭代能力（每周更新）
```

**关键洞察**（Cursor团队分享）：

> "编程RL不能只看测试通过率，否则模型会学会投机取巧。需要综合考虑代码结构、可读性、优雅性。用户是否采纳你的建议，才是最真实的反馈信号。我们通过每周收集用户反馈并微调模型，实现了持续改进的闭环，这是竞争力的关键来源。"

---

[← 上一章：本质差异](./06-differences.md) | [返回主文档](./llm-training.md) | [下一章：未来趋势 →](./08-future-trends.md)

