# 02 - èƒ½åŠ›è¿ç§»åœ°å›¾ï¼šä¼ ç»ŸæŠ€èƒ½å¦‚ä½•å¤ç”¨ä¸å‡çº§

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹å¸ˆè½¬å‹ LLM è®­ç»ƒï¼Œ**ä½ å·²ç»æ‹¥æœ‰ 70% çš„èƒ½åŠ›**ã€‚æœ¬æ–‡å°†ç³»ç»Ÿæ¢³ç†å“ªäº›æŠ€èƒ½å¯ä»¥ç›´æ¥å¤ç”¨ã€å“ªäº›éœ€è¦å‡çº§ï¼Œä»¥åŠå¦‚ä½•æœ€å¤§åŒ–ä½ çš„ä¼˜åŠ¿ã€‚

---

## ğŸ“– å¼•è¨€ï¼šå¥½æ¶ˆæ¯â€”â€”ä½ æ¯”æƒ³è±¡ä¸­æ›´æœ‰ä¼˜åŠ¿

å¾ˆå¤šä¼ ç»Ÿç¨‹åºå‘˜åœ¨è€ƒè™‘è½¬å‹ LLM è®­ç»ƒæ—¶ï¼Œç¬¬ä¸€ååº”æ˜¯ï¼š

> âŒ "æˆ‘æ²¡å­¦è¿‡æœºå™¨å­¦ä¹ ï¼Œæ•°å­¦ä¹Ÿå¿˜å…‰äº†ï¼Œæ˜¯ä¸æ˜¯è¦ä»é›¶å¼€å§‹ï¼Ÿ"  
> âŒ "æˆ‘åªä¼šå†™ä»£ç ï¼ŒML éœ€è¦æ‡‚ç®—æ³•ã€ç»Ÿè®¡å­¦ï¼Œæˆ‘æ ¹æœ¬ä¸å¤Ÿæ ¼..."  
> âŒ "æˆ‘å¾—å…ˆèŠ± 1-2 å¹´æŠŠæ•°å­¦ã€ML ç†è®ºå…¨éƒ¨å­¦å®Œæ‰èƒ½å¼€å§‹..."

**è¿™æ˜¯æœ€å¤§çš„è®¤çŸ¥è¯¯åŒºï¼** ğŸ”¥

æ ¹æ® [Jobs in Data - Software Engineer to ML Transition 2025](https://jobs-in-data.com/blog/software-engineer-transition-to-machine-learning) å’Œ [LinkedIn - Software Engineer to AI/ML 2025](https://www.linkedin.com/pulse/from-software-engineer-aiml-beginners-guide-malaika-f--siogf) çš„è°ƒç ”ï¼š

> **ç°ä»£ LLM å·¥ç¨‹çš„æœ¬è´¨**ï¼š
> - **70% æ˜¯ç³»ç»Ÿå·¥ç¨‹**ï¼ˆåˆ†å¸ƒå¼ç³»ç»Ÿã€æ€§èƒ½ä¼˜åŒ–ã€CI/CDã€ç›‘æ§ï¼‰
> - **30% æ˜¯ ML ç®—æ³•**ï¼ˆæ¨¡å‹è®­ç»ƒã€è¶…å‚æ•°è°ƒä¼˜ï¼‰
>
> è€Œä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹å¸ˆæ°æ°åœ¨**ç³»ç»Ÿå·¥ç¨‹æ–¹é¢å…·æœ‰ç¨€ç¼ºä¼˜åŠ¿**â€”â€”è¿™æ­£æ˜¯è®¸å¤šæ•°æ®ç§‘å­¦å®¶ç¼ºä¹çš„èƒ½åŠ›ï¼

---

### ğŸ”¥ è¡Œä¸šçœŸç›¸ï¼šML å›¢é˜Ÿæœ€ç¼ºçš„ä¸æ˜¯ç®—æ³•ä¸“å®¶

æ ¹æ® [Coursera - ML Roadmap 2026](https://www.coursera.org/resources/ml-learning-roadmap) å’Œ [Medium - ML Engineer 2026 Roadmap](https://medium.com/write-a-catalyst/how-to-become-a-machine-learning-engineer-2026-your-expert-roadmap-7cb2b7e5daa3)ï¼š

**2025-2026 å¹´ ML å›¢é˜Ÿçš„å…¸å‹ç—›ç‚¹**ï¼š

| é—®é¢˜ | åŸå›  | ä¼ ç»Ÿç¨‹åºå‘˜çš„ä¼˜åŠ¿ |
|------|------|----------------|
| **æ¨¡å‹è®­ç»ƒä¸ç¨³å®š** | åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ä¸åˆç† | âœ… ä½ æ‡‚ SPOFã€å®¹é”™ã€é‡è¯•æœºåˆ¶ |
| **æ¨ç†æœåŠ¡å´©æºƒ** | ç¼ºä¹ç”Ÿäº§ç¯å¢ƒç»éªŒ | âœ… ä½ æœ‰è´Ÿè½½å‡è¡¡ã€é™æµã€ç›‘æ§ç»éªŒ |
| **å®éªŒç»“æœæ— æ³•å¤ç°** | ç¼ºä¹ç‰ˆæœ¬æ§åˆ¶æ„è¯† | âœ… ä½ ç†Ÿæ‚‰ Git å·¥ä½œæµ |
| **GPU åˆ©ç”¨ç‡ä½ä¸‹** | ä¸æ‡‚æ€§èƒ½åˆ†æå’Œä¼˜åŒ– | âœ… ä½ ä¼š profilingã€æ€§èƒ½è°ƒä¼˜ |
| **éƒ¨ç½²æµç¨‹æ··ä¹±** | æ²¡æœ‰ CI/CD æµç¨‹ | âœ… ä½ ç†Ÿæ‚‰è‡ªåŠ¨åŒ–éƒ¨ç½² |

**å…³é”®æ´å¯Ÿ**ï¼š

> æ ¹æ® [MLOps Best Practices 2025](https://www.goml.io/blog/mlops-best-practices)ï¼š
> 
> **85% çš„ ML æ¨¡å‹æ°¸è¿œæ— æ³•ä¸Šçº¿ï¼Œ87% çš„ ML é¡¹ç›®å¤±è´¥â€”â€”ä¸æ˜¯å› ä¸ºç®—æ³•ä¸å¤Ÿå¥½ï¼Œè€Œæ˜¯å› ä¸ºç¼ºä¹å·¥ç¨‹èƒ½åŠ›ã€‚**
>
> ä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹å¸ˆåœ¨**ç³»ç»Ÿè®¾è®¡ã€ç¨³å®šæ€§ã€å¯æ‰©å±•æ€§**æ–¹é¢çš„ç»éªŒï¼Œæ­£æ˜¯ ML å›¢é˜Ÿæœ€éœ€è¦çš„ï¼

---

## ğŸ¯ æœ¬æ–‡å¯¼èˆª

### é˜…è¯»æŒ‡å—

| å¦‚æœä½ æƒ³äº†è§£... | ç›´æ¥è·³è½¬åˆ° |
|--------------|----------|
| å“ªäº›æŠ€èƒ½å¯ä»¥ç›´æ¥ç”¨ï¼Ÿ | [ä¸€ã€å¯ç›´æ¥è¿ç§»çš„æ ¸å¿ƒèƒ½åŠ›](#ä¸€å¯ç›´æ¥è¿ç§»çš„æ ¸å¿ƒèƒ½åŠ›-70-çš„ä»·å€¼) |
| å“ªäº›æŠ€èƒ½éœ€è¦å‡çº§ï¼Ÿ | [äºŒã€éœ€è¦"å‡çº§"çš„æŠ€èƒ½](#äºŒéœ€è¦å‡çº§çš„æŠ€èƒ½-ä»-10-åˆ°-100) |
| å¦‚ä½•è§„åˆ’å­¦ä¹ è·¯å¾„ï¼Ÿ | [å››ã€èƒ½åŠ›è¿ç§»è·¯çº¿å›¾](#å››èƒ½åŠ›è¿ç§»è·¯çº¿å›¾-3-6-ä¸ªæœˆè®¡åˆ’) |
| å®é™…æ¡ˆä¾‹æ€ä¹ˆåšï¼Ÿ | [ä¸‰ã€å®æˆ˜æ¡ˆä¾‹](#ä¸‰å®æˆ˜æ¡ˆä¾‹ä»ä¼ ç»Ÿ-cicd-åˆ°-mlops) |

---

## ä¸€ã€å¯ç›´æ¥è¿ç§»çš„æ ¸å¿ƒèƒ½åŠ› (70% çš„ä»·å€¼)

### 1.1 ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡ â­â­â­â­â­

**ä¸ºä»€ä¹ˆè¿™æ˜¯æœ€æœ‰ä»·å€¼çš„æŠ€èƒ½ï¼Ÿ**

LLM è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯**è¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿå·¥ç¨‹**ã€‚æ ¹æ® [arXiv - MegaScale (2024)](https://arxiv.org/abs/2402.15627)ï¼ŒByteDance è®­ç»ƒä¸‡å¡é›†ç¾¤çš„æ ¸å¿ƒæŒ‘æˆ˜ä¸æ˜¯ç®—æ³•ï¼Œè€Œæ˜¯ï¼š
- å¦‚ä½•åœ¨ 10,000+ GPU ä¸Šåè°ƒè®­ç»ƒä»»åŠ¡
- å¦‚ä½•å¤„ç†ç¡¬ä»¶æ•…éšœå’Œç½‘ç»œåˆ†åŒº
- å¦‚ä½•ä¼˜åŒ–é›†ç¾¤èµ„æºåˆ©ç”¨ç‡

**ä½ å·²ç»æŒæ¡çš„èƒ½åŠ›**ï¼š

| ä¼ ç»Ÿç³»ç»Ÿè®¾è®¡ | åœ¨ LLM è®­ç»ƒä¸­çš„åº”ç”¨ | ç›´æ¥å¤ç”¨åº¦ |
|------------|-------------------|----------|
| **å¾®æœåŠ¡æ¶æ„** | è®­ç»ƒ Pipeline è®¾è®¡ï¼ˆæ•°æ®é¢„å¤„ç†ã€è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²ï¼‰ | â­â­â­â­â­ |
| **è´Ÿè½½å‡è¡¡** | å¤š GPU/å¤šèŠ‚ç‚¹è´Ÿè½½åˆ†é… | â­â­â­â­â­ |
| **å®¹é”™è®¾è®¡** | Checkpoint æœºåˆ¶ã€è®­ç»ƒä»»åŠ¡è‡ªåŠ¨é‡å¯ | â­â­â­â­â­ |
| **èµ„æºè°ƒåº¦** | GPU é›†ç¾¤è°ƒåº¦ã€Spot å®ä¾‹ç®¡ç† | â­â­â­â­â­ |
| **ç¼“å­˜ç­–ç•¥** | æ•°æ®é›†ç¼“å­˜ã€KV Cache ä¼˜åŒ– | â­â­â­â­ |
| **å¼‚æ­¥å¤„ç†** | æ•°æ®é¢„å¤„ç†å¼‚æ­¥åŒ–ã€æµå¼æ¨ç† | â­â­â­â­ |

---

**å®é™…æ¡ˆä¾‹ï¼šä¼ ç»Ÿåç«¯æ¶æ„ vs ML è®­ç»ƒæ¶æ„**

```python
# ä¼ ç»Ÿå¾®æœåŠ¡æ¶æ„
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API GW  â”‚â”€â”€â”€â–¶â”‚ Service  â”‚â”€â”€â”€â–¶â”‚   DB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache   â”‚    â”‚  Queue   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# ML è®­ç»ƒæ¶æ„ï¼ˆæœ¬è´¨ç›¸åŒï¼ï¼‰
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data     â”‚â”€â”€â”€â–¶â”‚ Training â”‚â”€â”€â”€â–¶â”‚  Model   â”‚
â”‚ Pipeline â”‚    â”‚ Service  â”‚    â”‚ Registry â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature  â”‚    â”‚ Exp      â”‚
â”‚ Store    â”‚    â”‚ Tracking â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
```

**ä½ å·²ç»ç†è§£çš„æ ¸å¿ƒæ¦‚å¿µ**ï¼Œåœ¨ ML ä¸­**ç›´æ¥é€‚ç”¨**ï¼š
- âœ… æœåŠ¡æ‹†åˆ†ä¸è§£è€¦ï¼ˆæ•°æ® / è®­ç»ƒ / æ¨ç†åˆ†ç¦»ï¼‰
- âœ… çŠ¶æ€ç®¡ç†ï¼ˆCheckpoint = æ•°æ®åº“å¿«ç…§ï¼‰
- âœ… å¹‚ç­‰æ€§è®¾è®¡ï¼ˆè®­ç»ƒä»»åŠ¡å¯é‡è¯•ï¼‰
- âœ… ç›‘æ§å‘Šè­¦ï¼ˆGPU åˆ©ç”¨ç‡ = CPU åˆ©ç”¨ç‡ï¼‰

---

### 1.2 ğŸ”„ CI/CD ä¸è‡ªåŠ¨åŒ– â­â­â­â­â­

**ä¸ºä»€ä¹ˆ CI/CD åœ¨ ML ä¸­æ›´é‡è¦ï¼Ÿ**

æ ¹æ® [MLOps Best Practices 2025](https://www.mlopscrew.com/blog/cicd-best-practices-for-accelerating-mlops-deployment)ï¼š

> **ä¼ ç»Ÿè½¯ä»¶ CI/CD** åªéœ€è¦æµ‹è¯•ä»£ç ã€‚  
> **ML CI/CD** éœ€è¦æµ‹è¯•ï¼šä»£ç  + æ•°æ® + æ¨¡å‹ + é…ç½® + ç¯å¢ƒã€‚
>
> å¤æ‚åº¦æ˜¯ä¼ ç»Ÿè½¯ä»¶çš„ **5-10 å€**ï¼Œè€Œä½ çš„ CI/CD ç»éªŒæ­£æ˜¯ç ´å±€å…³é”®ï¼

---

**ä¼ ç»Ÿ CI/CD vs ML CI/CD å¯¹æ¯”**

| ç»´åº¦ | ä¼ ç»Ÿè½¯ä»¶ CI/CD | ML CI/CD (MLOps) | ä½ çš„ä¼˜åŠ¿ |
|------|--------------|----------------|---------|
| **ä»£ç æµ‹è¯•** | å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯• | å•å…ƒæµ‹è¯• + **æ¨¡å‹æµ‹è¯•**ï¼ˆå‡†ç¡®ç‡ã€æ¨ç†å»¶è¿Ÿï¼‰ | âœ… ç†Ÿæ‚‰æµ‹è¯•æ¡†æ¶ |
| **éƒ¨ç½²ç­–ç•¥** | Blue-Greenã€Canary | Blue-Greenã€Canaryã€**Shadow Deployment** | âœ… éƒ¨ç½²ç­–ç•¥ç›´æ¥å¤ç”¨ |
| **å›æ»šæœºåˆ¶** | ä»£ç å›æ»š | ä»£ç  + **æ¨¡å‹ç‰ˆæœ¬å›æ»š** | âœ… å›æ»šé€»è¾‘ç›¸åŒ |
| **è§¦å‘æ¡ä»¶** | ä»£ç æäº¤è§¦å‘ | ä»£ç æäº¤ + **æ•°æ®å˜åŒ–** + **æ€§èƒ½ä¸‹é™**è§¦å‘ | âœ… è§¦å‘æœºåˆ¶æ‰©å±•å³å¯ |
| **éªŒè¯æ ‡å‡†** | æµ‹è¯•ç”¨ä¾‹é€šè¿‡ | æµ‹è¯•é€šè¿‡ + **æ¨¡å‹æŒ‡æ ‡è¾¾æ ‡** + **æ•°æ®éªŒè¯** | âœ… æ–°å¢éªŒè¯æ­¥éª¤ |

---

**å®é™…æ¡ˆä¾‹ï¼šä¼ ç»Ÿ CI/CD Pipeline vs ML CI/CD Pipeline**

```yaml
# ä¼ ç»Ÿ CI/CD (.github/workflows/deploy.yml)
name: Deploy Backend Service
on: 
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: pytest tests/
      
  deploy:
    needs: test
    steps:
      - name: Build Docker image
        run: docker build -t myapp:latest .
      - name: Deploy to production
        run: kubectl apply -f k8s/deployment.yaml
```

```yaml
# ML CI/CD (.github/workflows/ml-deploy.yml)
name: Deploy ML Model
on: 
  push:
    branches: [main]
  # ğŸ†• æ–°å¢è§¦å‘æ¡ä»¶ï¼šæ•°æ®å˜åŒ–
  repository_dispatch:
    types: [data-updated]

jobs:
  validate-data:  # ğŸ†• æ•°æ®éªŒè¯æ­¥éª¤
    runs-on: ubuntu-latest
    steps:
      - name: Validate data schema
        run: python scripts/validate_data.py
      - name: Check data drift
        run: python scripts/check_drift.py
  
  test-model:
    needs: validate-data
    steps:
      - name: Run unit tests
        run: pytest tests/
      - name: Train model  # ğŸ†• æ¨¡å‹è®­ç»ƒ
        run: python train.py
      - name: Evaluate model  # ğŸ†• æ¨¡å‹è¯„ä¼°
        run: python evaluate.py --min-accuracy 0.90
      - name: Test inference latency  # ğŸ†• æ€§èƒ½æµ‹è¯•
        run: python test_latency.py --max-p95 50ms
      
  deploy:
    needs: test-model
    steps:
      - name: Register model  # ğŸ†• æ¨¡å‹æ³¨å†Œ
        run: mlflow models register --model-uri models/my-model
      - name: Deploy to staging (Canary 10%)  # âœ… éƒ¨ç½²ç­–ç•¥ç›¸åŒ
        run: kubectl apply -f k8s/canary-10.yaml
      - name: Monitor performance  # ğŸ†• ç›‘æ§æŒ‡æ ‡
        run: python monitor.py --duration 1h
      - name: Promote to 100% or rollback
        run: python decide_rollout.py
```

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ70%ï¼‰ï¼š
- Git å·¥ä½œæµï¼ˆä»£ç å®¡æŸ¥ã€åˆ†æ”¯ç®¡ç†ï¼‰
- æµ‹è¯•è‡ªåŠ¨åŒ–ï¼ˆpytestã€GitHub Actionsï¼‰
- éƒ¨ç½²ç­–ç•¥ï¼ˆCanaryã€Blue-Greenï¼‰
- ç›‘æ§å‘Šè­¦ï¼ˆPrometheusã€Grafanaï¼‰

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ30%ï¼‰ï¼š
- æ•°æ®éªŒè¯ï¼ˆschema æ£€æŸ¥ã€drift æ£€æµ‹ï¼‰
- æ¨¡å‹è¯„ä¼°ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ã€å»¶è¿Ÿï¼‰
- æ¨¡å‹æ³¨å†Œï¼ˆMLflowã€Model Registryï¼‰

**å®ç°éš¾åº¦**ï¼šâ­â­ï¼ˆåªéœ€æ‰©å±•ç°æœ‰çŸ¥è¯†ï¼‰

---

### 1.3 ğŸ“¦ å®¹å™¨åŒ–ä¸ç¼–æ’ (Docker / Kubernetes) â­â­â­â­â­

**ä¸ºä»€ä¹ˆå®¹å™¨åŒ–åœ¨ ML ä¸­è‡³å…³é‡è¦ï¼Ÿ**

ML è®­ç»ƒé¢ä¸´çš„æœ€å¤§ç—›ç‚¹ä¹‹ä¸€æ˜¯**ç¯å¢ƒä¸€è‡´æ€§**ï¼š
- æœ¬åœ°è·‘å¾—é€šï¼ŒæœåŠ¡å™¨è·‘ä¸é€šï¼ˆPython ç‰ˆæœ¬ã€CUDA ç‰ˆæœ¬ã€ä¾èµ–åº“ï¼‰
- æ•°æ®ç§‘å­¦å®¶çš„å®éªŒæ— æ³•å¤ç°
- å¤šä¸ªå®éªŒå…±äº«ç¯å¢ƒå¯¼è‡´ä¾èµ–å†²çª

**ä½ çš„ Docker/K8s ç»éªŒå¯ä»¥ç›´æ¥è§£å†³è¿™äº›é—®é¢˜ï¼**

---

**ä¼ ç»Ÿå®¹å™¨åŒ– vs ML å®¹å™¨åŒ–å¯¹æ¯”**

| æŠ€æœ¯æ ˆ | ä¼ ç»Ÿåº”ç”¨ | ML åº”ç”¨ | å¤ç”¨åº¦ |
|-------|---------|---------|-------|
| **Docker** | åº”ç”¨å®¹å™¨åŒ– | è®­ç»ƒ/æ¨ç†ç¯å¢ƒå®¹å™¨åŒ– | â­â­â­â­â­ |
| **Kubernetes** | æœåŠ¡ç¼–æ’ | GPU é›†ç¾¤ä»»åŠ¡ç¼–æ’ | â­â­â­â­â­ |
| **Helm Charts** | åº”ç”¨éƒ¨ç½²æ¨¡æ¿ | ML Pipeline éƒ¨ç½²æ¨¡æ¿ | â­â­â­â­â­ |
| **èµ„æºé™åˆ¶** | CPU/Memory limits | CPU/Memory + **GPU limits** | â­â­â­â­ |
| **ConfigMap/Secret** | é…ç½®ç®¡ç† | é…ç½® + **æ¨¡å‹è¶…å‚æ•°ç®¡ç†** | â­â­â­â­â­ |

---

**å®é™…æ¡ˆä¾‹ï¼šä» Web åº”ç”¨å®¹å™¨åŒ–åˆ° ML è®­ç»ƒå®¹å™¨åŒ–**

```dockerfile
# ä¼ ç»Ÿ Web åº”ç”¨ Dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
```

```dockerfile
# ML è®­ç»ƒåº”ç”¨ Dockerfile
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04  # ğŸ†• CUDA åŸºç¡€é•œåƒ

WORKDIR /workspace
COPY requirements.txt .
RUN pip install -r requirements.txt

# ğŸ†• å®‰è£…è®­ç»ƒæ¡†æ¶
RUN pip install torch==2.1.0 transformers==4.36.0 accelerate==0.25.0

COPY train.py .
COPY data/ ./data/

# ğŸ†• ç¯å¢ƒå˜é‡ï¼ˆGPU ç›¸å…³ï¼‰
ENV CUDA_VISIBLE_DEVICES=0,1,2,3
ENV NCCL_DEBUG=INFO

# è®­ç»ƒå…¥å£
CMD ["python", "train.py", "--config", "config.yaml"]
```

**Kubernetes éƒ¨ç½²é…ç½®å¯¹æ¯”**

```yaml
# ä¼ ç»Ÿ Web åº”ç”¨ K8s Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1"
            memory: "1Gi"
```

```yaml
# ML è®­ç»ƒä»»åŠ¡ K8s Job
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training-job
spec:
  template:
    spec:
      # ğŸ†• èŠ‚ç‚¹é€‰æ‹©å™¨ï¼ˆGPU èŠ‚ç‚¹ï¼‰
      nodeSelector:
        accelerator: nvidia-a100
      
      containers:
      - name: trainer
        image: my-ml-trainer:latest
        resources:
          requests:
            cpu: "8"
            memory: "64Gi"
            nvidia.com/gpu: "4"  # ğŸ†• GPU èµ„æºè¯·æ±‚
          limits:
            nvidia.com/gpu: "4"
        
        # ğŸ†• æŒ‚è½½æ•°æ®é›†
        volumeMounts:
        - name: dataset
          mountPath: /workspace/data
        - name: model-output
          mountPath: /workspace/models
      
      # ğŸ†• æŒä¹…åŒ–å­˜å‚¨
      volumes:
      - name: dataset
        persistentVolumeClaim:
          claimName: training-data-pvc
      - name: model-output
        persistentVolumeClaim:
          claimName: model-output-pvc
      
      restartPolicy: OnFailure
```

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ80%ï¼‰ï¼š
- Dockerfile ç¼–å†™ï¼ˆå¤šé˜¶æ®µæ„å»ºã€å±‚ç¼“å­˜ä¼˜åŒ–ï¼‰
- K8s èµ„æºç®¡ç†ï¼ˆPodã€Deploymentã€Serviceï¼‰
- å­˜å‚¨æŒ‚è½½ï¼ˆPVCã€ConfigMapï¼‰
- æ—¥å¿—é‡‡é›†ï¼ˆFluentdã€ELKï¼‰

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ20%ï¼‰ï¼š
- GPU èµ„æºè°ƒåº¦ï¼ˆ`nvidia.com/gpu`ï¼‰
- CUDA ç¯å¢ƒé…ç½®
- åˆ†å¸ƒå¼è®­ç»ƒçš„ç½‘ç»œé…ç½®ï¼ˆNCCLï¼‰

**å®ç°éš¾åº¦**ï¼šâ­â­ï¼ˆåªéœ€äº†è§£ GPU ç›¸å…³é…ç½®ï¼‰

---

æ ¹æ® [Google Cloud - Fine-tune Gemma with GPUs on GKE 2025](https://docs.cloud.google.com/kubernetes-engine/docs/tutorials/finetune-gemma-gpu) å’Œ [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/25.10/google-gke.html)ï¼š

> **2025 å¹´ GPU é›†ç¾¤ç®¡ç†å·²ç»æ ‡å‡†åŒ–**ï¼š
> - NVIDIA GPU Operator è‡ªåŠ¨ç®¡ç† GPU é©±åŠ¨
> - K8s åŸç”Ÿæ”¯æŒ GPU è°ƒåº¦
> - **ä½ çš„ K8s ç»éªŒå¯ä»¥æ— ç¼è¿ç§»ï¼**

---

### 1.4 âš¡ æ€§èƒ½ä¼˜åŒ– â­â­â­â­â­

**ä¸ºä»€ä¹ˆæ€§èƒ½ä¼˜åŒ–ç»éªŒæå…¶å®è´µï¼Ÿ**

LLM è®­ç»ƒå’Œæ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€æ˜¯**æˆæœ¬**ï¼š
- GPT-4 è®­ç»ƒæˆæœ¬ä¼°è®¡è¶…è¿‡ 1 äº¿ç¾å…ƒ
- DeepSeek-V3 é€šè¿‡ä¼˜åŒ–é™ä½åˆ° 557 ä¸‡ç¾å…ƒï¼ˆ[å®˜æ–¹æŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-V3)ï¼‰

**æ€§èƒ½ä¼˜åŒ–ç›´æ¥å½±å“æˆæœ¬å’Œç«äº‰åŠ›**ï¼Œè€Œä½ çš„ä¼˜åŒ–ç»éªŒå¯ä»¥ç›´æ¥åº”ç”¨ï¼

---

**ä¼ ç»Ÿæ€§èƒ½ä¼˜åŒ– vs ML æ€§èƒ½ä¼˜åŒ–**

| ä¼˜åŒ–æ–¹å‘ | ä¼ ç»Ÿè½¯ä»¶ | ML è®­ç»ƒ/æ¨ç† | å¤ç”¨åº¦ |
|---------|---------|------------|-------|
| **Profiling** | CPU profiler (gprof, perf) | GPU profiler (Nsight, PyTorch Profiler) | â­â­â­â­â­ |
| **ç“¶é¢ˆåˆ†æ** | æ‰¾å‡ºçƒ­ç‚¹å‡½æ•° | æ‰¾å‡ºè®¡ç®—/é€šä¿¡ç“¶é¢ˆ | â­â­â­â­â­ |
| **å¹¶è¡ŒåŒ–** | å¤šçº¿ç¨‹ã€è¿›ç¨‹æ±  | æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œ | â­â­â­â­ |
| **ç¼“å­˜ä¼˜åŒ–** | CPU Cacheã€Redis | KV Cacheã€æ¢¯åº¦ç´¯ç§¯ | â­â­â­â­ |
| **å†…å­˜ç®¡ç†** | å†…å­˜æ± ã€å¯¹è±¡å¤ç”¨ | æ˜¾å­˜ä¼˜åŒ–ã€æ··åˆç²¾åº¦ | â­â­â­â­ |
| **æ‰¹å¤„ç†** | Batch API è¯·æ±‚ | Batch æ¨ç†ã€Dynamic Batching | â­â­â­â­â­ |
| **å¼‚æ­¥ I/O** | async/await | æ•°æ®é¢„åŠ è½½ã€å¼‚æ­¥æ¢¯åº¦æ›´æ–° | â­â­â­â­ |

---

**å®é™…æ¡ˆä¾‹ï¼šAPI æ€§èƒ½ä¼˜åŒ– vs æ¨ç†æ€§èƒ½ä¼˜åŒ–**

```python
# ä¼ ç»Ÿ API æ€§èƒ½ä¼˜åŒ–æ€è·¯
"""
ä¼˜åŒ–å‰ï¼šå•ä¸ªè¯·æ±‚å»¶è¿Ÿ 500ms
1. Profiling å‘ç°æ•°æ®åº“æŸ¥è¯¢æ…¢ï¼ˆ200msï¼‰
   â†’ æ·»åŠ ç´¢å¼•ã€ä½¿ç”¨è¿æ¥æ± 
2. å‘ç°åºåˆ—åŒ–æ…¢ï¼ˆ100msï¼‰
   â†’ æ¢ç”¨æ›´å¿«çš„åºåˆ—åŒ–åº“ï¼ˆmsgpackï¼‰
3. å‘ç° CPU è®¡ç®—æ…¢ï¼ˆ150msï¼‰
   â†’ ä½¿ç”¨ç¼“å­˜ï¼ˆRedisï¼‰
ä¼˜åŒ–åï¼šå»¶è¿Ÿé™è‡³ 50ms
"""

# ML æ¨ç†æ€§èƒ½ä¼˜åŒ–ï¼ˆæ€è·¯å®Œå…¨ä¸€æ ·ï¼ï¼‰
"""
ä¼˜åŒ–å‰ï¼šå•æ¬¡æ¨ç†å»¶è¿Ÿ 500ms
1. Profiling å‘ç°æ¨¡å‹åŠ è½½æ…¢ï¼ˆ200msï¼‰
   â†’ æ¨¡å‹é¢„åŠ è½½ã€ä½¿ç”¨æ¨¡å‹æ± 
2. å‘ç°æ•°æ®é¢„å¤„ç†æ…¢ï¼ˆ100msï¼‰
   â†’ ä½¿ç”¨æ›´å¿«çš„ tokenizerã€æ‰¹å¤„ç†
3. å‘ç°æ¨¡å‹è®¡ç®—æ…¢ï¼ˆ150msï¼‰
   â†’ é‡åŒ–ï¼ˆINT8ï¼‰ã€KV Cache
ä¼˜åŒ–åï¼šå»¶è¿Ÿé™è‡³ 50ms
"""
```

---

**å®æˆ˜ä»£ç ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€å·§ç›´æ¥è¿ç§»**

```python
# âœ… ä¼ ç»Ÿä¼˜åŒ–æŠ€å·§1ï¼šæ‰¹å¤„ç†
# ä¼ ç»Ÿ APIï¼šæ‰¹é‡å¤„ç†è¯·æ±‚
def process_users_optimized(user_ids: list[int]):
    # âŒ é”™è¯¯ï¼šé€ä¸ªæŸ¥è¯¢
    # for uid in user_ids:
    #     user = db.query(User).filter(User.id == uid).first()
    
    # âœ… æ­£ç¡®ï¼šæ‰¹é‡æŸ¥è¯¢
    users = db.query(User).filter(User.id.in_(user_ids)).all()
    return users

# ML æ¨ç†ï¼šæ‰¹é‡æ¨ç†ï¼ˆå®Œå…¨ç›¸åŒçš„æ€è·¯ï¼ï¼‰
def predict_batch_optimized(texts: list[str]):
    # âŒ é”™è¯¯ï¼šé€æ¡æ¨ç†
    # results = [model(text) for text in texts]
    
    # âœ… æ­£ç¡®ï¼šæ‰¹é‡æ¨ç†ï¼ˆååé‡æå‡ 10xï¼‰
    inputs = tokenizer(texts, padding=True, return_tensors="pt")
    outputs = model(**inputs)
    return outputs
```

```python
# âœ… ä¼ ç»Ÿä¼˜åŒ–æŠ€å·§2ï¼šç¼“å­˜
# ä¼ ç»Ÿ APIï¼šç¼“å­˜çƒ­ç‚¹æ•°æ®
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_user_profile(user_id: int):
    return db.query(User).filter(User.id == user_id).first()

# ML æ¨ç†ï¼šç¼“å­˜ Embeddingï¼ˆå®Œå…¨ç›¸åŒçš„æ€è·¯ï¼ï¼‰
from functools import lru_cache

@lru_cache(maxsize=10000)
def get_embedding(text: str):
    """ç¼“å­˜å¸¸è§æŸ¥è¯¢çš„ Embeddingï¼Œé¿å…é‡å¤è®¡ç®—"""
    return embedding_model.encode(text)
```

```python
# âœ… ä¼ ç»Ÿä¼˜åŒ–æŠ€å·§3ï¼šå¼‚æ­¥ I/O
# ä¼ ç»Ÿ APIï¼šå¼‚æ­¥è°ƒç”¨å¤–éƒ¨æœåŠ¡
import asyncio

async def fetch_user_data(user_id: int):
    async with aiohttp.ClientSession() as session:
        async with session.get(f"/api/users/{user_id}") as resp:
            return await resp.json()

# ML æ¨ç†ï¼šå¼‚æ­¥æ•°æ®é¢„åŠ è½½ï¼ˆå®Œå…¨ç›¸åŒçš„æ€è·¯ï¼ï¼‰
import asyncio

async def preload_data_async(file_paths: list[str]):
    """å¼‚æ­¥åŠ è½½æ•°æ®ï¼Œå……åˆ†åˆ©ç”¨ I/O ç­‰å¾…æ—¶é—´"""
    tasks = [asyncio.to_thread(load_file, path) for path in file_paths]
    return await asyncio.gather(*tasks)
```

---

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„ä¼˜åŒ–æ€ç»´**ï¼ˆ90%ï¼‰ï¼š
- Profiling â†’ æ‰¾ç“¶é¢ˆ
- æ‰¹å¤„ç† â†’ æå‡ååé‡
- ç¼“å­˜ â†’ å‡å°‘é‡å¤è®¡ç®—
- å¼‚æ­¥ â†’ å¹¶å‘å¤„ç†
- èµ„æºæ±  â†’ å‡å°‘åˆå§‹åŒ–å¼€é”€

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ10%ï¼‰ï¼š
- GPU profiling å·¥å…·ï¼ˆNsightã€PyTorch Profilerï¼‰
- æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰
- æ¨¡å‹é‡åŒ–ï¼ˆINT8/INT4ï¼‰

**å®ç°éš¾åº¦**ï¼šâ­â­ï¼ˆå·¥å…·æ¢äº†ï¼Œæ€è·¯ä¸å˜ï¼‰

---

### 1.5 ğŸ“Š ç›‘æ§ä¸å¯è§‚æµ‹æ€§ â­â­â­â­

**ä¸ºä»€ä¹ˆç›‘æ§åœ¨ ML ä¸­æ›´åŠ é‡è¦ï¼Ÿ**

ä¼ ç»Ÿè½¯ä»¶çš„"å¤±è´¥"æ˜¯æ˜ç¡®çš„ï¼ˆ500 é”™è¯¯ã€å´©æºƒï¼‰ï¼Œä½† ML æ¨¡å‹å¯èƒ½**é™é»˜é€€åŒ–**ï¼ˆæ‚„æ‚„å˜å·®ï¼Œæ²¡æœ‰æŠ¥é”™ï¼‰ï¼š
- æ•°æ®åˆ†å¸ƒå˜åŒ–å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ï¼ˆData Driftï¼‰
- æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ç³Ÿç³•ï¼ˆModel Driftï¼‰
- æ¨ç†å»¶è¿Ÿé€æ¸å¢åŠ ï¼ˆå†…å­˜æ³„æ¼ï¼‰

**ä½ çš„ç›‘æ§ç»éªŒå¯ä»¥å¸®åŠ©å›¢é˜ŸåŠæ—©å‘ç°è¿™äº›é—®é¢˜ï¼**

---

**ä¼ ç»Ÿç›‘æ§ vs ML ç›‘æ§**

| ç›‘æ§ç»´åº¦ | ä¼ ç»Ÿè½¯ä»¶ | ML ç³»ç»Ÿ | å¤ç”¨åº¦ |
|---------|---------|---------|-------|
| **åŸºç¡€è®¾æ–½** | CPUã€å†…å­˜ã€ç½‘ç»œ | CPUã€å†…å­˜ã€**GPU åˆ©ç”¨ç‡** | â­â­â­â­â­ |
| **æœåŠ¡æ€§èƒ½** | QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡ | QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡ | â­â­â­â­â­ |
| **ä¸šåŠ¡æŒ‡æ ‡** | è½¬åŒ–ç‡ã€UVã€PV | è½¬åŒ–ç‡ + **æ¨¡å‹å‡†ç¡®ç‡** | â­â­â­â­ |
| **ç‰¹æœ‰æŒ‡æ ‡** | - | **æ•°æ®æ¼‚ç§»ã€æ¨¡å‹æ¼‚ç§»ã€é¢„æµ‹åˆ†å¸ƒ** | ğŸ†• |
| **æ—¥å¿—é‡‡é›†** | ELKã€Loki | ELK + **æ¨¡å‹è¾“å…¥è¾“å‡ºæ—¥å¿—** | â­â­â­â­ |
| **å‘Šè­¦ç­–ç•¥** | é˜ˆå€¼å‘Šè­¦ã€å¼‚å¸¸æ£€æµ‹ | é˜ˆå€¼å‘Šè­¦ + **ç»Ÿè®¡æ£€éªŒ** | â­â­â­â­ |

---

**å®é™…æ¡ˆä¾‹ï¼šç›‘æ§ Dashboard å¯¹æ¯”**

```yaml
# ä¼ ç»Ÿ API ç›‘æ§ (Prometheus + Grafana)
# å…³é”®æŒ‡æ ‡ï¼š
# - è¯·æ±‚å»¶è¿Ÿ (p50, p95, p99)
# - QPS (æ¯ç§’è¯·æ±‚æ•°)
# - é”™è¯¯ç‡ (5xx errors / total requests)
# - æœåŠ¡å™¨èµ„æº (CPU, Memory, Disk)

# ML æ¨ç†æœåŠ¡ç›‘æ§ï¼ˆæ‰©å±•ä¼ ç»Ÿç›‘æ§ï¼‰
# åŸºç¡€æŒ‡æ ‡ï¼ˆå®Œå…¨ç›¸åŒï¼‰ï¼š
# - è¯·æ±‚å»¶è¿Ÿ (p50, p95, p99)  âœ…
# - QPS  âœ…
# - é”™è¯¯ç‡  âœ…
# - æœåŠ¡å™¨èµ„æº (CPU, Memory, GPU)  âœ…

# ğŸ†• ML ç‰¹æœ‰æŒ‡æ ‡ï¼š
# - æ¨¡å‹å‡†ç¡®ç‡ï¼ˆé€šè¿‡äººå·¥æ ‡æ³¨æ ·æœ¬è®¡ç®—ï¼‰
# - é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆä½ç½®ä¿¡åº¦æ¯”ä¾‹å¢åŠ  â†’ å¯èƒ½æ¨¡å‹é€€åŒ–ï¼‰
# - è¾“å…¥æ•°æ®åˆ†å¸ƒï¼ˆä¸è®­ç»ƒæ•°æ®å¯¹æ¯”ï¼Œæ£€æµ‹ driftï¼‰
# - GPU åˆ©ç”¨ç‡ï¼ˆé¿å…èµ„æºæµªè´¹ï¼‰
# - æ¨ç†æ‰¹å¤§å°ï¼ˆä¼˜åŒ–ååé‡ï¼‰
```

**Prometheus ç›‘æ§é…ç½®ç¤ºä¾‹**

```python
# ä¼ ç»Ÿ API ç›‘æ§ä»£ç 
from prometheus_client import Counter, Histogram
import time

request_count = Counter('api_requests_total', 'Total API requests')
request_latency = Histogram('api_request_latency_seconds', 'API request latency')

@app.get("/users/{user_id}")
def get_user(user_id: int):
    request_count.inc()  # è®¡æ•°å™¨+1
    
    start = time.time()
    user = db.query(User).filter(User.id == user_id).first()
    request_latency.observe(time.time() - start)  # è®°å½•å»¶è¿Ÿ
    
    return user
```

```python
# ML æ¨ç†æœåŠ¡ç›‘æ§ï¼ˆæ‰©å±•ä¼ ç»Ÿç›‘æ§ï¼‰
from prometheus_client import Counter, Histogram, Gauge
import time

# âœ… å¤ç”¨ä¼ ç»ŸæŒ‡æ ‡
inference_count = Counter('inference_requests_total', 'Total inference requests')
inference_latency = Histogram('inference_latency_seconds', 'Inference latency')

# ğŸ†• æ–°å¢ ML ç‰¹æœ‰æŒ‡æ ‡
model_accuracy = Gauge('model_accuracy', 'Current model accuracy')
low_confidence_ratio = Gauge('low_confidence_ratio', 'Ratio of low confidence predictions')
gpu_utilization = Gauge('gpu_utilization_percent', 'GPU utilization')

@app.post("/predict")
def predict(text: str):
    inference_count.inc()
    
    start = time.time()
    result = model(text)
    inference_latency.observe(time.time() - start)
    
    # ğŸ†• ç›‘æ§é¢„æµ‹ç½®ä¿¡åº¦
    if result['confidence'] < 0.7:
        low_confidence_ratio.inc()
    
    # ğŸ†• å®šæœŸæ›´æ–°å‡†ç¡®ç‡ï¼ˆé€šè¿‡äººå·¥æ ‡æ³¨æ ·æœ¬ï¼‰
    if random.random() < 0.01:  # 1% é‡‡æ ·
        log_for_human_review(text, result)
    
    return result
```

---

**æ•°æ®æ¼‚ç§»æ£€æµ‹ï¼ˆData Driftï¼‰**

```python
# ğŸ†• ML ç‰¹æœ‰ï¼šæ£€æµ‹è¾“å…¥æ•°æ®åˆ†å¸ƒå˜åŒ–
from scipy.stats import ks_test

def check_data_drift(current_data, reference_data):
    """
    æ£€æµ‹å½“å‰æ•°æ®åˆ†å¸ƒæ˜¯å¦ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒæ˜¾è‘—ä¸åŒ
    ç±»ä¼¼äºä¼ ç»Ÿç›‘æ§ä¸­çš„"å¼‚å¸¸æ£€æµ‹"
    """
    # Kolmogorov-Smirnov æ£€éªŒ
    statistic, p_value = ks_test(current_data, reference_data)
    
    if p_value < 0.05:  # ç»Ÿè®¡æ˜¾è‘—
        alert("Data drift detected! Model may need retraining.")
        return True
    return False

# å®æ—¶ç›‘æ§
def monitor_input_distribution():
    current_batch = get_recent_inputs()  # æœ€è¿‘ 1 å°æ—¶çš„è¾“å…¥
    reference_batch = load_training_data_sample()  # è®­ç»ƒæ•°æ®æ ·æœ¬
    
    if check_data_drift(current_batch, reference_batch):
        # ç±»ä¼¼ä¼ ç»Ÿç›‘æ§ä¸­çš„"å¼‚å¸¸å‘Šè­¦"
        send_alert_to_slack("âš ï¸  Data drift detected in production!")
```

---

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ75%ï¼‰ï¼š
- Prometheus/Grafana ç›‘æ§æ ˆ
- å»¶è¿Ÿã€QPSã€é”™è¯¯ç‡ç›‘æ§
- æ—¥å¿—é‡‡é›†ï¼ˆELKã€Fluentdï¼‰
- å‘Šè­¦ç­–ç•¥ï¼ˆé˜ˆå€¼ã€å¼‚å¸¸æ£€æµ‹ï¼‰

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ25%ï¼‰ï¼š
- GPU ç›‘æ§ï¼ˆnvidia-smiã€dcgm-exporterï¼‰
- æ•°æ®æ¼‚ç§»æ£€æµ‹ï¼ˆç»Ÿè®¡æ£€éªŒï¼‰
- æ¨¡å‹æ€§èƒ½ç›‘æ§ï¼ˆå‡†ç¡®ç‡ã€ç½®ä¿¡åº¦ï¼‰

**å®ç°éš¾åº¦**ï¼šâ­â­â­ï¼ˆéœ€è¦ç†è§£ ML ç‰¹æœ‰æ¦‚å¿µï¼‰

---

### 1.6 ğŸ”§ ç‰ˆæœ¬æ§åˆ¶ (Git â†’ Git + DVC + MLflow) â­â­â­â­

**ä¸ºä»€ä¹ˆç‰ˆæœ¬æ§åˆ¶åœ¨ ML ä¸­æ›´å¤æ‚ï¼Ÿ**

ä¼ ç»Ÿè½¯ä»¶åªéœ€è¦ç‰ˆæœ¬æ§åˆ¶**ä»£ç **ï¼Œä½† ML éœ€è¦ç‰ˆæœ¬æ§åˆ¶ï¼š
- ä»£ç ï¼ˆè®­ç»ƒè„šæœ¬ã€æ¨ç†ä»£ç ï¼‰
- æ•°æ®ï¼ˆè®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®ï¼‰
- æ¨¡å‹ï¼ˆæ¨¡å‹æƒé‡ã€æ¶æ„ï¼‰
- é…ç½®ï¼ˆè¶…å‚æ•°ã€ç¯å¢ƒï¼‰
- å®éªŒç»“æœï¼ˆæŒ‡æ ‡ã€æ—¥å¿—ï¼‰

**ä½ çš„ Git ç»éªŒæ˜¯åŸºç¡€ï¼Œåªéœ€æ‰©å±•åˆ°å¤šç»´ç‰ˆæœ¬ç®¡ç†ï¼**

---

**ç‰ˆæœ¬æ§åˆ¶å¯¹æ¯”**

| å†…å®¹ | ä¼ ç»Ÿè½¯ä»¶ | ML ç³»ç»Ÿ | å·¥å…· |
|------|---------|---------|------|
| **ä»£ç ** | Git | Git | âœ… ç›¸åŒ |
| **é…ç½®** | Git (config.yaml) | Git (config.yaml) | âœ… ç›¸åŒ |
| **æ•°æ®** | - | **DVC** (Data Version Control) | ğŸ†• |
| **æ¨¡å‹** | - | **MLflow / Weights&Biases** | ğŸ†• |
| **å®éªŒ** | - | **MLflow Tracking** | ğŸ†• |

---

**å®é™…æ¡ˆä¾‹ï¼šä» Git åˆ° Git + DVC + MLflow**

```bash
# ä¼ ç»Ÿè½¯ä»¶é¡¹ç›®ç»“æ„
my-app/
â”œâ”€â”€ .git/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/
â”œâ”€â”€ config.yaml
â””â”€â”€ README.md

# Git å·¥ä½œæµ
git add src/main.py
git commit -m "Add new feature"
git push origin main
```

```bash
# ML é¡¹ç›®ç»“æ„ï¼ˆæ‰©å±•ä¼ ç»Ÿç»“æ„ï¼‰
ml-project/
â”œâ”€â”€ .git/              # âœ… ä»£ç ç‰ˆæœ¬æ§åˆ¶
â”œâ”€â”€ .dvc/              # ğŸ†• æ•°æ®ç‰ˆæœ¬æ§åˆ¶
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv.dvc  # ğŸ†• DVC è¿½è¸ªæ•°æ®æ–‡ä»¶
â”‚   â””â”€â”€ test.csv.dvc
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model.pkl.dvc  # ğŸ†• DVC è¿½è¸ªæ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ mlruns/            # ğŸ†• MLflow å®éªŒè¿½è¸ª
â”œâ”€â”€ params.yaml        # è¶…å‚æ•°é…ç½®
â””â”€â”€ dvc.yaml           # ğŸ†• DVC pipeline å®šä¹‰

# ML å·¥ä½œæµï¼ˆæ‰©å±• Git å·¥ä½œæµï¼‰
# 1. è¿½è¸ªæ•°æ®æ–‡ä»¶ï¼ˆç±»ä¼¼ git addï¼‰
dvc add data/train.csv
git add data/train.csv.dvc .gitignore
git commit -m "Add training data v1"

# 2. è¿½è¸ªæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼ˆç±»ä¼¼ git commitï¼‰
mlflow run . --experiment-name "sentiment-classifier"

# 3. æ¨é€ä»£ç  + æ•°æ® + æ¨¡å‹
git push origin main
dvc push  # æ¨é€æ•°æ®/æ¨¡å‹åˆ°è¿œç¨‹å­˜å‚¨ï¼ˆS3/GCSï¼‰
```

---

**DVC ä½¿ç”¨ç¤ºä¾‹ï¼ˆç±»ä¼¼ Git æ“ä½œï¼‰**

æ ¹æ® [DVC + MLflow Best Practices 2025](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)ï¼š

```bash
# ğŸ†• DVC æ“ä½œï¼ˆç±»ä¼¼ Gitï¼‰
# "git add" â†’ "dvc add"
dvc add data/large_dataset.csv
# ç”Ÿæˆ large_dataset.csv.dvc æ–‡ä»¶ï¼ˆç±»ä¼¼ Git pointerï¼‰

# "git commit" â†’ "git commit .dvc files"
git add data/large_dataset.csv.dvc .gitignore
git commit -m "Add dataset v1"

# "git push" â†’ "dvc push"
dvc push  # æ¨é€åˆ°è¿œç¨‹å­˜å‚¨ï¼ˆS3/GCS/Azure Blobï¼‰

# "git pull" â†’ "dvc pull"
dvc pull  # æ‹‰å–æ•°æ®æ–‡ä»¶

# "git checkout" â†’ "dvc checkout"
git checkout experiment-v2
dvc checkout  # åˆ‡æ¢åˆ°å¯¹åº”ç‰ˆæœ¬çš„æ•°æ®/æ¨¡å‹
```

---

**MLflow å®éªŒè¿½è¸ªï¼ˆç±»ä¼¼ Git logï¼‰**

```python
# ğŸ†• MLflow è¿½è¸ªå®éªŒï¼ˆç±»ä¼¼ Git commitï¼‰
import mlflow

# å¼€å§‹å®éªŒï¼ˆç±»ä¼¼ git commitï¼‰
with mlflow.start_run(run_name="experiment-1"):
    # è®°å½•è¶…å‚æ•°ï¼ˆç±»ä¼¼ commit messageï¼‰
    mlflow.log_param("learning_rate", 0.001)
    mlflow.log_param("batch_size", 32)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(lr=0.001, batch_size=32)
    
    # è®°å½•æŒ‡æ ‡ï¼ˆç±»ä¼¼ commit diffï¼‰
    mlflow.log_metric("accuracy", 0.92)
    mlflow.log_metric("f1_score", 0.89)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆç±»ä¼¼ git addï¼‰
    mlflow.sklearn.log_model(model, "model")

# æŸ¥çœ‹å®éªŒå†å²ï¼ˆç±»ä¼¼ git logï¼‰
runs = mlflow.search_runs(experiment_ids=["0"])
print(runs[["start_time", "params.learning_rate", "metrics.accuracy"]])
```

**è¾“å‡ºï¼ˆç±»ä¼¼ git logï¼‰**ï¼š
```
start_time            params.learning_rate  metrics.accuracy
2026-01-14 10:30:00   0.001                 0.92
2026-01-13 15:20:00   0.0001                0.90
2026-01-12 09:10:00   0.01                  0.85
```

---

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ60%ï¼‰ï¼š
- Git åŸºæœ¬æ“ä½œï¼ˆaddã€commitã€pushã€pullã€branchï¼‰
- ä»£ç å®¡æŸ¥ï¼ˆPRã€code reviewï¼‰
- åˆ†æ”¯ç®¡ç†ï¼ˆfeature branchã€release branchï¼‰
- å†²çªè§£å†³ï¼ˆmerge conflictï¼‰

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ40%ï¼‰ï¼š
- DVC æ•°æ®ç‰ˆæœ¬æ§åˆ¶ï¼ˆç±»ä¼¼ Git for dataï¼‰
- MLflow å®éªŒè¿½è¸ªï¼ˆè®°å½•æ¯æ¬¡è®­ç»ƒçš„å‚æ•°å’Œç»“æœï¼‰
- å¤šç»´ç‰ˆæœ¬å…³è”ï¼ˆä»£ç ç‰ˆæœ¬ + æ•°æ®ç‰ˆæœ¬ + æ¨¡å‹ç‰ˆæœ¬ï¼‰

**å®ç°éš¾åº¦**ï¼šâ­â­â­ï¼ˆæ¦‚å¿µæ‰©å±•ï¼Œä½†æ“ä½œç±»ä¼¼ Gitï¼‰

---

### 1.7 ğŸŒ API è®¾è®¡ä¸æœåŠ¡åŒ– â­â­â­â­

**ä¸ºä»€ä¹ˆ API è®¾è®¡ç»éªŒå¯ä»¥ç›´æ¥å¤ç”¨ï¼Ÿ**

ML æ¨¡å‹æœ€ç»ˆéœ€è¦ä»¥**æœåŠ¡çš„å½¢å¼**å¯¹å¤–æä¾›ï¼Œè¿™å’Œä¼ ç»Ÿåç«¯ API æ²¡æœ‰æœ¬è´¨åŒºåˆ«ï¼š
- RESTful API è®¾è®¡åŸåˆ™
- æ¥å£ç‰ˆæœ¬ç®¡ç†
- é”™è¯¯å¤„ç†
- é€Ÿç‡é™åˆ¶
- æ–‡æ¡£ç”Ÿæˆ

**ä½ çš„ API è®¾è®¡ç»éªŒå¯ä»¥ç›´æ¥åº”ç”¨ï¼**

---

**ä¼ ç»Ÿ API vs ML æ¨ç† API**

```python
# ä¼ ç»Ÿ RESTful API
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class UserCreate(BaseModel):
    name: str
    email: str

@app.post("/users", response_model=User)
def create_user(user: UserCreate):
    if not validate_email(user.email):
        raise HTTPException(status_code=400, detail="Invalid email")
    
    new_user = db.create_user(user)
    return new_user
```

```python
# ML æ¨ç† APIï¼ˆè®¾è®¡æ€è·¯å®Œå…¨ç›¸åŒï¼ï¼‰
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    text: str
    max_length: int = 100

class PredictionResponse(BaseModel):
    label: str
    confidence: float
    latency_ms: float

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    # âœ… è¾“å…¥éªŒè¯ï¼ˆå’Œä¼ ç»Ÿ API ä¸€æ ·ï¼‰
    if len(request.text) > 1000:
        raise HTTPException(status_code=400, detail="Text too long")
    
    # ğŸ†• æ¨¡å‹æ¨ç†ï¼ˆæ›¿ä»£æ•°æ®åº“æŸ¥è¯¢ï¼‰
    start = time.time()
    result = model.predict(request.text)
    latency = (time.time() - start) * 1000
    
    return PredictionResponse(
        label=result['label'],
        confidence=result['confidence'],
        latency_ms=latency
    )
```

---

**API æœ€ä½³å®è·µå¯¹æ¯”**

| æœ€ä½³å®è·µ | ä¼ ç»Ÿ API | ML æ¨ç† API | å¤ç”¨åº¦ |
|---------|---------|------------|-------|
| **ç‰ˆæœ¬ç®¡ç†** | `/v1/users`, `/v2/users` | `/v1/predict`, `/v2/predict` | â­â­â­â­â­ |
| **é”™è¯¯å¤„ç†** | 400/404/500 çŠ¶æ€ç  | 400/404/500 + **æ¨¡å‹é”™è¯¯** | â­â­â­â­â­ |
| **é€Ÿç‡é™åˆ¶** | Rate limiting (Redis) | Rate limiting + **GPU é˜Ÿåˆ—ç®¡ç†** | â­â­â­â­ |
| **æ‰¹å¤„ç†** | Batch insert API | **Batch inference API** | â­â­â­â­â­ |
| **å¼‚æ­¥å¤„ç†** | Celery ä»»åŠ¡é˜Ÿåˆ— | å¼‚æ­¥æ¨ç†é˜Ÿåˆ— | â­â­â­â­â­ |
| **ç¼“å­˜ç­–ç•¥** | Redis ç¼“å­˜ | æ¨ç†ç»“æœç¼“å­˜ | â­â­â­â­â­ |
| **æ–‡æ¡£ç”Ÿæˆ** | Swagger/OpenAPI | Swagger/OpenAPI | â­â­â­â­â­ |

---

**æ‰¹å¤„ç† API ç¤ºä¾‹**

```python
# ä¼ ç»Ÿæ‰¹å¤„ç† API
@app.post("/users/batch")
def create_users_batch(users: list[UserCreate]):
    """æ‰¹é‡åˆ›å»ºç”¨æˆ·ï¼ˆé¿å… N æ¬¡æ•°æ®åº“è¿æ¥ï¼‰"""
    return db.bulk_create(users)

# ML æ‰¹å¤„ç† APIï¼ˆæ€è·¯å®Œå…¨ç›¸åŒï¼ï¼‰
@app.post("/predict/batch")
def predict_batch(requests: list[PredictionRequest]):
    """æ‰¹é‡æ¨ç†ï¼ˆGPU æ‰¹å¤„ç†ï¼Œååé‡æå‡ 10xï¼‰"""
    texts = [req.text for req in requests]
    results = model.predict_batch(texts)  # ğŸ†• æ‰¹é‡æ¨ç†
    return results
```

---

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ85%ï¼‰ï¼š
- RESTful API è®¾è®¡åŸåˆ™
- è¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆPydanticï¼‰
- é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç 
- API æ–‡æ¡£ï¼ˆSwaggerï¼‰
- é€Ÿç‡é™åˆ¶ã€ç¼“å­˜ã€æ‰¹å¤„ç†

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ15%ï¼‰ï¼š
- æ¨¡å‹åŠ è½½å’Œé¢„çƒ­
- GPU èµ„æºç®¡ç†
- æµå¼å“åº”ï¼ˆLLM ç”Ÿæˆåœºæ™¯ï¼‰

**å®ç°éš¾åº¦**ï¼šâ­ï¼ˆå‡ ä¹æ— å­¦ä¹ æˆæœ¬ï¼‰

---

### 1.8 ğŸ›¡ï¸ ç³»ç»Ÿç¨³å®šæ€§å·¥ç¨‹ â­â­â­â­

**ä¸ºä»€ä¹ˆç¨³å®šæ€§ç»éªŒåœ¨ ML ä¸­æ›´é‡è¦ï¼Ÿ**

ML è®­ç»ƒä»»åŠ¡é€šå¸¸éœ€è¦**å¾ˆé•¿æ—¶é—´**æ‰èƒ½å®Œæˆï¼š
- GPT-3 è®­ç»ƒäº† 34 å¤©
- å¦‚æœè®­ç»ƒåˆ°ç¬¬ 30 å¤©å´©æºƒï¼Œæ²¡æœ‰ Checkpoint æœºåˆ¶ â†’ å…¨éƒ¨é‡æ¥

**ä½ çš„å®¹é”™è®¾è®¡ã€Checkpointã€é‡è¯•æœºåˆ¶ç»éªŒå¯ä»¥ç›´æ¥æ‹¯æ•‘å›¢é˜Ÿï¼**

---

**ç¨³å®šæ€§è®¾è®¡å¯¹æ¯”**

| æŠ€æœ¯ | ä¼ ç»Ÿç³»ç»Ÿ | ML è®­ç»ƒç³»ç»Ÿ | å¤ç”¨åº¦ |
|------|---------|------------|-------|
| **Checkpoint** | æ•°æ®åº“å¿«ç…§ã€Redis æŒä¹…åŒ– | æ¨¡å‹æƒé‡å®šæœŸä¿å­˜ | â­â­â­â­â­ |
| **è‡ªåŠ¨é‡è¯•** | è¯·æ±‚å¤±è´¥é‡è¯•ã€ä»»åŠ¡é˜Ÿåˆ—é‡è¯• | è®­ç»ƒä»»åŠ¡å¤±è´¥è‡ªåŠ¨é‡å¯ | â­â­â­â­â­ |
| **å¥åº·æ£€æŸ¥** | HTTP health check | GPU å¥åº·æ£€æŸ¥ã€è®­ç»ƒæŒ‡æ ‡ç›‘æ§ | â­â­â­â­â­ |
| **ä¼˜é›…é™çº§** | æœåŠ¡é™çº§ã€ç†”æ–­ | å›é€€åˆ°å°æ¨¡å‹ã€ç¼“å­˜ç»“æœ | â­â­â­â­ |
| **æ•…éšœæ¢å¤** | ä¸»ä»åˆ‡æ¢ã€è‡ªåŠ¨é‡å¯ | ä» Checkpoint æ¢å¤è®­ç»ƒ | â­â­â­â­â­ |

---

**Checkpoint æœºåˆ¶å¯¹æ¯”**

```python
# ä¼ ç»Ÿç³»ç»Ÿï¼šæ•°æ®åº“äº‹åŠ¡ + å¿«ç…§
def process_orders_with_checkpoint():
    """å¤„ç†å¤§æ‰¹é‡è®¢å•ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
    checkpoint = load_checkpoint()  # åŠ è½½ä¸Šæ¬¡å¤„ç†åˆ°çš„ä½ç½®
    
    for order_id in range(checkpoint['last_processed_id'], total_orders):
        try:
            process_order(order_id)
            
            # æ¯ 1000 æ¡ä¿å­˜ä¸€æ¬¡ checkpoint
            if order_id % 1000 == 0:
                save_checkpoint({'last_processed_id': order_id})
        except Exception as e:
            log_error(e)
            # ä¸‹æ¬¡é‡å¯ä» checkpoint ç»§ç»­
            break
```

```python
# ML è®­ç»ƒï¼šæ¨¡å‹ Checkpointï¼ˆæ€è·¯å®Œå…¨ç›¸åŒï¼ï¼‰
def train_with_checkpoint(model, train_loader, epochs=100):
    """è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒä» checkpoint æ¢å¤"""
    checkpoint = load_checkpoint()  # åŠ è½½ä¸Šæ¬¡è®­ç»ƒçš„ epoch
    start_epoch = checkpoint.get('epoch', 0)
    
    for epoch in range(start_epoch, epochs):
        for batch in train_loader:
            loss = train_step(model, batch)
        
        # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpointï¼ˆç±»ä¼¼æ•°æ®åº“å¿«ç…§ï¼‰
        save_checkpoint({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss
        })
    
    # å¦‚æœè®­ç»ƒå´©æºƒï¼Œä¸‹æ¬¡é‡å¯ä¼šä»æœ€åä¸€ä¸ª checkpoint ç»§ç»­
```

---

**è‡ªåŠ¨é‡è¯•æœºåˆ¶**

```python
# ä¼ ç»Ÿç³»ç»Ÿï¼šAPI è¯·æ±‚é‡è¯•
import requests
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def call_external_api(url):
    """å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡"""
    response = requests.get(url)
    response.raise_for_status()
    return response.json()
```

```python
# ML è®­ç»ƒï¼šè®­ç»ƒä»»åŠ¡è‡ªåŠ¨é‡è¯•ï¼ˆæ€è·¯å®Œå…¨ç›¸åŒï¼ï¼‰
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=60, max=600))
def train_model_with_retry():
    """è®­ç»ƒå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼ˆGPU OOMã€ç½‘ç»œä¸­æ–­ç­‰ï¼‰"""
    try:
        checkpoint = load_checkpoint()
        model = load_model_from_checkpoint(checkpoint)
        train(model)
    except torch.cuda.OutOfMemoryError:
        # æ¸…ç†æ˜¾å­˜åé‡è¯•
        torch.cuda.empty_cache()
        raise
    except Exception as e:
        log_error(e)
        raise
```

---

**Kubernetes Job å®¹é”™é…ç½®**

```yaml
# ML è®­ç»ƒ Job é…ç½®ï¼ˆåˆ©ç”¨ K8s å®¹é”™æœºåˆ¶ï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-training
spec:
  # âœ… è‡ªåŠ¨é‡è¯•ï¼ˆç±»ä¼¼ tenacityï¼‰
  backoffLimit: 3
  
  template:
    spec:
      # âœ… é‡å¯ç­–ç•¥
      restartPolicy: OnFailure
      
      containers:
      - name: trainer
        image: my-trainer:latest
        
        # âœ… å¥åº·æ£€æŸ¥ï¼ˆç±»ä¼¼ HTTP health checkï¼‰
        livenessProbe:
          exec:
            command:
            - python
            - check_training_progress.py
          initialDelaySeconds: 300
          periodSeconds: 60
        
        # âœ… èµ„æºé™åˆ¶ï¼ˆé¿å… OOMï¼‰
        resources:
          limits:
            memory: "64Gi"
            nvidia.com/gpu: "8"
```

---

**å…³é”®æ´å¯Ÿ**ï¼š

âœ… **ä½ å·²ç»ç†Ÿæ‚‰çš„éƒ¨åˆ†**ï¼ˆ90%ï¼‰ï¼š
- Checkpoint æœºåˆ¶ï¼ˆå¿«ç…§ã€æ–­ç‚¹ç»­ä¼ ï¼‰
- è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ã€æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰
- å¥åº·æ£€æŸ¥ï¼ˆLiveness/Readiness Probeï¼‰
- ä¼˜é›…é™çº§ï¼ˆç†”æ–­ã€é™æµï¼‰
- æ—¥å¿—å’Œå‘Šè­¦

ğŸ†• **éœ€è¦æ–°å¢çš„éƒ¨åˆ†**ï¼ˆ10%ï¼‰ï¼š
- æ¨¡å‹æƒé‡ä¿å­˜æ ¼å¼ï¼ˆPyTorch `.pt`, TensorFlow `.ckpt`ï¼‰
- GPU OOM å¤„ç†

**å®ç°éš¾åº¦**ï¼šâ­ï¼ˆæ¦‚å¿µå®Œå…¨ç›¸åŒï¼‰

---

## ğŸ“Š å°ç»“ï¼šå¯ç›´æ¥è¿ç§»èƒ½åŠ›æ€»è§ˆ

| èƒ½åŠ› | è¿ç§»ä»·å€¼ | å¤ç”¨åº¦ | å­¦ä¹ æˆæœ¬ | å…³é”®å·®å¼‚ |
|------|---------|-------|---------|---------|
| **ç³»ç»Ÿæ¶æ„è®¾è®¡** | â­â­â­â­â­ | 90% | â­ | éœ€è¦äº†è§£ GPU é›†ç¾¤ |
| **CI/CD** | â­â­â­â­â­ | 70% | â­â­ | æ–°å¢æ•°æ®/æ¨¡å‹éªŒè¯ |
| **å®¹å™¨åŒ– (Docker/K8s)** | â­â­â­â­â­ | 80% | â­â­ | éœ€è¦é…ç½® GPU |
| **æ€§èƒ½ä¼˜åŒ–** | â­â­â­â­â­ | 90% | â­â­ | å·¥å…·æ¢äº†ï¼Œæ€è·¯ä¸å˜ |
| **ç›‘æ§ä¸å¯è§‚æµ‹æ€§** | â­â­â­â­ | 75% | â­â­â­ | æ–°å¢æ¨¡å‹/æ•°æ®ç›‘æ§ |
| **ç‰ˆæœ¬æ§åˆ¶** | â­â­â­â­ | 60% | â­â­â­ | æ‰©å±•åˆ°æ•°æ®/æ¨¡å‹ |
| **API è®¾è®¡** | â­â­â­â­ | 85% | â­ | å‡ ä¹æ— å·®å¼‚ |
| **ç³»ç»Ÿç¨³å®šæ€§** | â­â­â­â­ | 90% | â­ | Checkpoint æ¦‚å¿µç›¸åŒ |

---

**ğŸ”¥ æ ¸å¿ƒç»“è®º**ï¼š

> **ä½ å·²ç»æ‹¥æœ‰ 70-80% çš„èƒ½åŠ›ï¼Œå‰©ä¸‹ 20-30% åªæ˜¯å·¥å…·å’Œé¢†åŸŸçŸ¥è¯†çš„æ‰©å±•ï¼**
>
> ä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹å¸ˆåœ¨**ç³»ç»Ÿå·¥ç¨‹ã€å·¥ç¨‹åŒ–èƒ½åŠ›**æ–¹é¢çš„ä¼˜åŠ¿ï¼Œæ­£æ˜¯ ML å›¢é˜Ÿæœ€ç¨€ç¼ºçš„èƒ½åŠ›ã€‚

---

## äºŒã€éœ€è¦"å‡çº§"çš„æŠ€èƒ½ (ä» 1.0 åˆ° 2.0)

è™½ç„¶å¤§éƒ¨åˆ†æŠ€èƒ½å¯ä»¥ç›´æ¥å¤ç”¨ï¼Œä½†æœ‰äº›æŠ€èƒ½éœ€è¦**å‡çº§åˆ° ML åœºæ™¯**ã€‚

### 2.1 Docker â†’ GPU é›†ç¾¤ç®¡ç† ğŸš€

**å‡çº§æ–¹å‘**ï¼šå•æœºå®¹å™¨ â†’ åˆ†å¸ƒå¼ GPU é›†ç¾¤

| èƒ½åŠ›å±‚çº§ | ä¼ ç»ŸæŠ€èƒ½ | å‡çº§åæŠ€èƒ½ |
|---------|---------|----------|
| **å…¥é—¨** | Docker åŸºç¡€ | Docker + CUDA é•œåƒ |
| **è¿›é˜¶** | Docker Compose | Kubernetes + GPU Operator |
| **é«˜çº§** | K8s å•é›†ç¾¤ | å¤š GPU èŠ‚ç‚¹è°ƒåº¦ã€æ•…éšœæ¢å¤ |
| **ä¸“å®¶** | - | åˆ†å¸ƒå¼è®­ç»ƒç¼–æ’ï¼ˆKubeflowã€Rayï¼‰ |

**å‡çº§å­¦ä¹ è·¯å¾„**ï¼š

```bash
# é˜¶æ®µ1: CUDA Docker é•œåƒ
# å­¦ä¹ ï¼šNVIDIA å®˜æ–¹é•œåƒã€CUDA ç‰ˆæœ¬åŒ¹é…
docker run --gpus all nvidia/cuda:12.1.0-base nvidia-smi

# é˜¶æ®µ2: K8s GPU è°ƒåº¦
# å­¦ä¹ ï¼šGPU Operatorã€èŠ‚ç‚¹æ ‡ç­¾ã€èµ„æºé™åˆ¶
kubectl label nodes node-1 accelerator=nvidia-a100
kubectl apply -f gpu-job.yaml

# é˜¶æ®µ3: åˆ†å¸ƒå¼è®­ç»ƒ
# å­¦ä¹ ï¼šKubeflow Pipelinesã€Horovodã€DeepSpeed
kubeflow pipelines create --pipeline train-llm.yaml
```

---

### 2.2 å•æœºæ€§èƒ½ä¼˜åŒ– â†’ åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ– ğŸš€

**å‡çº§æ–¹å‘**ï¼šå•è¿›ç¨‹ä¼˜åŒ– â†’ å¤š GPU/å¤šèŠ‚ç‚¹ä¼˜åŒ–

| ä¼˜åŒ–ç»´åº¦ | ä¼ ç»Ÿä¼˜åŒ– | ML åˆ†å¸ƒå¼ä¼˜åŒ– | å­¦ä¹ éš¾åº¦ |
|---------|---------|-------------|---------|
| **å¹¶è¡Œç­–ç•¥** | å¤šçº¿ç¨‹ã€è¿›ç¨‹æ±  | æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œ | â­â­â­ |
| **é€šä¿¡ä¼˜åŒ–** | è¿›ç¨‹é—´é€šä¿¡ (IPC) | NCCLã€Ring All-Reduce | â­â­â­ |
| **å†…å­˜ä¼˜åŒ–** | å†…å­˜æ± ã€å…±äº«å†…å­˜ | ZeROã€æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦ | â­â­â­ |
| **ç½‘ç»œä¼˜åŒ–** | TCP è°ƒä¼˜ | InfiniBandã€RDMA | â­â­ï¼ˆäº†è§£å³å¯ï¼‰ |

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```python
# ä¼ ç»Ÿå¹¶è¡Œï¼šå¤šè¿›ç¨‹
from multiprocessing import Pool

def process_data(data_chunk):
    return expensive_computation(data_chunk)

with Pool(processes=8) as pool:
    results = pool.map(process_data, data_chunks)
```

```python
# ML åˆ†å¸ƒå¼è®­ç»ƒï¼šæ•°æ®å¹¶è¡Œï¼ˆæ€è·¯ç±»ä¼¼å¤šè¿›ç¨‹ï¼‰
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆç±»ä¼¼ Pool åˆå§‹åŒ–ï¼‰
dist.init_process_group(backend="nccl", world_size=8, rank=rank)

# æ¨¡å‹åŒ…è£…ï¼ˆç±»ä¼¼ pool.mapï¼‰
model = DDP(model, device_ids=[local_rank])

# æ¯ä¸ª GPU å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®ï¼ˆç±»ä¼¼ data_chunksï¼‰
for batch in data_loader:
    loss = model(batch)
    loss.backward()
    optimizer.step()
```

---

### 2.3 Git â†’ Git + DVC + MLflow ğŸš€

**å‡çº§æ–¹å‘**ï¼šå•ä¸€ç»´åº¦ç‰ˆæœ¬æ§åˆ¶ â†’ å¤šç»´åº¦ç‰ˆæœ¬æ§åˆ¶

| ç‰ˆæœ¬æ§åˆ¶å¯¹è±¡ | ä¼ ç»Ÿå·¥å…· | ML å·¥å…· | å­¦ä¹ éš¾åº¦ |
|------------|---------|--------|---------|
| **ä»£ç ** | Git | Git | âœ…ï¼ˆå·²æŒæ¡ï¼‰ |
| **æ•°æ®** | - | DVC | â­â­ |
| **æ¨¡å‹** | - | MLflow / W&B | â­â­ |
| **å®éªŒ** | - | MLflow Tracking | â­â­ |
| **Pipeline** | - | DVC Pipelines | â­â­â­ |

**å­¦ä¹ è·¯çº¿**ï¼š

```bash
# é˜¶æ®µ1: DVC æ•°æ®ç‰ˆæœ¬æ§åˆ¶
dvc init
dvc add data/train.csv
git add data/train.csv.dvc
git commit -m "Track training data"

# é˜¶æ®µ2: MLflow å®éªŒè¿½è¸ª
mlflow run . --experiment-name "sentiment-model"
mlflow ui  # æŸ¥çœ‹å®éªŒå†å²

# é˜¶æ®µ3: DVC Pipelinesï¼ˆè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹ï¼‰
dvc stage add -n preprocess python preprocess.py
dvc stage add -n train python train.py
dvc repro  # è‡ªåŠ¨æ‰§è¡Œ pipeline
```

---

### 2.4 SQL æ•°æ®åº“ â†’ å‘é‡æ•°æ®åº“ ğŸš€

**å‡çº§æ–¹å‘**ï¼šå…³ç³»å‹æŸ¥è¯¢ â†’ è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢

| èƒ½åŠ› | ä¼ ç»Ÿæ•°æ®åº“ | å‘é‡æ•°æ®åº“ | å­¦ä¹ éš¾åº¦ |
|------|-----------|----------|---------|
| **æŸ¥è¯¢æ–¹å¼** | WHERE name = 'John' | æ‰¾åˆ°è¯­ä¹‰æœ€ç›¸ä¼¼çš„æ–‡æ¡£ | â­â­ |
| **ç´¢å¼•** | B-Treeã€Hash Index | HNSWã€IVF | â­â­ |
| **å·¥å…·** | MySQLã€PostgreSQL | Weaviateã€Milvusã€Chroma | â­â­ |

**æ ¸å¿ƒæ¦‚å¿µå¯¹æ¯”**ï¼š

```sql
-- ä¼ ç»Ÿ SQLï¼šç²¾ç¡®åŒ¹é…
SELECT * FROM users WHERE name = 'John' AND age > 25;
```

```python
# å‘é‡æ•°æ®åº“ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
from chromadb import Client

client = Client()
collection = client.create_collection("documents")

# æŸ¥è¯¢ï¼š"æ‰¾åˆ°ä¸è¿™æ®µè¯è¯­ä¹‰æœ€ç›¸ä¼¼çš„ 5 ä¸ªæ–‡æ¡£"
results = collection.query(
    query_texts=["How to reset password?"],
    n_results=5
)
# è¿”å›ï¼š["How to change password", "Forgot password guide", ...]
```

---

## ä¸‰ã€å®æˆ˜æ¡ˆä¾‹ï¼šä»ä¼ ç»Ÿ CI/CD åˆ° MLOps

### æ¡ˆä¾‹èƒŒæ™¯

æŸç”µå•†å…¬å¸éœ€è¦æ„å»ºä¸€ä¸ª**å•†å“è¯„è®ºæƒ…æ„Ÿåˆ†ç±»ç³»ç»Ÿ**ï¼Œä½ ä½œä¸ºä¼ ç»Ÿåç«¯å·¥ç¨‹å¸ˆï¼Œå¦‚ä½•åº”ç”¨ç°æœ‰æŠ€èƒ½å¿«é€Ÿä¸Šæ‰‹ï¼Ÿ

---

### é˜¶æ®µ 1ï¼šç”¨ä¼ ç»ŸæŠ€èƒ½å¿«é€Ÿæ­å»ºåŸå‹

**âœ… å¤ç”¨ç°æœ‰æŠ€èƒ½ï¼š**

```python
# 1. API è®¾è®¡ï¼ˆå®Œå…¨å¤ç”¨åç«¯ç»éªŒï¼‰
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class ReviewRequest(BaseModel):
    text: str

@app.post("/classify")
def classify_review(review: ReviewRequest):
    # å…ˆç”¨è§„åˆ™å®ç° MVPï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰
    positive_keywords = ["å¥½", "æ£’", "æ»¡æ„", "æ¨è"]
    if any(kw in review.text for kw in positive_keywords):
        return {"sentiment": "positive", "confidence": 0.8}
    return {"sentiment": "negative", "confidence": 0.6}
```

```dockerfile
# 2. å®¹å™¨åŒ–ï¼ˆå®Œå…¨å¤ç”¨ Docker ç»éªŒï¼‰
FROM python:3.10-slim
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY app.py .
CMD ["uvicorn", "app:app", "--host", "0.0.0.0"]
```

```yaml
# 3. K8s éƒ¨ç½²ï¼ˆå®Œå…¨å¤ç”¨ K8s ç»éªŒï¼‰
apiVersion: apps/v1
kind: Deployment
metadata:
  name: review-classifier
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: review-classifier:v1
```

---

### é˜¶æ®µ 2ï¼šå‡çº§åˆ° ML æ¨¡å‹

**ğŸ†• æ–°å¢ ML éƒ¨åˆ†ï¼ˆä½¿ç”¨ Hugging Faceï¼Œæ— éœ€ä»é›¶è®­ç»ƒï¼‰**ï¼š

```python
# å‡çº§ï¼šæ›¿æ¢è§„åˆ™ä¸ºé¢„è®­ç»ƒæ¨¡å‹
from transformers import pipeline

# ä½¿ç”¨ç°æˆçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ï¼ˆæ— éœ€è‡ªå·±è®­ç»ƒï¼‰
classifier = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-dianping-chinese")

@app.post("/classify")
def classify_review(review: ReviewRequest):
    result = classifier(review.text)[0]
    return {
        "sentiment": result['label'],
        "confidence": result['score']
    }
```

**âœ… å¤ç”¨ç›‘æ§ç»éªŒ**ï¼š

```python
# æ·»åŠ  Prometheus ç›‘æ§ï¼ˆå®Œå…¨å¤ç”¨ä¼ ç»Ÿç›‘æ§ï¼‰
from prometheus_client import Counter, Histogram

requests_total = Counter('classify_requests_total', 'Total requests')
latency = Histogram('classify_latency_seconds', 'Classification latency')

@app.post("/classify")
def classify_review(review: ReviewRequest):
    requests_total.inc()
    
    with latency.time():
        result = classifier(review.text)[0]
    
    return result
```

---

### é˜¶æ®µ 3ï¼šæ„å»º MLOps æµç¨‹

**ğŸ†• æ·»åŠ  ML ç‰¹æœ‰æµç¨‹ï¼ˆåŸºäºä¼ ç»Ÿ CI/CD æ‰©å±•ï¼‰**ï¼š

```yaml
# .github/workflows/ml-pipeline.yml
name: ML CI/CD Pipeline

on:
  push:
    branches: [main]
  schedule:  # ğŸ†• å®šæœŸé‡æ–°è®­ç»ƒ
    - cron: '0 0 * * 0'  # å®šæœŸé‡æ–°è®­ç»ƒ

jobs:
  validate-data:  # ğŸ†• æ•°æ®éªŒè¯
    runs-on: ubuntu-latest
    steps:
      - name: Check data quality
        run: |
          python scripts/validate_data.py
          python scripts/check_data_drift.py
  
  test-model:
    needs: validate-data
    steps:
      - name: Run unit tests  # âœ… ä¼ ç»Ÿæµ‹è¯•
        run: pytest tests/
      
      - name: Fine-tune model  # ğŸ†• æ¨¡å‹è®­ç»ƒ
        run: python train.py
      
      - name: Evaluate model  # ğŸ†• æ¨¡å‹è¯„ä¼°
        run: |
          python evaluate.py
          if [ $(cat metrics.json | jq '.accuracy') < 0.85 ]; then
            echo "Model accuracy too low"
            exit 1
          fi
      
      - name: Test inference latency  # ğŸ†• æ€§èƒ½æµ‹è¯•
        run: python test_latency.py --max-p95=100ms
  
  deploy:
    needs: test-model
    steps:
      - name: Register model  # ğŸ†• æ¨¡å‹æ³¨å†Œ
        run: mlflow models register-model --name sentiment-classifier
      
      - name: Deploy Canary (10%)  # âœ… ä¼ ç»Ÿéƒ¨ç½²ç­–ç•¥
        run: kubectl apply -f k8s/canary-10.yaml
      
      - name: Monitor for 1 hour  # âœ… ä¼ ç»Ÿç›‘æ§
        run: python monitor.py --duration=1h
      
      - name: Rollout or rollback  # âœ… ä¼ ç»Ÿå†³ç­–é€»è¾‘
        run: python decide_rollout.py
```

---

**DVC æ•°æ®ç‰ˆæœ¬æ§åˆ¶**ï¼š

```yaml
# dvc.yamlï¼ˆå®šä¹‰ ML Pipelineï¼‰
stages:
  preprocess:
    cmd: python preprocess.py
    deps:
      - data/raw/reviews.csv
    outs:
      - data/processed/train.csv
      - data/processed/test.csv
  
  train:
    cmd: python train.py
    deps:
      - data/processed/train.csv
      - src/train.py
    params:
      - train.learning_rate
      - train.batch_size
    outs:
      - models/sentiment_model.pkl
    metrics:
      - metrics.json:
          cache: false
```

```bash
# è¿è¡Œå®Œæ•´ pipeline
dvc repro

# æŸ¥çœ‹æŒ‡æ ‡å˜åŒ–
dvc metrics diff
```

---

### ğŸ¯ æ¡ˆä¾‹æ€»ç»“

| é˜¶æ®µ | ä¸»è¦ä½¿ç”¨çš„æŠ€èƒ½ | å¤ç”¨ vs æ–°å­¦ | éš¾åº¦ |
|------|--------------|-------------|------|
| **é˜¶æ®µ 1ï¼šåŸå‹** | FastAPIã€Dockerã€K8s | âœ… 100% å¤ç”¨ | â­ |
| **é˜¶æ®µ 2ï¼šML æ¨¡å‹** | ç›‘æ§ã€API è®¾è®¡ + Hugging Face | âœ… 80% å¤ç”¨ + ğŸ†• 20% æ–°å­¦ | â­â­ |
| **é˜¶æ®µ 3ï¼šMLOps** | CI/CDã€ç›‘æ§ + DVCã€MLflow | âœ… 60% å¤ç”¨ + ğŸ†• 40% æ–°å­¦ | â­â­â­ |

**æ¸è¿›å¼è¿ç§»ï¼Œé€æ­¥æŒæ¡å®Œæ•´çš„ç”Ÿäº§çº§ ML ç³»ç»Ÿï¼**

---

## å››ã€èƒ½åŠ›è¿ç§»è·¯çº¿å›¾

### ğŸ“… é˜¶æ®µ 1: å·©å›ºä¼˜åŠ¿ï¼Œå¿«é€Ÿå‡ºæˆæœ

**ç›®æ ‡**ï¼šç”¨ç°æœ‰æŠ€èƒ½å¿«é€Ÿè¯æ˜ä»·å€¼

| å­¦ä¹ å†…å®¹ | å®è·µé¡¹ç›® | å¤ç”¨æŠ€èƒ½ |
|---------|---------|---------|
| Hugging Face åŸºç¡€ | è·‘é€šä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¨ç† | API è®¾è®¡ |
| FastAPI + æ¨¡å‹æœåŠ¡åŒ– | æ„å»ºæ¨ç† API | Dockerã€K8s |
| Prometheus ç›‘æ§ ML æŒ‡æ ‡ | æ·»åŠ æ¨¡å‹ç›‘æ§ | ç›‘æ§ã€å‘Šè­¦ |
| å‹æµ‹å’Œæ€§èƒ½ä¼˜åŒ– | ä¼˜åŒ–æ¨ç†å»¶è¿Ÿ | æ€§èƒ½ä¼˜åŒ– |

**é‡Œç¨‹ç¢‘**ï¼šå®Œæˆä¸€ä¸ªå¯éƒ¨ç½²çš„ ML æ¨ç†æœåŠ¡ âœ…

---

### ğŸ“… é˜¶æ®µ 2: è¡¥å…… ML åŸºç¡€

**ç›®æ ‡**ï¼šç†è§£ ML æ ¸å¿ƒæ¦‚å¿µï¼ˆä¸æ·±å…¥æ•°å­¦ï¼‰

| å­¦ä¹ å†…å®¹ | å®è·µé¡¹ç›® | æ–°å¢æŠ€èƒ½ |
|---------|---------|---------|
| æ¨¡å‹å¾®è°ƒåŸºç¡€ (LoRA) | å¾®è°ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ | Hugging Face Trainer |
| DVC æ•°æ®ç‰ˆæœ¬æ§åˆ¶ | è¿½è¸ªæ•°æ®é›†ç‰ˆæœ¬ | DVC åŸºç¡€ |
| MLflow å®éªŒè¿½è¸ª | è®°å½•è®­ç»ƒå®éªŒ | MLflow Tracking |
| æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ | ç†è§£ Precision/Recall/F1 | ML è¯„ä¼° |

**é‡Œç¨‹ç¢‘**ï¼šèƒ½å¤Ÿå¾®è°ƒæ¨¡å‹å¹¶è¿½è¸ªå®éªŒ âœ…

---

### ğŸ“… é˜¶æ®µ 3: æ„å»º MLOps æµç¨‹

**ç›®æ ‡**ï¼šæ­å»ºç«¯åˆ°ç«¯çš„ ML Pipeline

| å­¦ä¹ å†…å®¹ | å®è·µé¡¹ç›® | æ–°å¢æŠ€èƒ½ |
|---------|---------|---------|
| ML CI/CD Pipeline | è‡ªåŠ¨åŒ–è®­ç»ƒ-æµ‹è¯•-éƒ¨ç½² | GitHub Actions + ML |
| æ•°æ®éªŒè¯å’Œæ¼‚ç§»æ£€æµ‹ | æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥ | Great Expectations |
| æ¨¡å‹ A/B æµ‹è¯• | Canary éƒ¨ç½² + æ€§èƒ½å¯¹æ¯” | A/B æµ‹è¯•ç­–ç•¥ |
| å‘é‡æ•°æ®åº“ (RAG åŸºç¡€) | æ„å»ºç®€å•çš„æ–‡æ¡£é—®ç­”ç³»ç»Ÿ | Chroma / Weaviate |

**é‡Œç¨‹ç¢‘**ï¼šå®Œæ•´çš„ MLOps æµç¨‹ âœ…

---

### ğŸ“… é˜¶æ®µ 4: æ·±å…¥ä¸“ä¸šæ–¹å‘

**æ ¹æ®èŒä¸šç›®æ ‡é€‰æ‹©æ·±å…¥æ–¹å‘**ï¼š

| æ–¹å‘ | å­¦ä¹ å†…å®¹ | å­¦ä¹ éš¾åº¦ |
|------|---------|---------|
| **MLOps å·¥ç¨‹å¸ˆ** | Kubeflowã€Airflowã€é«˜çº§ç›‘æ§ | â­â­â­ |
| **è®­ç»ƒåŸºç¡€è®¾æ–½** | åˆ†å¸ƒå¼è®­ç»ƒã€GPU é›†ç¾¤ç®¡ç† | â­â­â­â­ |
| **RAG/Agent å·¥ç¨‹å¸ˆ** | LangChainã€å‘é‡æ•°æ®åº“ä¼˜åŒ– | â­â­â­ |
| **æ¨ç†ä¼˜åŒ–** | é‡åŒ–ã€vLLMã€TensorRT | â­â­â­â­ |

---

## äº”ã€æ€»ç»“ï¼šä½ çš„ä¼˜åŠ¿åœ°å›¾

### ğŸ”¥ æ ¸å¿ƒä¼˜åŠ¿ï¼ˆç¨€ç¼ºä¸”é«˜ä»·å€¼ï¼‰

| ä½ çš„ä¼˜åŠ¿ | ML å›¢é˜Ÿçš„ç—›ç‚¹ | ä½ çš„ä»·å€¼ |
|---------|-------------|---------|
| **ç³»ç»Ÿè®¾è®¡** | è®­ç»ƒ Pipeline æ··ä¹± | è®¾è®¡ç¨³å®šã€å¯æ‰©å±•çš„æ¶æ„ |
| **CI/CD** | æ¨¡å‹éƒ¨ç½²æµç¨‹æ··ä¹± | å»ºç«‹è‡ªåŠ¨åŒ– MLOps æµç¨‹ |
| **æ€§èƒ½ä¼˜åŒ–** | GPU åˆ©ç”¨ç‡ä½ã€æ¨ç†æ…¢ | ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡å’Œå»¶è¿Ÿ |
| **ç¨³å®šæ€§å·¥ç¨‹** | è®­ç»ƒä»»åŠ¡ç»å¸¸å¤±è´¥ | è®¾è®¡å®¹é”™æœºåˆ¶ã€Checkpoint |
| **ç›‘æ§å‘Šè­¦** | æ¨¡å‹æ‚„æ‚„é€€åŒ–æ— äººçŸ¥æ™“ | å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³» |

---

### ğŸ¯ æœ€ä½³è½¬å‹ç­–ç•¥

**Step 1: å¿«é€Ÿè¯æ˜ä»·å€¼**
- ç”¨ç°æœ‰æŠ€èƒ½ï¼ˆAPIã€Dockerã€K8sï¼‰å¿«é€Ÿæ­å»º ML æ¨ç†æœåŠ¡
- å±•ç¤ºä½ åœ¨å·¥ç¨‹åŒ–æ–¹é¢çš„ä¼˜åŠ¿

**Step 2: è¡¥å…… ML åŸºç¡€**
- å­¦ä¹ æ¨¡å‹å¾®è°ƒã€å®éªŒè¿½è¸ªã€æ•°æ®ç‰ˆæœ¬æ§åˆ¶
- ç†è§£ ML æ ¸å¿ƒæ¦‚å¿µï¼ˆä¸éœ€è¦æ·±å…¥æ•°å­¦ï¼‰

**Step 3: æˆä¸ºå…¨æ ˆ ML å·¥ç¨‹å¸ˆ**
- æ„å»ºç«¯åˆ°ç«¯çš„ MLOps æµç¨‹
- é€‰æ‹©ä¸€ä¸ªæ–¹å‘æ·±å…¥ï¼ˆMLOps / åŸºç¡€è®¾æ–½ / RAGï¼‰

---

### ğŸ’¡ å…³é”®å»ºè®®

1. **ä¸è¦ä¸€å¼€å§‹å°±å­¦æ•°å­¦**
   - å…ˆç”¨ Hugging Face è·‘é€šæ¨¡å‹ï¼Œè¯æ˜ä»·å€¼
   - é‡åˆ°éœ€è¦æ—¶å†è¡¥å……æ•°å­¦çŸ¥è¯†

2. **å……åˆ†å‘æŒ¥å·¥ç¨‹ä¼˜åŠ¿**
   - ML å›¢é˜Ÿæœ€ç¼ºçš„ä¸æ˜¯ç®—æ³•ä¸“å®¶ï¼Œè€Œæ˜¯å·¥ç¨‹èƒ½åŠ›
   - ä½ çš„ç³»ç»Ÿè®¾è®¡ã€ç¨³å®šæ€§ç»éªŒæ˜¯æ ¸å¿ƒç«äº‰åŠ›

3. **é€‰æ‹©åˆé€‚çš„èµ·ç‚¹**
   - **åŸºç¡€è®¾æ–½/MLOps**ï¼šæœ€å¿«ä¸Šæ‰‹ï¼Œæ€ç»´å†²å‡»å°
   - **RAG/Agent**ï¼šåº”ç”¨å¹¿æ³›ï¼Œå¿«é€Ÿçœ‹åˆ°æˆæœ
   - **é¢„è®­ç»ƒ/å¾®è°ƒ**ï¼šæŠ€æœ¯æ·±åº¦å¤§ï¼ŒèŒä¸šå¤©èŠ±æ¿é«˜

4. **å®è·µé©±åŠ¨å­¦ä¹ **
   - 70% æ—¶é—´åšé¡¹ç›®ï¼Œ30% æ—¶é—´å­¦ç†è®º
   - ä¿æŒå­¦ä¹ èŠ‚å¥ï¼ŒæŒç»­å®Œæˆå°é¡¹ç›®

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æŠ€èƒ½è¿ç§»ä¸è½¬å‹

- [Jobs in Data - Software Engineer to ML Transition (2025)](https://jobs-in-data.com/blog/software-engineer-transition-to-machine-learning)
- [LinkedIn - Software Engineer to AI/ML (2025)](https://www.linkedin.com/pulse/from-software-engineer-aiml-beginners-guide-malaika-f--siogf)
- [Medium - ML Engineer 2026 Roadmap](https://medium.com/write-a-catalyst/how-to-become-a-machine-learning-engineer-2026-your-expert-roadmap-7cb2b7e5daa3)
- [Coursera - ML Learning Roadmap (2026)](https://www.coursera.org/resources/ml-learning-roadmap)

### MLOps ä¸ CI/CD

- [MLOps Best Practices for Accelerating Deployments (2025)](https://www.mlopscrew.com/blog/cicd-best-practices-for-accelerating-mlops-deployment)
- [GoML - 10 MLOps Best Practices 2025](https://www.goml.io/blog/mlops-best-practices)
- [Clarifai - MLOps Best Practices](https://www.clarifai.com/blog/mlops-best-practices)

### GPU é›†ç¾¤ä¸ Kubernetes

- [NVIDIA GPU Operator with GKE (2025)](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/25.10/google-gke.html)
- [Google Cloud - Fine-tune Gemma with GPUs on GKE](https://docs.cloud.google.com/kubernetes-engine/docs/tutorials/finetune-gemma-gpu)
- [AnantaCloud - AI/ML Workloads on Kubernetes](https://www.anantacloud.com/post/ai-ml-workloads-on-kubernetes-running-scalable-machine-learning-pipelines-with-gpu-acceleration-and)

### åˆ†å¸ƒå¼è®­ç»ƒ

- [CrowdStrike - Training GenAI Models with Distributed Computing](https://www.crowdstrike.com/en-us/blog/how-crowdstrike-trains-genai-models-at-scale-using-distributed-computing/)
- [Red Hat - Distributed Inference with llm-d](https://developers.redhat.com/articles/2025/11/21/introduction-distributed-inference-llm-d)
- [arXiv - Scalability and Resilience in Distributed LLM Training](https://jicrcr.com/index.php/jicrcr/article/download/3189/2721/6817)

### æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶

- [Dev.to - ML Done Right: Versioning with DVC & MLflow](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)
- [CodeZup - ML Model Versioning: MLflow & DVC Guide](https://codezup.com/ml-model-versioning-mlflow-dvc/)
- [LakeFS - Model Versioning Tools & Best Practices](https://lakefs.io/blog/model-versioning/)

---

> ğŸ“… **æœ€åæ›´æ–°**ï¼š2026 å¹´ 1 æœˆ  
> ğŸ¯ **ä¸‹ä¸€ç¯‡**ï¼š[03 - å­¦ä¹ è·¯å¾„è®¾è®¡ï¼šç³»ç»ŸåŒ–å­¦ä¹ è§„åˆ’æŒ‡å—](./03-learning-path.md)  
> ğŸ’¬ **åé¦ˆ**ï¼šå‘ç°é”™è¯¯æˆ–æœ‰å»ºè®®ï¼Ÿæ¬¢è¿æ Issueï¼

---

**è®°ä½**ï¼šä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹å¸ˆè½¬å‹ LLM è®­ç»ƒï¼Œ**ä½ å·²ç»æ‹¥æœ‰ 70% çš„èƒ½åŠ›**ã€‚

ä½ çš„ç³»ç»Ÿè®¾è®¡ã€å·¥ç¨‹åŒ–ç»éªŒåœ¨ ML é¢†åŸŸ**æå…¶å®è´µä¸”ç¨€ç¼º**â€”â€”ä¸è¦ä½ä¼°è‡ªå·±çš„ä¼˜åŠ¿ï¼Œå¤§èƒ†å¼€å§‹å§ï¼ğŸ’ª
