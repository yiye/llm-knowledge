# 04 - æ•°å­¦åŸºç¡€é€Ÿæˆï¼šLLMå·¥ç¨‹å¸ˆçš„æœ€å°æ•°å­¦ä½“ç³»

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šä¼ ç»Ÿç¨‹åºå‘˜ä¸éœ€è¦æ•°å­¦åšå£«æ°´å¹³ï¼æœ¬æ–‡å°†ç”¨æœ€ç›´è§‚çš„æ–¹å¼è®²è§£ LLM æ‰€éœ€çš„"**æœ€å°å¿…è¦æ•°å­¦é›†**"ï¼Œé‡ç‚¹æ˜¯**å¤Ÿç”¨**è€Œé**å®Œç¾**ã€‚ç”¨ä»£ç å’Œå®ä¾‹ç†è§£æ•°å­¦ï¼Œè€Œéå½¢å¼åŒ–è¯æ˜ã€‚

---

## ğŸ“– å¼•è¨€ï¼šç¨‹åºå‘˜éœ€è¦å¤šå°‘æ•°å­¦ï¼Ÿ

### âŒ å¸¸è§çš„æ•°å­¦ç„¦è™‘

å¾ˆå¤šä¼ ç»Ÿç¨‹åºå‘˜çœ‹åˆ° LLM ç›¸å…³çš„è®ºæ–‡å’Œæ•™ç¨‹ï¼Œç¬¬ä¸€ååº”æ˜¯ï¼š

```
âŒ "å®Œäº†ï¼Œå…¨æ˜¯å¸Œè…Šå­—æ¯ï¼Œçœ‹ä¸æ‡‚"
âŒ "æˆ‘é«˜ç­‰æ•°å­¦æ—©å°±è¿˜ç»™è€å¸ˆäº†"
âŒ "æ˜¯ä¸æ˜¯è¦é‡æ–°å­¦ä¸€éå¤§å­¦æ•°å­¦ï¼Ÿ"
âŒ "æˆ‘è¿çŸ©é˜µä¹˜æ³•éƒ½å¿˜äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ"
```

### âœ… çœŸç›¸ï¼šä½ éœ€è¦çš„æ•°å­¦æ¯”æƒ³è±¡ä¸­å°‘å¾—å¤š

æ ¹æ® [Mathematics for Machine Learning and Data Science Specialization](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science) å’Œ [Mathematical Foundations for Understanding LLMs 2025](https://actionbridge.io/en-US/llmtutorial/p/part1-mathematical-foundations-for-understanding-llms-introduction)ï¼š

> **æ ¸å¿ƒè§‚ç‚¹**ï¼šLLM å·¥ç¨‹å¸ˆéœ€è¦çš„æ•°å­¦æ˜¯**å·¥ç¨‹æ•°å­¦**ï¼Œä¸æ˜¯**ç†è®ºæ•°å­¦**ã€‚ä½ éœ€è¦çŸ¥é“"æ€ä¹ˆç”¨"ï¼Œè€Œä¸æ˜¯"æ€ä¹ˆè¯æ˜"ã€‚

**å¯¹æ¯”**ï¼š

| æ•°å­¦é¢†åŸŸ | ç†è®ºæ•°å­¦å®¶éœ€è¦ | LLM å·¥ç¨‹å¸ˆéœ€è¦ | å·®è· |
|---------|-------------|-------------|------|
| **çº¿æ€§ä»£æ•°** | å‘é‡ç©ºé—´ç†è®ºã€æŠ½è±¡ä»£æ•° | çŸ©é˜µä¹˜æ³•ã€ç‚¹ç§¯ã€ç‰¹å¾å€¼ | 10x ç®€å• |
| **å¾®ç§¯åˆ†** | å®åˆ†æã€æµ‹åº¦è®º | åå¯¼æ•°ã€é“¾å¼æ³•åˆ™ | 20x ç®€å• |
| **æ¦‚ç‡ç»Ÿè®¡** | éšæœºè¿‡ç¨‹ã€æµ‹åº¦è®º | æ¦‚ç‡åˆ†å¸ƒã€æœŸæœ›ã€æ–¹å·® | 15x ç®€å• |
| **ä¼˜åŒ–ç†è®º** | å‡¸åˆ†æã€å¯¹å¶ç†è®º | æ¢¯åº¦ä¸‹é™ã€å­¦ä¹ ç‡ | 10x ç®€å• |

---

### ğŸ¯ æœ¬æ–‡ç­–ç•¥ï¼šä»ä»£ç åˆ°æ•°å­¦

**ä¼ ç»Ÿæ•™å­¦æ–¹å¼**ï¼ˆâŒ ä¸æ¨èï¼‰ï¼š
```
ç†è®ºå…¬å¼ â†’ å½¢å¼åŒ–è¯æ˜ â†’ æŠ½è±¡ç†è§£ â†’ (å¯èƒ½)çœ‹ä¸ªä»£ç 
```

**æœ¬æ–‡æ–¹å¼**ï¼ˆâœ… ç¨‹åºå‘˜å‹å¥½ï¼‰ï¼š
```
ä»£ç ç¤ºä¾‹ â†’ ç›´è§‚ç†è§£ â†’ å¿…è¦å…¬å¼ â†’ å®é™…åº”ç”¨
```

**æ ¸å¿ƒåŸåˆ™**ï¼š
1. **å…ˆçœ‹ä»£ç ï¼Œå†çœ‹å…¬å¼**ï¼šç”¨ NumPy/PyTorch ä»£ç å»ºç«‹ç›´è§‰
2. **å‡ ä½•ç›´è§‰ä¼˜å…ˆ**ï¼šç”¨å¯è§†åŒ–ç†è§£ï¼Œè€Œéå…¬å¼æ¨å¯¼
3. **æŒ‰éœ€å­¦ä¹ **ï¼šé‡åˆ°ä¸æ‡‚çš„å†æ·±å…¥ï¼Œè€Œéä¸€æ¬¡å­¦å®Œ
4. **å®è·µéªŒè¯**ï¼šæ¯ä¸ªæ¦‚å¿µéƒ½é…ä»£ç ï¼Œå¯ä»¥ç«‹å³è¿è¡ŒéªŒè¯

---

## ä¸€ã€çº¿æ€§ä»£æ•°ï¼šLLM çš„è¯­è¨€

> **ä¸ºä»€ä¹ˆé‡è¦**ï¼šTransformer çš„æ ¸å¿ƒâ€”â€”Attention æœºåˆ¶ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç³»åˆ—çŸ©é˜µè¿ç®—ã€‚ä¸æ‡‚çº¿æ€§ä»£æ•°ï¼Œå°±çœ‹ä¸æ‡‚ Attention åœ¨åšä»€ä¹ˆã€‚

æ ¹æ® [Deep Learning, Transformers and Linear Algebra Perspective 2025](https://link.springer.com/article/10.1007/s11075-025-02218-2)ï¼š

> æ‰€æœ‰ AI æŠ€æœ¯éƒ½ä¾èµ–å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ•°æ®ã€ä¼˜åŒ–æ–¹æ³•ã€ç»Ÿè®¡ç›´è§‰å’Œ**çº¿æ€§ä»£æ•°**ã€‚åœ¨ LLM ä¸­ï¼Œè¯è¢«åµŒå…¥åˆ°æ¬§å‡ é‡Œå¾—ç©ºé—´ï¼Œä¹‹åçš„æ‰€æœ‰æ“ä½œéƒ½é«˜åº¦ä¾èµ–å‘é‡ã€çŸ©é˜µå’Œå¼ é‡ã€‚

---

### 1.1 å‘é‡ï¼ˆVectorï¼‰ï¼šè¯çš„æ•°å­—è¡¨ç¤º

#### ğŸ”¢ å‘é‡æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šå‘é‡å°±æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ã€‚

```python
import numpy as np

# ä¸€ä¸ªå‘é‡ï¼ˆè¯åµŒå…¥ï¼‰
word_embedding = np.array([0.2, -0.5, 0.8, 0.1])

# è¿™æ˜¯ä»€ä¹ˆï¼Ÿ
# â†’ "apple" è¿™ä¸ªè¯åœ¨ 4 ç»´ç©ºé—´çš„åæ ‡
# â†’ æ¯ä¸ªç»´åº¦æ•æ‰è¯çš„æŸä¸ª"è¯­ä¹‰ç‰¹å¾"
```

#### ğŸ“ å‘é‡çš„å‡ ä½•æ„ä¹‰

```python
import matplotlib.pyplot as plt
import numpy as np

# å¯è§†åŒ–ï¼š2D å‘é‡
vec1 = np.array([2, 3])  # "king"
vec2 = np.array([1, 2])  # "queen"

plt.figure(figsize=(6, 6))
plt.quiver(0, 0, vec1[0], vec1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='king')
plt.quiver(0, 0, vec2[0], vec2[1], angles='xy', scale_units='xy', scale=1, color='red', label='queen')
plt.xlim(-1, 4)
plt.ylim(-1, 4)
plt.grid()
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.legend()
plt.title('Words as Vectors')
plt.show()

# å…³é”®æ´å¯Ÿï¼š
# å‘é‡ä¸ä»…ä»…æ˜¯æ•°å­—åˆ—è¡¨ï¼Œå®ƒä»¬åœ¨ç©ºé—´ä¸­æœ‰"æ–¹å‘"å’Œ"é•¿åº¦"
```

---

#### ğŸ”¥ å…³é”®æ“ä½œ 1ï¼šç‚¹ç§¯ï¼ˆDot Productï¼‰

**ç‚¹ç§¯æ˜¯ Attention æœºåˆ¶çš„æ ¸å¿ƒï¼**

```python
# ç‚¹ç§¯ï¼šè¡¡é‡ä¸¤ä¸ªå‘é‡çš„"ç›¸ä¼¼åº¦"
vec1 = np.array([1, 2, 3])
vec2 = np.array([4, 5, 6])

dot_product = np.dot(vec1, vec2)
print(dot_product)  # 32

# æ‰‹åŠ¨è®¡ç®—ï¼š
# 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32
```

**å‡ ä½•æ„ä¹‰**ï¼š

```python
# ç‚¹ç§¯ = |vec1| Ã— |vec2| Ã— cos(Î¸)
# Î¸ æ˜¯ä¸¤ä¸ªå‘é‡çš„å¤¹è§’

# æ¡ˆä¾‹ 1ï¼šå®Œå…¨ç›¸åŒçš„å‘é‡
vec_a = np.array([1, 0])
vec_b = np.array([1, 0])
print(np.dot(vec_a, vec_b))  # 1.0ï¼ˆå¤¹è§’ 0Â°ï¼Œcos(0Â°)=1ï¼‰

# æ¡ˆä¾‹ 2ï¼šå®Œå…¨ç›¸åçš„å‘é‡
vec_c = np.array([1, 0])
vec_d = np.array([-1, 0])
print(np.dot(vec_c, vec_d))  # -1.0ï¼ˆå¤¹è§’ 180Â°ï¼Œcos(180Â°)=-1ï¼‰

# æ¡ˆä¾‹ 3ï¼šå‚ç›´çš„å‘é‡
vec_e = np.array([1, 0])
vec_f = np.array([0, 1])
print(np.dot(vec_e, vec_f))  # 0.0ï¼ˆå¤¹è§’ 90Â°ï¼Œcos(90Â°)=0ï¼‰

# ğŸ”¥ å…³é”®æ´å¯Ÿï¼š
# ç‚¹ç§¯è¶Šå¤§ â†’ å‘é‡è¶Š"ç›¸ä¼¼"
# ç‚¹ç§¯ä¸º 0 â†’ å‘é‡"æ­£äº¤"ï¼ˆå®Œå…¨ä¸ç›¸å…³ï¼‰
# ç‚¹ç§¯ä¸ºè´Ÿ â†’ å‘é‡"ç›¸å"
```

**åœ¨ LLM ä¸­çš„åº”ç”¨**ï¼š

```python
# Attention Score çš„æ ¸å¿ƒè®¡ç®—
query = np.array([1, 2, 3])  # å½“å‰è¯ï¼š"cat"
key1 = np.array([1, 2, 2])   # å€™é€‰è¯ 1ï¼š"dog"
key2 = np.array([0, 0, 1])   # å€™é€‰è¯ 2ï¼š"table"

score1 = np.dot(query, key1)  # 9ï¼ˆç›¸ä¼¼ï¼ï¼‰
score2 = np.dot(query, key2)  # 3ï¼ˆä¸å¤ªç›¸ä¼¼ï¼‰

# â†’ Attention æœºåˆ¶ä¼šæ›´å…³æ³¨ "dog"ï¼ˆscore æ›´é«˜ï¼‰
```

---

#### ğŸ”¥ å…³é”®æ“ä½œ 2ï¼šå‘é‡é•¿åº¦ï¼ˆèŒƒæ•°ï¼‰

```python
# L2 èŒƒæ•°ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
vec = np.array([3, 4])
length = np.linalg.norm(vec)
print(length)  # 5.0

# æ‰‹åŠ¨è®¡ç®—ï¼š
# sqrt(3^2 + 4^2) = sqrt(9 + 16) = sqrt(25) = 5

# ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
# åœ¨ Attention ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸éœ€è¦"å½’ä¸€åŒ–"å‘é‡ï¼ˆè®©é•¿åº¦ä¸º 1ï¼‰
vec_normalized = vec / np.linalg.norm(vec)
print(vec_normalized)  # [0.6 0.8]
print(np.linalg.norm(vec_normalized))  # 1.0
```

**å•ä½å‘é‡çš„æ„ä¹‰**ï¼š

```python
# å½’ä¸€åŒ–åï¼Œç‚¹ç§¯ = cos(Î¸)ï¼ˆçº¯ç²¹çš„è§’åº¦ç›¸ä¼¼åº¦ï¼‰
vec1 = np.array([1, 2, 3])
vec2 = np.array([4, 5, 6])

# å½’ä¸€åŒ–
vec1_norm = vec1 / np.linalg.norm(vec1)
vec2_norm = vec2 / np.linalg.norm(vec2)

# ä½™å¼¦ç›¸ä¼¼åº¦
cosine_similarity = np.dot(vec1_norm, vec2_norm)
print(f"Cosine similarity: {cosine_similarity:.4f}")  # 0.9746

# ğŸ”¥ è¿™å°±æ˜¯"ä½™å¼¦ç›¸ä¼¼åº¦"ï¼ˆCosine Similarityï¼‰
# ç”¨äºè¡¡é‡è¯å‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼
```

---

### 1.2 çŸ©é˜µï¼ˆMatrixï¼‰ï¼šæ‰¹é‡å¤„ç†çš„åˆ©å™¨

#### ğŸ”¢ çŸ©é˜µæ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šçŸ©é˜µå°±æ˜¯äºŒç»´æ•°ç»„ã€‚

```python
# çŸ©é˜µï¼š3ä¸ªè¯çš„åµŒå…¥ï¼ˆæ¯ä¸ªè¯ 4 ç»´ï¼‰
word_embeddings = np.array([
    [0.2, -0.5, 0.8, 0.1],  # "the"
    [0.1,  0.3, -0.2, 0.9],  # "cat"
    [-0.4, 0.6,  0.1, 0.3]   # "sat"
])

print(word_embeddings.shape)  # (3, 4) â†’ 3 ä¸ªè¯ï¼Œæ¯ä¸ª 4 ç»´
```

---

#### ğŸ”¥ å…³é”®æ“ä½œï¼šçŸ©é˜µä¹˜æ³•ï¼ˆMatrix Multiplicationï¼‰

**è¿™æ˜¯ Transformer ä¸­æœ€é¢‘ç¹çš„æ“ä½œï¼**

```python
# çŸ©é˜µ A (mÃ—n) Ã— çŸ©é˜µ B (nÃ—p) = çŸ©é˜µ C (mÃ—p)

A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])  # (2, 3)

B = np.array([
    [7, 8],
    [9, 10],
    [11, 12]
])  # (3, 2)

C = np.dot(A, B)  # æˆ–è€… A @ B
print(C)
# [[ 58  64]
#  [139 154]]

print(C.shape)  # (2, 2)
```

**æ‰‹åŠ¨è®¡ç®—ç¬¬ä¸€ä¸ªå…ƒç´ **ï¼š

```python
# C[0, 0] = A[0, :] Â· B[:, 0]
# = 1*7 + 2*9 + 3*11
# = 7 + 18 + 33
# = 58 âœ…
```

**å‡ ä½•æ„ä¹‰**ï¼šçŸ©é˜µä¹˜æ³•æ˜¯**çº¿æ€§å˜æ¢**ã€‚

```python
# æ¡ˆä¾‹ï¼šæ—‹è½¬å˜æ¢
import matplotlib.pyplot as plt

# åŸå§‹å‘é‡
vec = np.array([1, 0])

# æ—‹è½¬çŸ©é˜µï¼ˆé€†æ—¶é’ˆæ—‹è½¬ 90Â°ï¼‰
rotation_matrix = np.array([
    [0, -1],
    [1, 0]
])

# åº”ç”¨å˜æ¢
vec_rotated = rotation_matrix @ vec
print(vec_rotated)  # [0, 1]ï¼ˆä» x è½´æ—‹è½¬åˆ° y è½´ï¼‰

# å¯è§†åŒ–
plt.figure(figsize=(6, 6))
plt.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Original')
plt.quiver(0, 0, vec_rotated[0], vec_rotated[1], angles='xy', scale_units='xy', scale=1, color='red', label='Rotated')
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.grid()
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.legend()
plt.title('Matrix as Linear Transformation')
plt.show()
```

---

#### ğŸ”¥ åœ¨ Transformer ä¸­çš„åº”ç”¨ï¼šQ/K/V æŠ•å½±

```python
# Attention çš„æ ¸å¿ƒï¼šå°†è¾“å…¥æŠ•å½±åˆ° Query/Key/Value ç©ºé—´

# è¾“å…¥ï¼š3 ä¸ªè¯ï¼Œæ¯ä¸ª 4 ç»´
X = np.array([
    [0.2, -0.5, 0.8, 0.1],  # word 1
    [0.1,  0.3, -0.2, 0.9],  # word 2
    [-0.4, 0.6,  0.1, 0.3]   # word 3
])  # shape: (3, 4)

# å­¦ä¹ åˆ°çš„æƒé‡çŸ©é˜µ
W_Q = np.random.randn(4, 2)  # (4, 2) - å°† 4 ç»´æŠ•å½±åˆ° 2 ç»´
W_K = np.random.randn(4, 2)  # (4, 2)
W_V = np.random.randn(4, 2)  # (4, 2)

# æŠ•å½±
Q = X @ W_Q  # (3, 4) @ (4, 2) = (3, 2)
K = X @ W_K  # (3, 2)
V = X @ W_V  # (3, 2)

print(f"Query shape: {Q.shape}")   # (3, 2) - 3 ä¸ªè¯ï¼Œæ¯ä¸ª 2 ç»´
print(f"Key shape: {K.shape}")     # (3, 2)
print(f"Value shape: {V.shape}")   # (3, 2)

# ğŸ”¥ å…³é”®æ´å¯Ÿï¼š
# çŸ©é˜µä¹˜æ³•è®©æˆ‘ä»¬å¯ä»¥"æ‰¹é‡"å¤„ç†æ‰€æœ‰è¯çš„æŠ•å½±ï¼
# ä¸éœ€è¦å†™å¾ªç¯ï¼ŒGPU å¯ä»¥é«˜æ•ˆå¹¶è¡Œè®¡ç®—ï¼
```

---

### 1.3 ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ï¼ˆEigenvalues & Eigenvectorsï¼‰

#### ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªï¼Ÿ

**ç›´è§‚ç†è§£**ï¼šç‰¹å¾å‘é‡æ˜¯çŸ©é˜µå˜æ¢çš„"ç¨³å®šæ–¹å‘"ã€‚

```python
# çŸ©é˜µ A
A = np.array([
    [2, 1],
    [1, 2]
])

# è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Eigenvalues:")
print(eigenvalues)  # [3. 1.]

print("\nEigenvectors:")
print(eigenvectors)
# [[ 0.70710678 -0.70710678]
#  [ 0.70710678  0.70710678]]
```

**ä»€ä¹ˆæ„æ€ï¼Ÿ**

```python
# ç‰¹å¾å‘é‡ï¼šçŸ©é˜µå˜æ¢åªæ”¹å˜é•¿åº¦ï¼Œä¸æ”¹å˜æ–¹å‘çš„å‘é‡

v1 = eigenvectors[:, 0]  # ç¬¬ä¸€ä¸ªç‰¹å¾å‘é‡
Î»1 = eigenvalues[0]      # å¯¹åº”çš„ç‰¹å¾å€¼

# éªŒè¯ï¼šA @ v = Î» * v
print("A @ v1:")
print(A @ v1)  # [ 2.12132034  2.12132034]

print("\nÎ»1 * v1:")
print(Î»1 * v1)  # [ 2.12132034  2.12132034]

# âœ… ç›¸ç­‰ï¼è¯´æ˜ v1 æ˜¯ç‰¹å¾å‘é‡ï¼ŒÎ»1 æ˜¯ç‰¹å¾å€¼

# ğŸ”¥ å‡ ä½•æ„ä¹‰ï¼š
# å½“çŸ©é˜µ A ä½œç”¨åœ¨ v1 ä¸Šæ—¶ï¼Œv1 çš„æ–¹å‘ä¸å˜ï¼Œåªæ˜¯é•¿åº¦å˜ä¸ºåŸæ¥çš„ 3 å€
```

**å¯è§†åŒ–**ï¼š

```python
import matplotlib.pyplot as plt

# åŸå§‹å‘é‡å’Œå˜æ¢åçš„å‘é‡
v = eigenvectors[:, 0]
Av = A @ v

plt.figure(figsize=(8, 8))
plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', width=0.01, label='v (eigenvector)')
plt.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, color='red', width=0.01, label='AÂ·v (stretched)')

# å¯¹æ¯”ï¼šéç‰¹å¾å‘é‡
v_other = np.array([1, 0])
Av_other = A @ v_other
plt.quiver(0, 0, v_other[0], v_other[1], angles='xy', scale_units='xy', scale=1, color='green', width=0.005, label='v_other')
plt.quiver(0, 0, Av_other[0], Av_other[1], angles='xy', scale_units='xy', scale=1, color='orange', width=0.005, label='AÂ·v_other (rotated!)')

plt.xlim(-1, 3)
plt.ylim(-1, 3)
plt.grid()
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.legend()
plt.title('Eigenvectors: Stable Directions')
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼š
# - è“è‰² â†’ çº¢è‰²ï¼šç‰¹å¾å‘é‡æ–¹å‘ä¸å˜ï¼Œåªæ˜¯æ‹‰ä¼¸
# - ç»¿è‰² â†’ æ©™è‰²ï¼šéç‰¹å¾å‘é‡æ–¹å‘æ”¹å˜äº†ï¼
```

---

#### ğŸ”¥ åœ¨ LLM ä¸­çš„åº”ç”¨

**æ¡ˆä¾‹ 1ï¼šä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰é™ç»´**

```python
from sklearn.decomposition import PCA

# é«˜ç»´è¯åµŒå…¥
word_embeddings = np.random.randn(100, 768)  # 100 ä¸ªè¯ï¼Œ768 ç»´

# PCA é™ç»´åˆ° 2 ç»´
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(word_embeddings)

print(f"Original shape: {word_embeddings.shape}")  # (100, 768)
print(f"Reduced shape: {embeddings_2d.shape}")     # (100, 2)

# ğŸ”¥ PCA çš„æ ¸å¿ƒï¼šæ‰¾åˆ°æ•°æ®åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡
# è¿™äº›ç‰¹å¾å‘é‡ä»£è¡¨æ•°æ®çš„"ä¸»è¦æ–¹å‘"
```

**æ¡ˆä¾‹ 2ï¼šLayer Normalization çš„ç¨³å®šæ€§åˆ†æ**

åœ¨ Transformer çš„ Layer Normalization ä¸­ï¼Œç‰¹å¾å€¼ç”¨äºåˆ†æç½‘ç»œçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚

---

### ğŸ† çº¿æ€§ä»£æ•°å°ç»“

| æ¦‚å¿µ | ç¨‹åºå‘˜ç±»æ¯” | åœ¨ LLM ä¸­çš„ä½œç”¨ | é‡è¦æ€§ |
|-----|----------|--------------|-------|
| **å‘é‡** | ä¸€ç»´æ•°ç»„ | è¯åµŒå…¥ã€éšè—çŠ¶æ€ | â­â­â­â­â­ |
| **ç‚¹ç§¯** | ä¸¤ä¸ªæ•°ç»„å¯¹åº”å…ƒç´ ç›¸ä¹˜å†æ±‚å’Œ | Attention Score è®¡ç®— | â­â­â­â­â­ |
| **çŸ©é˜µ** | äºŒç»´æ•°ç»„ | æƒé‡çŸ©é˜µã€æ‰¹é‡å¤„ç† | â­â­â­â­â­ |
| **çŸ©é˜µä¹˜æ³•** | åµŒå¥—å¾ªç¯ï¼ˆä½† GPU å¹¶è¡Œï¼‰ | Q/K/V æŠ•å½±ã€å‰é¦ˆå±‚ | â­â­â­â­â­ |
| **ç‰¹å¾å€¼/å‘é‡** | çŸ©é˜µçš„"ç¨³å®šæ–¹å‘" | PCA é™ç»´ã€ç¨³å®šæ€§åˆ†æ | â­â­â­ |

**æ¨èå­¦ä¹ èµ„æº**ï¼š
- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)ï¼ˆå¯è§†åŒ–è®²è§£ï¼Œå¼ºçƒˆæ¨èï¼ï¼‰
- [Building Intuition for Transformers Linear Algebra 2025](https://medium.com/@ziyamomin/building-intuition-for-how-transformers-use-linear-algebra-without-a-math-background-7c6812b2f037)

---

## äºŒã€å¾®ç§¯åˆ†ï¼šè®­ç»ƒçš„æ•°å­¦åŸºç¡€

> **ä¸ºä»€ä¹ˆé‡è¦**ï¼šç¥ç»ç½‘ç»œçš„è®­ç»ƒæœ¬è´¨æ˜¯ä¼˜åŒ–é—®é¢˜â€”â€”å¦‚ä½•è°ƒæ•´å‚æ•°è®©æŸå¤±å‡½æ•°æœ€å°ï¼Ÿå¾®ç§¯åˆ†ï¼ˆç‰¹åˆ«æ˜¯æ¢¯åº¦ï¼‰å‘Šè¯‰æˆ‘ä»¬"å¾€å“ªä¸ªæ–¹å‘è°ƒæ•´"ã€‚

æ ¹æ® [Backpropagation: Calculus on Computational Graphs](http://colah.github.io/posts/2015-08-Backprop/index.html)ï¼š

> åå‘ä¼ æ’­ç®—æ³•ï¼ˆä¹Ÿç§°"é€†å‘å¾®åˆ†"ï¼‰ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œå¯ä»¥è®©æ¢¯åº¦ä¸‹é™çš„è®­ç»ƒé€Ÿåº¦æå‡**ä¸€åƒä¸‡å€**ï¼

---

### 2.1 å¯¼æ•°ï¼ˆDerivativeï¼‰ï¼šå˜åŒ–ç‡

#### ğŸ”¢ å¯¼æ•°æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šå¯¼æ•°æ˜¯å‡½æ•°çš„"æ–œç‡"ï¼Œå‘Šè¯‰ä½ "è¾“å…¥å˜åŒ–æ—¶ï¼Œè¾“å‡ºå¦‚ä½•å˜åŒ–"ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# å‡½æ•°ï¼šf(x) = x^2
def f(x):
    return x ** 2

# å¯¼æ•°ï¼šf'(x) = 2x
def f_derivative(x):
    return 2 * x

# å¯è§†åŒ–
x = np.linspace(-3, 3, 100)
y = f(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, label='f(x) = xÂ²')

# åœ¨ x=1 å¤„çš„åˆ‡çº¿
x0 = 1
y0 = f(x0)
slope = f_derivative(x0)  # 2
tangent_line = slope * (x - x0) + y0

plt.plot(x, tangent_line, 'r--', label=f'Tangent at x={x0} (slope={slope})')
plt.scatter([x0], [y0], color='red', s=100, zorder=5)
plt.grid()
plt.legend()
plt.title('Derivative as Slope')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.show()

# ğŸ”¥ å…³é”®æ´å¯Ÿï¼š
# å¯¼æ•° = 2 â†’ å½“ x å¢åŠ  1 æ—¶ï¼Œf(x) å¢åŠ çº¦ 2
# å¯¼æ•° = -3 â†’ å½“ x å¢åŠ  1 æ—¶ï¼Œf(x) å‡å°‘çº¦ 3
```

---

#### ğŸ”¥ æ•°å€¼å¯¼æ•°ï¼šç”¨ä»£ç ç†è§£

```python
# å¯¼æ•°çš„å®šä¹‰ï¼š
# f'(x) = lim(hâ†’0) [f(x+h) - f(x)] / h

def numerical_derivative(f, x, h=1e-5):
    """ç”¨æ•°å€¼æ–¹æ³•è®¡ç®—å¯¼æ•°"""
    return (f(x + h) - f(x)) / h

# æµ‹è¯•
x = 2.0
print(f"Analytical derivative at x={x}: {f_derivative(x)}")  # 4.0
print(f"Numerical derivative at x={x}: {numerical_derivative(f, x):.4f}")  # 4.0000

# âœ… éå¸¸æ¥è¿‘ï¼

# ğŸ”¥ è¿™å°±æ˜¯ PyTorch èƒŒååšçš„äº‹æƒ…ï¼ˆæ›´é«˜æ•ˆçš„å®ç°ï¼‰
```

---

### 2.2 åå¯¼æ•°ï¼ˆPartial Derivativeï¼‰ï¼šå¤šå˜é‡å‡½æ•°

#### ğŸ”¢ åå¯¼æ•°æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šå½“å‡½æ•°æœ‰å¤šä¸ªè¾“å…¥æ—¶ï¼Œåå¯¼æ•°å‘Šè¯‰ä½ "å›ºå®šå…¶ä»–è¾“å…¥ï¼Œæ”¹å˜æŸä¸ªè¾“å…¥æ—¶çš„å˜åŒ–ç‡"ã€‚

```python
# å‡½æ•°ï¼šf(x, y) = x^2 + 2xy + y^2
def f(x, y):
    return x**2 + 2*x*y + y**2

# åå¯¼æ•°ï¼š
# âˆ‚f/âˆ‚x = 2x + 2y
# âˆ‚f/âˆ‚y = 2x + 2y

def df_dx(x, y):
    return 2*x + 2*y

def df_dy(x, y):
    return 2*x + 2*y

# æµ‹è¯•
x, y = 1.0, 2.0
print(f"âˆ‚f/âˆ‚x at ({x}, {y}): {df_dx(x, y)}")  # 6
print(f"âˆ‚f/âˆ‚y at ({x}, {y}): {df_dy(x, y)}")  # 6

# æ•°å€¼éªŒè¯
h = 1e-5
print(f"Numerical âˆ‚f/âˆ‚x: {(f(x+h, y) - f(x, y)) / h:.4f}")  # 6.0000
print(f"Numerical âˆ‚f/âˆ‚y: {(f(x, y+h) - f(x, y)) / h:.4f}")  # 6.0000
```

---

#### ğŸ”¥ æ¢¯åº¦ï¼ˆGradientï¼‰ï¼šæ‰€æœ‰åå¯¼æ•°çš„å‘é‡

```python
# æ¢¯åº¦ = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]
def gradient(x, y):
    return np.array([df_dx(x, y), df_dy(x, y)])

# åœ¨ (1, 2) å¤„çš„æ¢¯åº¦
grad = gradient(1.0, 2.0)
print(f"Gradient at (1, 2): {grad}")  # [6 6]

# ğŸ”¥ æ¢¯åº¦çš„å‡ ä½•æ„ä¹‰ï¼šå‡½æ•°ä¸Šå‡æœ€å¿«çš„æ–¹å‘ï¼
```

**å¯è§†åŒ–æ¢¯åº¦**ï¼š

```python
from mpl_toolkits.mplot3d import Axes3D

# åˆ›å»ºç½‘æ ¼
x_range = np.linspace(-2, 2, 50)
y_range = np.linspace(-2, 2, 50)
X, Y = np.meshgrid(x_range, y_range)
Z = f(X, Y)

# 3D å›¾
fig = plt.figure(figsize=(12, 5))

# å·¦å›¾ï¼š3D æ›²é¢
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)
ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x, y)')
ax1.set_title('Function Surface')

# å³å›¾ï¼šç­‰é«˜çº¿ + æ¢¯åº¦å‘é‡
ax2 = fig.add_subplot(122)
contour = ax2.contour(X, Y, Z, levels=20)
ax2.clabel(contour, inline=True, fontsize=8)

# ç”»å‡ ä¸ªç‚¹çš„æ¢¯åº¦å‘é‡
points = [(0, 0), (1, 1), (-1, 1), (1, -1)]
for px, py in points:
    grad = gradient(px, py)
    ax2.quiver(px, py, grad[0], grad[1], color='red', scale=20, width=0.005)

ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Gradient Vectors')
ax2.grid()

plt.tight_layout()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼šæ¢¯åº¦å‘é‡æ€»æ˜¯æŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ï¼
```

---

### 2.3 é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ï¼šåå‘ä¼ æ’­çš„æ ¸å¿ƒ

#### ğŸ”¢ é“¾å¼æ³•åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜ç±»æ¯”**ï¼šå°±åƒå‡½æ•°è°ƒç”¨é“¾ï¼Œè¾“å‡ºå¯¹è¾“å…¥çš„å¯¼æ•° = ä¸­é—´æ¯ä¸€æ­¥å¯¼æ•°çš„ä¹˜ç§¯ã€‚

```python
# å‡½æ•°ç»„åˆï¼šh(x) = f(g(x))
# ä¾‹å¦‚ï¼šh(x) = (2x + 1)^2

# åˆ†è§£ï¼š
# g(x) = 2x + 1
# f(u) = u^2
# h(x) = f(g(x))

# é“¾å¼æ³•åˆ™ï¼šh'(x) = f'(g(x)) Ã— g'(x)

def g(x):
    return 2 * x + 1

def f(u):
    return u ** 2

def h(x):
    return f(g(x))

# å¯¼æ•°
def g_derivative(x):
    return 2

def f_derivative(u):
    return 2 * u

def h_derivative(x):
    # é“¾å¼æ³•åˆ™
    u = g(x)
    return f_derivative(u) * g_derivative(x)

# æµ‹è¯•
x = 3.0
print(f"h'({x}) = {h_derivative(x)}")  # 20

# éªŒè¯ï¼šh(x) = (2x+1)^2 = 4x^2 + 4x + 1 â†’ h'(x) = 8x + 4
print(f"Direct calculation: {8*x + 4}")  # 28... ç­‰ç­‰ï¼Œä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿ

# é‡æ–°ç®—ï¼š
u = g(x)  # 7
print(f"f'(g({x})) = f'({u}) = {f_derivative(u)}")  # 14
print(f"g'({x}) = {g_derivative(x)}")  # 2
print(f"h'({x}) = {f_derivative(u) * g_derivative(x)}")  # 28 âœ…

# æ•°å€¼éªŒè¯
h_val = 1e-5
print(f"Numerical: {(h(x + h_val) - h(x)) / h_val:.4f}")  # 28.0000
```

---

#### ğŸ”¥ åœ¨ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨ï¼šåå‘ä¼ æ’­

```python
# ç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ
# y = f(W2 Â· f(W1 Â· x))
# å…¶ä¸­ f æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ sigmoidï¼‰

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# å‰å‘ä¼ æ’­
x = np.array([1.0, 2.0])  # è¾“å…¥
W1 = np.array([[0.5, 0.3], [0.2, 0.8]])  # (2, 2)
W2 = np.array([1.0, 0.5])  # (2,)

# Layer 1
z1 = W1 @ x  # (2, 2) @ (2,) = (2,)
a1 = sigmoid(z1)  # æ¿€æ´»

# Layer 2
z2 = W2 @ a1  # (2,) @ (2,) = scalar
y_pred = sigmoid(z2)  # è¾“å‡º

print(f"Predicted output: {y_pred:.4f}")

# å‡è®¾çœŸå®æ ‡ç­¾
y_true = 1.0

# æŸå¤±å‡½æ•°ï¼šMSE
loss = (y_pred - y_true) ** 2
print(f"Loss: {loss:.4f}")

# ğŸ”¥ åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
# âˆ‚Loss/âˆ‚W2 = ?

# é“¾å¼æ³•åˆ™ï¼š
# âˆ‚Loss/âˆ‚W2 = âˆ‚Loss/âˆ‚y_pred Ã— âˆ‚y_pred/âˆ‚z2 Ã— âˆ‚z2/âˆ‚W2

# 1. âˆ‚Loss/âˆ‚y_pred
dLoss_dy_pred = 2 * (y_pred - y_true)

# 2. âˆ‚y_pred/âˆ‚z2ï¼ˆsigmoid çš„å¯¼æ•°ï¼‰
dy_pred_dz2 = sigmoid_derivative(z2)

# 3. âˆ‚z2/âˆ‚W2ï¼ˆz2 = W2 Â· a1ï¼‰
dz2_dW2 = a1  # å¯¹ W2[i] çš„åå¯¼æ•°æ˜¯ a1[i]

# ç»„åˆ
dLoss_dW2 = dLoss_dy_pred * dy_pred_dz2 * dz2_dW2
print(f"Gradient âˆ‚Loss/âˆ‚W2: {dLoss_dW2}")

# ğŸ”¥ è¿™å°±æ˜¯åå‘ä¼ æ’­ï¼
# PyTorch çš„ autograd è‡ªåŠ¨å¸®ä½ åšè¿™äº›è®¡ç®—
```

---

#### ğŸ”¥ PyTorch è‡ªåŠ¨å¾®åˆ†ç¤ºä¾‹

```python
import torch

# å®šä¹‰éœ€è¦æ¢¯åº¦çš„å˜é‡
x = torch.tensor([1.0, 2.0], requires_grad=True)
W1 = torch.tensor([[0.5, 0.3], [0.2, 0.8]], requires_grad=True)
W2 = torch.tensor([1.0, 0.5], requires_grad=True)

# å‰å‘ä¼ æ’­
z1 = W1 @ x
a1 = torch.sigmoid(z1)
z2 = W2 @ a1
y_pred = torch.sigmoid(z2)

# æŸå¤±
y_true = torch.tensor(1.0)
loss = (y_pred - y_true) ** 2

print(f"Loss: {loss.item():.4f}")

# ğŸ”¥ è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼
loss.backward()

print(f"Gradient âˆ‚Loss/âˆ‚W2: {W2.grad}")
print(f"Gradient âˆ‚Loss/âˆ‚W1: {W1.grad}")

# âœ… PyTorch è‡ªåŠ¨å®Œæˆäº†é“¾å¼æ³•åˆ™ï¼
```

---

### 2.4 æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ï¼šä¼˜åŒ–ç®—æ³•

#### ğŸ”¢ æ¢¯åº¦ä¸‹é™æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šæ²¿ç€æ¢¯åº¦çš„**åæ–¹å‘**ç§»åŠ¨ï¼Œæ‰¾åˆ°å‡½æ•°çš„æœ€å°å€¼ã€‚

```python
# ä¼˜åŒ–ç›®æ ‡ï¼šæ‰¾åˆ° f(x) = (x - 3)^2 çš„æœ€å°å€¼

def f(x):
    return (x - 3) ** 2

def f_derivative(x):
    return 2 * (x - 3)

# æ¢¯åº¦ä¸‹é™
x = 0.0  # åˆå§‹å€¼
learning_rate = 0.1  # å­¦ä¹ ç‡
history = [x]

for iteration in range(20):
    grad = f_derivative(x)
    x = x - learning_rate * grad  # ğŸ”¥ æ ¸å¿ƒå…¬å¼ï¼
    history.append(x)
    print(f"Iteration {iteration+1}: x={x:.4f}, f(x)={f(x):.4f}, grad={grad:.4f}")

# å¯è§†åŒ–
x_range = np.linspace(-1, 6, 100)
y_range = f(x_range)

plt.figure(figsize=(10, 6))
plt.plot(x_range, y_range, label='f(x) = (x-3)Â²')
plt.plot(history, [f(x) for x in history], 'ro-', label='Gradient Descent Path')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.title('Gradient Descent Optimization')
plt.grid()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼šx é€æ¸æ”¶æ•›åˆ° 3ï¼ˆæœ€å°å€¼ç‚¹ï¼‰
```

---

#### ğŸ”¥ å­¦ä¹ ç‡çš„å½±å“

```python
# å®éªŒï¼šä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ

learning_rates = [0.01, 0.1, 0.5, 0.9]
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for idx, lr in enumerate(learning_rates):
    x = 0.0
    history = [x]
    
    for _ in range(20):
        grad = f_derivative(x)
        x = x - lr * grad
        history.append(x)
    
    # ç»˜å›¾
    ax = axes[idx]
    x_range = np.linspace(-1, 6, 100)
    y_range = f(x_range)
    ax.plot(x_range, y_range, label='f(x)')
    ax.plot(history, [f(x) for x in history], 'ro-', markersize=4)
    ax.set_title(f'Learning Rate = {lr}')
    ax.set_xlabel('x')
    ax.set_ylabel('f(x)')
    ax.grid()
    ax.legend()

plt.tight_layout()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼š
# lr=0.01: æ”¶æ•›å¤ªæ…¢
# lr=0.1: åˆšåˆšå¥½
# lr=0.5: éœ‡è¡ä½†èƒ½æ”¶æ•›
# lr=0.9: éœ‡è¡å‰§çƒˆï¼Œç”šè‡³å¯èƒ½å‘æ•£ï¼
```

---

### ğŸ† å¾®ç§¯åˆ†å°ç»“

| æ¦‚å¿µ | ç¨‹åºå‘˜ç±»æ¯” | åœ¨ LLM ä¸­çš„ä½œç”¨ | é‡è¦æ€§ |
|-----|----------|--------------|-------|
| **å¯¼æ•°** | å‡½æ•°çš„"æ–œç‡" | å‚æ•°æ›´æ–°æ–¹å‘ | â­â­â­â­â­ |
| **åå¯¼æ•°** | å¤šå˜é‡å‡½æ•°çš„"æ–œç‡" | è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ | â­â­â­â­â­ |
| **æ¢¯åº¦** | åå¯¼æ•°ç»„æˆçš„å‘é‡ | ä¼˜åŒ–æ–¹å‘ | â­â­â­â­â­ |
| **é“¾å¼æ³•åˆ™** | å‡½æ•°è°ƒç”¨é“¾çš„æ±‚å¯¼ | åå‘ä¼ æ’­ç®—æ³• | â­â­â­â­â­ |
| **æ¢¯åº¦ä¸‹é™** | è¿­ä»£ä¼˜åŒ–ç®—æ³• | æ¨¡å‹è®­ç»ƒ | â­â­â­â­â­ |

**æ¨èå­¦ä¹ èµ„æº**ï¼š
- [3Blue1Brown - Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)ï¼ˆå¯è§†åŒ–è®²è§£ï¼‰
- [Backpropagation Calculus (colah's blog)](http://colah.github.io/posts/2015-08-Backprop/)ï¼ˆåå‘ä¼ æ’­æ·±åº¦è§£æï¼‰

---

## ä¸‰ã€æ¦‚ç‡ä¸ç»Ÿè®¡ï¼šç†è§£ä¸ç¡®å®šæ€§

> **ä¸ºä»€ä¹ˆé‡è¦**ï¼šLLM æ˜¯æ¦‚ç‡æ¨¡å‹ï¼Œè¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒã€‚ç†è§£æ¦‚ç‡ç»Ÿè®¡ï¼Œæ‰èƒ½ç†è§£æ¨¡å‹çš„è¡Œä¸ºã€è¯„ä¼°å’Œä¼˜åŒ–ã€‚

æ ¹æ® [Probability Statistics for LLM Evaluation 2025](https://openreview.net/forum?id=E8gYIrbP00)ï¼š

> 2025 å¹´ LLM è¯„ä¼°çš„ç»Ÿè®¡æŒ‘æˆ˜åŒ…æ‹¬ï¼šäººç±»æ ‡æ³¨çš„ä¸ç¡®å®šæ€§ã€ä¸€è‡´æ€§åº¦é‡çš„å±€é™æ€§ã€ä»¥åŠéœ€è¦æ›´å¥½çš„ç»Ÿè®¡æ¡†æ¶ï¼ˆå¦‚è´å¶æ–¯æ–¹æ³•ã€ä¿¡å™ªæ¯”åˆ†æï¼‰ã€‚

---

### 3.1 æ¦‚ç‡åˆ†å¸ƒï¼ˆProbability Distributionï¼‰

#### ğŸ”¢ æ¦‚ç‡åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šæ¦‚ç‡åˆ†å¸ƒå‘Šè¯‰ä½ "æ¯ä¸ªå¯èƒ½ç»“æœçš„å‘ç”Ÿæ¦‚ç‡"ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# æ¡ˆä¾‹ 1ï¼šæ·éª°å­ï¼ˆç¦»æ•£å‡åŒ€åˆ†å¸ƒï¼‰
outcomes = [1, 2, 3, 4, 5, 6]
probabilities = [1/6] * 6

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(outcomes, probabilities, color='skyblue', edgecolor='black')
plt.xlabel('Outcome')
plt.ylabel('Probability')
plt.title('Discrete Uniform Distribution (Dice)')
plt.ylim(0, 0.3)
plt.grid(axis='y')

# æ¡ˆä¾‹ 2ï¼šæ­£æ€åˆ†å¸ƒï¼ˆè¿ç»­ï¼‰
x = np.linspace(-4, 4, 1000)
mean, std = 0, 1
prob_density = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)

plt.subplot(1, 2, 2)
plt.plot(x, prob_density, color='green', linewidth=2)
plt.fill_between(x, prob_density, alpha=0.3, color='green')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.title('Normal Distribution (Î¼=0, Ïƒ=1)')
plt.grid()

plt.tight_layout()
plt.show()
```

---

#### ğŸ”¥ åœ¨ LLM ä¸­çš„åº”ç”¨ï¼šè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ

```python
# LLM çš„è¾“å‡ºï¼šä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ

vocab = ["apple", "banana", "cat", "dog", "the"]
logits = np.array([2.0, 1.5, 0.5, 0.3, 3.0])  # æ¨¡å‹è¾“å‡ºçš„"åˆ†æ•°"

# Softmaxï¼šå°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # æ•°å€¼ç¨³å®šæ€§æŠ€å·§
    return exp_x / exp_x.sum()

probs = softmax(logits)

plt.figure(figsize=(10, 6))
plt.bar(vocab, probs, color='coral', edgecolor='black')
plt.xlabel('Token')
plt.ylabel('Probability')
plt.title('LLM Output Distribution (Softmax)')
plt.ylim(0, 1)
for i, (word, prob) in enumerate(zip(vocab, probs)):
    plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center')
plt.grid(axis='y')
plt.show()

print("Probabilities:", probs)
print("Sum:", probs.sum())  # 1.0ï¼ˆæ¦‚ç‡å’Œä¸º 1ï¼‰

# ğŸ”¥ é‡‡æ ·ï¼šæ ¹æ®æ¦‚ç‡åˆ†å¸ƒé€‰æ‹© token
sampled_token = np.random.choice(vocab, p=probs)
print(f"Sampled token: {sampled_token}")
```

---

### 3.2 æœŸæœ›ï¼ˆExpectationï¼‰ä¸æ–¹å·®ï¼ˆVarianceï¼‰

#### ğŸ”¢ æœŸæœ›æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šæœŸæœ›å°±æ˜¯"åŠ æƒå¹³å‡"ï¼Œå‘Šè¯‰ä½ "å¹³å‡æ¥è¯´ä¼šå¾—åˆ°ä»€ä¹ˆç»“æœ"ã€‚

```python
# æ·éª°å­çš„æœŸæœ›
outcomes = np.array([1, 2, 3, 4, 5, 6])
probabilities = np.array([1/6] * 6)

expectation = np.sum(outcomes * probabilities)
print(f"Expectation (mean): {expectation}")  # 3.5

# æ¨¡æ‹ŸéªŒè¯
rolls = np.random.choice(outcomes, size=10000, p=probabilities)
print(f"Simulated mean: {rolls.mean():.2f}")  # æ¥è¿‘ 3.5
```

---

#### ğŸ”¢ æ–¹å·®æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¨‹åºå‘˜è§†è§’**ï¼šæ–¹å·®è¡¡é‡"ç»“æœçš„åˆ†æ•£ç¨‹åº¦"ã€‚

```python
# æ–¹å·®ï¼šVar(X) = E[(X - E[X])^2]
variance = np.sum(probabilities * (outcomes - expectation) ** 2)
std_dev = np.sqrt(variance)

print(f"Variance: {variance:.2f}")  # 2.92
print(f"Standard deviation: {std_dev:.2f}")  # 1.71

# æ¨¡æ‹ŸéªŒè¯
print(f"Simulated variance: {rolls.var():.2f}")  # æ¥è¿‘ 2.92
```

---

#### ğŸ”¥ åœ¨ LLM ä¸­çš„åº”ç”¨ï¼šé‡‡æ ·æ¸©åº¦ï¼ˆTemperatureï¼‰

```python
# Temperature æ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„"é™¡å³­ç¨‹åº¦"

logits = np.array([2.0, 1.5, 0.5, 0.3, 3.0])
vocab = ["apple", "banana", "cat", "dog", "the"]

def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    exp_x = np.exp(scaled_logits - np.max(scaled_logits))
    return exp_x / exp_x.sum()

temperatures = [0.1, 0.5, 1.0, 2.0]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for idx, temp in enumerate(temperatures):
    probs = softmax_with_temperature(logits, temp)
    
    ax = axes[idx]
    ax.bar(vocab, probs, color='skyblue', edgecolor='black')
    ax.set_title(f'Temperature = {temp}')
    ax.set_ylabel('Probability')
    ax.set_ylim(0, 1)
    ax.grid(axis='y')
    
    # æ˜¾ç¤ºæ¦‚ç‡å€¼
    for i, (word, prob) in enumerate(zip(vocab, probs)):
        ax.text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=9)
    
    # è®¡ç®—æ–¹å·®ï¼ˆè¡¡é‡åˆ†æ•£ç¨‹åº¦ï¼‰
    variance = np.var(probs)
    ax.text(0.5, 0.9, f'Variance: {variance:.4f}', transform=ax.transAxes, 
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼š
# temp=0.1: é«˜æ¦‚ç‡çš„ token æ¦‚ç‡æ¥è¿‘ 1ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰
# temp=2.0: æ¦‚ç‡æ›´å‡åŒ€ï¼ˆéšæœºæ€§æ›´é«˜ï¼‰
```

---

### 3.3 ä¿¡æ¯è®ºåŸºç¡€ï¼šäº¤å‰ç†µä¸ KL æ•£åº¦

#### ğŸ”¢ ç†µï¼ˆEntropyï¼‰ï¼šä¿¡æ¯çš„"ä¸ç¡®å®šæ€§"

```python
# ç†µï¼šH(P) = -Î£ p(x) log p(x)

def entropy(probs):
    # é¿å… log(0)
    probs = probs[probs > 0]
    return -np.sum(probs * np.log2(probs))

# æ¡ˆä¾‹ 1ï¼šç¡®å®šæ€§åˆ†å¸ƒ
probs_certain = np.array([1.0, 0.0, 0.0])
print(f"Entropy (certain): {entropy(probs_certain):.2f} bits")  # 0.0

# æ¡ˆä¾‹ 2ï¼šå‡åŒ€åˆ†å¸ƒ
probs_uniform = np.array([1/3, 1/3, 1/3])
print(f"Entropy (uniform): {entropy(probs_uniform):.2f} bits")  # 1.58

# ğŸ”¥ ç†µè¶Šé«˜ â†’ ä¸ç¡®å®šæ€§è¶Šå¤§ â†’ éœ€è¦æ›´å¤šä¿¡æ¯æ¥æè¿°
```

---

#### ğŸ”¥ äº¤å‰ç†µï¼ˆCross-Entropyï¼‰ï¼šæ¨¡å‹æŸå¤±å‡½æ•°

```python
# äº¤å‰ç†µï¼šH(P, Q) = -Î£ p(x) log q(x)
# P: çœŸå®åˆ†å¸ƒï¼ŒQ: é¢„æµ‹åˆ†å¸ƒ

def cross_entropy(true_probs, pred_probs):
    # é¿å… log(0)
    pred_probs = np.clip(pred_probs, 1e-10, 1.0)
    return -np.sum(true_probs * np.log(pred_probs))

# æ¡ˆä¾‹ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
true_label = 2  # çœŸå® token æ˜¯ "cat"ï¼ˆç´¢å¼• 2ï¼‰
vocab_size = 5

# One-hot ç¼–ç 
true_probs = np.zeros(vocab_size)
true_probs[true_label] = 1.0

# æ¨¡å‹é¢„æµ‹
pred_probs = np.array([0.1, 0.2, 0.5, 0.1, 0.1])

loss = cross_entropy(true_probs, pred_probs)
print(f"Cross-Entropy Loss: {loss:.4f}")

# éªŒè¯ï¼šPyTorch å®ç°
import torch
import torch.nn.functional as F

logits = torch.tensor([1.0, 2.0, 3.0, 1.0, 1.0])
target = torch.tensor(2)

loss_pytorch = F.cross_entropy(logits, target)
print(f"PyTorch Cross-Entropy: {loss_pytorch.item():.4f}")

# ğŸ”¥ äº¤å‰ç†µè¶Šå° â†’ é¢„æµ‹è¶Šå‡†ç¡®
```

---

#### ğŸ”¥ KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰ï¼šåˆ†å¸ƒå·®å¼‚

```python
# KL æ•£åº¦ï¼šD_KL(P || Q) = Î£ p(x) log(p(x) / q(x))
# è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„"å·®å¼‚"

def kl_divergence(p, q):
    p = np.clip(p, 1e-10, 1.0)
    q = np.clip(q, 1e-10, 1.0)
    return np.sum(p * np.log(p / q))

# ä¸¤ä¸ªåˆ†å¸ƒ
P = np.array([0.3, 0.3, 0.4])
Q1 = np.array([0.3, 0.3, 0.4])  # å®Œå…¨ç›¸åŒ
Q2 = np.array([0.1, 0.1, 0.8])  # ä¸åŒ

print(f"KL(P || Q1): {kl_divergence(P, Q1):.4f}")  # 0.0ï¼ˆç›¸åŒï¼‰
print(f"KL(P || Q2): {kl_divergence(P, Q2):.4f}")  # 0.5ï¼ˆä¸åŒï¼‰

# ğŸ”¥ KL æ•£åº¦åœ¨ DPOï¼ˆDirect Preference Optimizationï¼‰ä¸­ç”¨äºçº¦æŸæ¨¡å‹ä¸è¦åç¦»å¤ªè¿œ
```

---

### 3.4 è´å¶æ–¯å®šç†ï¼ˆBayes' Theoremï¼‰

#### ğŸ”¢ è´å¶æ–¯å®šç†æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ¸å¿ƒæ€æƒ³**ï¼šæ ¹æ®æ–°è¯æ®æ›´æ–°æˆ‘ä»¬çš„ä¿¡å¿µã€‚

```
P(A|B) = P(B|A) Ã— P(A) / P(B)

å…¶ä¸­ï¼š
- P(A|B): åéªŒæ¦‚ç‡ï¼ˆç»™å®š Bï¼ŒA çš„æ¦‚ç‡ï¼‰
- P(B|A): ä¼¼ç„¶ï¼ˆç»™å®š Aï¼ŒB çš„æ¦‚ç‡ï¼‰
- P(A): å…ˆéªŒæ¦‚ç‡
- P(B): è¯æ®æ¦‚ç‡
```

**æ¡ˆä¾‹ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹**

```python
# å·²çŸ¥ï¼š
# P(Spam) = 0.3ï¼ˆ30% çš„é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶ï¼‰
# P("free" | Spam) = 0.8ï¼ˆåƒåœ¾é‚®ä»¶ä¸­ 80% åŒ…å« "free"ï¼‰
# P("free" | Not Spam) = 0.1ï¼ˆæ­£å¸¸é‚®ä»¶ä¸­ 10% åŒ…å« "free"ï¼‰

# é—®é¢˜ï¼šæ”¶åˆ°åŒ…å« "free" çš„é‚®ä»¶ï¼Œå®ƒæ˜¯åƒåœ¾é‚®ä»¶çš„æ¦‚ç‡ï¼Ÿ

P_spam = 0.3
P_not_spam = 0.7
P_free_given_spam = 0.8
P_free_given_not_spam = 0.1

# P(free) = P(free|spam) Ã— P(spam) + P(free|not_spam) Ã— P(not_spam)
P_free = P_free_given_spam * P_spam + P_free_given_not_spam * P_not_spam

# P(spam | free) = P(free | spam) Ã— P(spam) / P(free)
P_spam_given_free = (P_free_given_spam * P_spam) / P_free

print(f"P(Spam | 'free'): {P_spam_given_free:.4f}")  # 0.7742

# ğŸ”¥ ç»“è®ºï¼šåŒ…å« "free" çš„é‚®ä»¶æœ‰ 77% æ¦‚ç‡æ˜¯åƒåœ¾é‚®ä»¶ï¼
```

---

### 3.5 è¯„ä¼°æŒ‡æ ‡çš„ç»Ÿè®¡ç†è§£

æ ¹æ® [Beyond Correlation: LLM Evaluation 2025](https://openreview.net/forum?id=E8gYIrbP00)ï¼Œ2025 å¹´ LLM è¯„ä¼°é¢ä¸´çš„ç»Ÿè®¡æŒ‘æˆ˜åŒ…æ‹¬ï¼š

#### ğŸ”¥ ç½®ä¿¡åŒºé—´ï¼ˆConfidence Intervalï¼‰

```python
from scipy import stats

# æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡
num_samples = 100
num_correct = 85
accuracy = num_correct / num_samples

# è®¡ç®— 95% ç½®ä¿¡åŒºé—´
confidence_level = 0.95
margin_of_error = stats.norm.ppf((1 + confidence_level) / 2) * np.sqrt(accuracy * (1 - accuracy) / num_samples)

ci_lower = accuracy - margin_of_error
ci_upper = accuracy + margin_of_error

print(f"Accuracy: {accuracy:.2%}")
print(f"95% Confidence Interval: [{ci_lower:.2%}, {ci_upper:.2%}]")

# ğŸ”¥ æ­£ç¡®è§£è¯»ï¼š
# "æˆ‘ä»¬æœ‰ 95% çš„ä¿¡å¿ƒï¼ŒçœŸå®å‡†ç¡®ç‡åœ¨ 78% - 92% ä¹‹é—´"
# è€Œä¸æ˜¯"å‡†ç¡®ç‡å°±æ˜¯ 85%"
```

---

#### ğŸ”¥ A/B æµ‹è¯•çš„ç»Ÿè®¡æ˜¾è‘—æ€§

```python
from scipy.stats import ttest_ind

# æ¨¡å‹ A å’Œæ¨¡å‹ B åœ¨å¤šæ¬¡è¿è¡Œä¸­çš„å¾—åˆ†
scores_A = np.random.normal(0.80, 0.05, 30)  # å‡å€¼ 0.80ï¼Œæ ‡å‡†å·® 0.05
scores_B = np.random.normal(0.82, 0.05, 30)  # å‡å€¼ 0.82

# t-test
t_statistic, p_value = ttest_ind(scores_A, scores_B)

print(f"Model A mean: {scores_A.mean():.4f}")
print(f"Model B mean: {scores_B.mean():.4f}")
print(f"t-statistic: {t_statistic:.4f}")
print(f"p-value: {p_value:.4f}")

if p_value < 0.05:
    print("âœ… ç»“æœå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆæ¨¡å‹ B æ˜¾è‘—æ›´å¥½ï¼‰")
else:
    print("âŒ ç»“æœä¸å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆå¯èƒ½åªæ˜¯éšæœºæ³¢åŠ¨ï¼‰")

# ğŸ”¥ å…³é”®ï¼šä¸è¦åªçœ‹å‡å€¼ï¼Œè¦çœ‹ç»Ÿè®¡æ˜¾è‘—æ€§ï¼
```

---

### ğŸ† æ¦‚ç‡ç»Ÿè®¡å°ç»“

| æ¦‚å¿µ | ç¨‹åºå‘˜ç±»æ¯” | åœ¨ LLM ä¸­çš„ä½œç”¨ | é‡è¦æ€§ |
|-----|----------|--------------|-------|
| **æ¦‚ç‡åˆ†å¸ƒ** | æ¯ä¸ªç»“æœçš„å‘ç”Ÿæ¦‚ç‡ | è¾“å‡º token çš„æ¦‚ç‡ | â­â­â­â­â­ |
| **æœŸæœ›** | åŠ æƒå¹³å‡ | æ¨¡å‹é¢„æµ‹çš„"å¹³å‡"è¡Œä¸º | â­â­â­â­ |
| **æ–¹å·®** | åˆ†æ•£ç¨‹åº¦ | Temperature æ§åˆ¶ | â­â­â­â­ |
| **ç†µ** | ä¸ç¡®å®šæ€§ | ä¿¡æ¯é‡åº¦é‡ | â­â­â­ |
| **äº¤å‰ç†µ** | åˆ†å¸ƒå·®å¼‚ | è®­ç»ƒæŸå¤±å‡½æ•° | â­â­â­â­â­ |
| **KL æ•£åº¦** | åˆ†å¸ƒç›¸ä¼¼åº¦ | DPO å¯¹é½çº¦æŸ | â­â­â­â­ |
| **è´å¶æ–¯å®šç†** | æ ¹æ®è¯æ®æ›´æ–°ä¿¡å¿µ | åéªŒæ¨ç† | â­â­â­ |
| **ç½®ä¿¡åŒºé—´** | ç»“æœçš„ä¸ç¡®å®šæ€§èŒƒå›´ | è¯„ä¼°å¯é æ€§ | â­â­â­â­ |

**æ¨èå­¦ä¹ èµ„æº**ï¼š
- [StatQuest - Statistics Fundamentals](https://www.youtube.com/c/joshstarmer)ï¼ˆç”¨ç®€å•ä¾‹å­è®²ç»Ÿè®¡ï¼‰
- [Probability Statistics for LLM Evaluation 2025](https://openreview.net/forum?id=E8gYIrbP00)ï¼ˆæœ€æ–°ç ”ç©¶ï¼‰

---

## å››ã€ä¼˜åŒ–ç†è®ºï¼šè®­ç»ƒçš„è‰ºæœ¯

> **ä¸ºä»€ä¹ˆé‡è¦**ï¼šè®­ç»ƒ LLM æœ¬è´¨ä¸Šæ˜¯ä¼˜åŒ–é—®é¢˜â€”â€”åœ¨æ•°åäº¿å‚æ•°çš„ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä¼˜è§£ã€‚ç†è§£ä¼˜åŒ–ç†è®ºï¼Œæ‰èƒ½è°ƒå¥½è¶…å‚æ•°ã€åŠ é€Ÿè®­ç»ƒã€‚

---

### 4.1 æ¢¯åº¦ä¸‹é™çš„å˜ä½“

#### ğŸ”¥ SGDï¼ˆStochastic Gradient Descentï¼‰

```python
import numpy as np

# ç”Ÿæˆæ•°æ®ï¼šy = 2x + 1 + noise
np.random.seed(42)
X = np.random.randn(1000, 1)
y = 2 * X + 1 + 0.1 * np.random.randn(1000, 1)

# å‚æ•°åˆå§‹åŒ–
w = np.random.randn(1, 1)
b = np.random.randn(1, 1)
learning_rate = 0.01
batch_size = 32

losses = []

for epoch in range(100):
    # Shuffle data
    indices = np.random.permutation(len(X))
    X_shuffled = X[indices]
    y_shuffled = y[indices]
    
    epoch_loss = 0
    
    # Mini-batch SGD
    for i in range(0, len(X), batch_size):
        X_batch = X_shuffled[i:i+batch_size]
        y_batch = y_shuffled[i:i+batch_size]
        
        # Forward
        y_pred = X_batch @ w + b
        loss = np.mean((y_pred - y_batch) ** 2)
        epoch_loss += loss
        
        # Backward
        dw = 2 * X_batch.T @ (y_pred - y_batch) / batch_size
        db = 2 * np.mean(y_pred - y_batch)
        
        # Update
        w = w - learning_rate * dw
        b = b - learning_rate * db
    
    losses.append(epoch_loss / (len(X) // batch_size))

print(f"Final w: {w[0, 0]:.4f} (true: 2.0)")
print(f"Final b: {b[0, 0]:.4f} (true: 1.0)")

# å¯è§†åŒ–
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss (Mini-batch SGD)')
plt.grid()
plt.show()
```

---

#### ğŸ”¥ Momentumï¼šåŠ é€Ÿæ”¶æ•›

```python
# Momentum SGD
w = np.random.randn(1, 1)
b = np.random.randn(1, 1)
v_w = 0  # velocity for w
v_b = 0  # velocity for b
beta = 0.9  # momentum coefficient

losses_momentum = []

for epoch in range(100):
    indices = np.random.permutation(len(X))
    X_shuffled = X[indices]
    y_shuffled = y[indices]
    
    epoch_loss = 0
    
    for i in range(0, len(X), batch_size):
        X_batch = X_shuffled[i:i+batch_size]
        y_batch = y_shuffled[i:i+batch_size]
        
        y_pred = X_batch @ w + b
        loss = np.mean((y_pred - y_batch) ** 2)
        epoch_loss += loss
        
        dw = 2 * X_batch.T @ (y_pred - y_batch) / batch_size
        db = 2 * np.mean(y_pred - y_batch)
        
        # ğŸ”¥ Momentum update
        v_w = beta * v_w + (1 - beta) * dw
        v_b = beta * v_b + (1 - beta) * db
        
        w = w - learning_rate * v_w
        b = b - learning_rate * v_b
    
    losses_momentum.append(epoch_loss / (len(X) // batch_size))

# å¯¹æ¯”
plt.plot(losses, label='SGD')
plt.plot(losses_momentum, label='Momentum SGD')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('SGD vs Momentum')
plt.legend()
plt.grid()
plt.show()

# ğŸ”¥ Momentum æ”¶æ•›æ›´å¿«ã€æ›´ç¨³å®šï¼
```

---

#### ğŸ”¥ Adamï¼šè‡ªé€‚åº”å­¦ä¹ ç‡

```python
# Adam Optimizer (æœ€å¸¸ç”¨ï¼)
w = np.random.randn(1, 1)
b = np.random.randn(1, 1)

# Adam å‚æ•°
m_w, v_w = 0, 0  # ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©
m_b, v_b = 0, 0
beta1, beta2 = 0.9, 0.999
epsilon = 1e-8

losses_adam = []

for epoch in range(100):
    indices = np.random.permutation(len(X))
    X_shuffled = X[indices]
    y_shuffled = y[indices]
    
    epoch_loss = 0
    
    for i in range(0, len(X), batch_size):
        t = epoch * (len(X) // batch_size) + i // batch_size + 1  # time step
        
        X_batch = X_shuffled[i:i+batch_size]
        y_batch = y_shuffled[i:i+batch_size]
        
        y_pred = X_batch @ w + b
        loss = np.mean((y_pred - y_batch) ** 2)
        epoch_loss += loss
        
        dw = 2 * X_batch.T @ (y_pred - y_batch) / batch_size
        db = 2 * np.mean(y_pred - y_batch)
        
        # ğŸ”¥ Adam update
        # ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„å‡å€¼ï¼‰
        m_w = beta1 * m_w + (1 - beta1) * dw
        m_b = beta1 * m_b + (1 - beta1) * db
        
        # äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦å¹³æ–¹çš„å‡å€¼ï¼‰
        v_w = beta2 * v_w + (1 - beta2) * (dw ** 2)
        v_b = beta2 * v_b + (1 - beta2) * (db ** 2)
        
        # åå·®ä¿®æ­£
        m_w_hat = m_w / (1 - beta1 ** t)
        m_b_hat = m_b / (1 - beta1 ** t)
        v_w_hat = v_w / (1 - beta2 ** t)
        v_b_hat = v_b / (1 - beta2 ** t)
        
        # æ›´æ–°
        w = w - learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        b = b - learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)
    
    losses_adam.append(epoch_loss / (len(X) // batch_size))

# ä¸‰ç§æ–¹æ³•å¯¹æ¯”
plt.plot(losses, label='SGD', alpha=0.7)
plt.plot(losses_momentum, label='Momentum', alpha=0.7)
plt.plot(losses_adam, label='Adam', alpha=0.7)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Optimizer Comparison')
plt.legend()
plt.grid()
plt.show()

# ğŸ”¥ Adam é€šå¸¸æ˜¯æœ€ä½³é€‰æ‹©ï¼ˆæ”¶æ•›å¿«ä¸”ç¨³å®šï¼‰
```

---

### 4.2 å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰

#### ğŸ”¥ å¸¸è§ç­–ç•¥

```python
import matplotlib.pyplot as plt

epochs = 100
initial_lr = 0.1

# 1. Constant
lr_constant = [initial_lr] * epochs

# 2. Step Decay
lr_step = []
for epoch in range(epochs):
    lr = initial_lr * (0.5 ** (epoch // 30))
    lr_step.append(lr)

# 3. Exponential Decay
lr_exp = [initial_lr * (0.95 ** epoch) for epoch in range(epochs)]

# 4. Cosine Annealing
lr_cosine = [initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs)) for epoch in range(epochs)]

# 5. Warmup + Cosine
warmup_epochs = 10
lr_warmup_cosine = []
for epoch in range(epochs):
    if epoch < warmup_epochs:
        lr = initial_lr * (epoch + 1) / warmup_epochs
    else:
        progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
        lr = initial_lr * 0.5 * (1 + np.cos(np.pi * progress))
    lr_warmup_cosine.append(lr)

# å¯è§†åŒ–
plt.figure(figsize=(14, 8))

strategies = [
    ('Constant', lr_constant),
    ('Step Decay', lr_step),
    ('Exponential Decay', lr_exp),
    ('Cosine Annealing', lr_cosine),
    ('Warmup + Cosine', lr_warmup_cosine)
]

for i, (name, lr_schedule) in enumerate(strategies, 1):
    plt.subplot(2, 3, i)
    plt.plot(lr_schedule, linewidth=2)
    plt.title(name)
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.grid()

plt.tight_layout()
plt.show()

# ğŸ”¥ 2025 å¹´ LLM è®­ç»ƒä¸»æµï¼šWarmup + Cosine Annealing
```

---

### 4.3 å‡¸ä¼˜åŒ– vs. éå‡¸ä¼˜åŒ–

#### ğŸ”¢ å‡¸å‡½æ•°ï¼ˆConvex Functionï¼‰

```python
# å‡¸å‡½æ•°ï¼šåªæœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼
x = np.linspace(-5, 5, 100)
y_convex = x ** 2  # f(x) = x^2

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x, y_convex, linewidth=2)
plt.scatter([0], [0], color='red', s=100, zorder=5, label='Global Minimum')
plt.title('Convex Function: f(x) = xÂ²')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()
plt.legend()

# éå‡¸å‡½æ•°ï¼šæœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼
y_nonconvex = x ** 4 - 3 * x ** 3 + 2 * x ** 2 + x

plt.subplot(1, 2, 2)
plt.plot(x, y_nonconvex, linewidth=2)
plt.title('Non-Convex Function (Neural Networks)')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid()

plt.tight_layout()
plt.show()

# ğŸ”¥ æ·±åº¦å­¦ä¹ çš„æŸå¤±å‡½æ•°æ˜¯éå‡¸çš„ï¼
# â†’ å¯èƒ½é™·å…¥å±€éƒ¨æœ€å°å€¼
# â†’ éœ€è¦å¥½çš„åˆå§‹åŒ–ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦
```

---

### ğŸ† ä¼˜åŒ–ç†è®ºå°ç»“

| æ¦‚å¿µ | æ ¸å¿ƒæ€æƒ³ | åœ¨ LLM ä¸­çš„åº”ç”¨ | é‡è¦æ€§ |
|-----|---------|--------------|-------|
| **SGD** | éšæœºæ¢¯åº¦ä¸‹é™ | åŸºç¡€ä¼˜åŒ–ç®—æ³• | â­â­â­â­ |
| **Momentum** | åˆ©ç”¨å†å²æ¢¯åº¦åŠ é€Ÿ | æ›´å¿«æ”¶æ•› | â­â­â­â­ |
| **Adam** | è‡ªé€‚åº”å­¦ä¹ ç‡ | **æœ€å¸¸ç”¨ä¼˜åŒ–å™¨** | â­â­â­â­â­ |
| **Learning Rate Schedule** | åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ | æå‡è®­ç»ƒç¨³å®šæ€§ | â­â­â­â­â­ |
| **Warmup** | è®­ç»ƒåˆæœŸå°å­¦ä¹ ç‡ | é¿å…ä¸ç¨³å®š | â­â­â­â­ |

**æ¨èå­¦ä¹ èµ„æº**ï¼š
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)ï¼ˆä¼˜åŒ–å™¨ç»¼è¿°ï¼‰
- [CS231n - Neural Networks Part 3: Learning and Evaluation](https://cs231n.github.io/neural-networks-3/)

---

## äº”ã€ç»¼åˆå®æˆ˜ï¼šç”¨æ•°å­¦ç†è§£ Attention

> **ç›®æ ‡**ï¼šæŠŠå‰é¢å­¦çš„çº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†ã€æ¦‚ç‡ç»Ÿè®¡ç»¼åˆèµ·æ¥ï¼Œå®Œæ•´ç†è§£ Attention æœºåˆ¶çš„æ•°å­¦åŸç†ã€‚

---

### 5.1 Attention çš„æ•°å­¦è¡¨è¾¾

```python
import torch
import torch.nn.functional as F

# è¾“å…¥ï¼š3 ä¸ªè¯ï¼Œæ¯ä¸ª 4 ç»´
X = torch.tensor([
    [1.0, 0.5, 0.2, 0.1],  # word 1: "the"
    [0.3, 0.8, 0.1, 0.4],  # word 2: "cat"
    [0.2, 0.1, 0.9, 0.3]   # word 3: "sat"
])  # shape: (3, 4)

seq_len, d_model = X.shape
d_k = 2  # Query/Key çš„ç»´åº¦

# æƒé‡çŸ©é˜µï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
torch.manual_seed(42)
W_Q = torch.randn(d_model, d_k)  # (4, 2)
W_K = torch.randn(d_model, d_k)  # (4, 2)
W_V = torch.randn(d_model, d_k)  # (4, 2)

# ğŸ”¥ æ­¥éª¤ 1ï¼šçº¿æ€§æŠ•å½±ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
Q = X @ W_Q  # (3, 4) @ (4, 2) = (3, 2)
K = X @ W_K  # (3, 2)
V = X @ W_V  # (3, 2)

print("Query shape:", Q.shape)
print("Key shape:", K.shape)
print("Value shape:", V.shape)

# ğŸ”¥ æ­¥éª¤ 2ï¼šè®¡ç®— Attention Scoresï¼ˆç‚¹ç§¯ï¼‰
scores = Q @ K.T  # (3, 2) @ (2, 3) = (3, 3)
print("\nAttention Scores:")
print(scores)

# è§£é‡Šï¼šscores[i, j] = Q[i] Â· K[j]
# â†’ è¯ i å¯¹è¯ j çš„"å…³æ³¨ç¨‹åº¦"

# ğŸ”¥ æ­¥éª¤ 3ï¼šç¼©æ”¾ï¼ˆé¿å…æ•°å€¼è¿‡å¤§ï¼‰
scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
print("\nScaled Scores:")
print(scores)

# ğŸ”¥ æ­¥éª¤ 4ï¼šSoftmaxï¼ˆè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼‰
attention_weights = F.softmax(scores, dim=-1)
print("\nAttention Weights (probabilities):")
print(attention_weights)

# éªŒè¯ï¼šæ¯ä¸€è¡Œçš„å’Œä¸º 1
print("\nRow sums:", attention_weights.sum(dim=-1))  # [1., 1., 1.]

# ğŸ”¥ æ­¥éª¤ 5ï¼šåŠ æƒæ±‚å’Œï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
output = attention_weights @ V  # (3, 3) @ (3, 2) = (3, 2)
print("\nOutput shape:", output.shape)
print("Output:")
print(output)
```

---

### 5.2 å¯è§†åŒ– Attention

```python
import matplotlib.pyplot as plt
import seaborn as sns

# å¯è§†åŒ– Attention Weights
words = ["the", "cat", "sat"]

plt.figure(figsize=(8, 6))
sns.heatmap(attention_weights.numpy(), annot=True, fmt='.3f', 
            xticklabels=words, yticklabels=words, cmap='YlOrRd')
plt.title('Attention Weights')
plt.xlabel('Key (attending to)')
plt.ylabel('Query (current word)')
plt.show()

# ğŸ”¥ è§£è¯»ï¼š
# attention_weights[0, 1] = 0.345
# â†’ å½“å¤„ç† "the" æ—¶ï¼Œæ¨¡å‹ç»™ "cat" åˆ†é…äº† 34.5% çš„æ³¨æ„åŠ›
```

---

### 5.3 Multi-Head Attention

```python
# Multi-Head Attentionï¼šå¹¶è¡Œè¿è¡Œå¤šä¸ª Attention

num_heads = 2
d_k_per_head = d_k // num_heads  # æ¯ä¸ª head çš„ç»´åº¦

def split_heads(x, num_heads):
    """å°†æœ€åä¸€ç»´åˆ†å‰²ä¸ºå¤šä¸ª heads"""
    batch_size, seq_len, d_model = x.shape
    x = x.view(batch_size, seq_len, num_heads, d_model // num_heads)
    return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k_per_head)

# æ‰¹é‡åŒ–ï¼šæ·»åŠ  batch ç»´åº¦
X_batch = X.unsqueeze(0)  # (1, 3, 4)

# æŠ•å½±
Q_batch = (X_batch @ W_Q).unsqueeze(0)  # (1, 3, 2)
K_batch = (X_batch @ W_K).unsqueeze(0)
V_batch = (X_batch @ W_V).unsqueeze(0)

# åˆ†å‰² heads
Q_heads = split_heads(Q_batch, num_heads)  # (1, 2, 3, 1)
K_heads = split_heads(K_batch, num_heads)
V_heads = split_heads(V_batch, num_heads)

print("Q_heads shape:", Q_heads.shape)

# æ¯ä¸ª head ç‹¬ç«‹è®¡ç®— Attention
# ... (çœç•¥å…·ä½“å®ç°)

# ğŸ”¥ Multi-Head çš„ä¼˜åŠ¿ï¼š
# - ä¸åŒ head å¯ä»¥å…³æ³¨ä¸åŒçš„è¯­ä¹‰ç‰¹å¾
# - å¹¶è¡Œè®¡ç®—ï¼Œæé«˜æ•ˆç‡
```

---

### ğŸ† Attention æ•°å­¦å°ç»“

| æ­¥éª¤ | æ•°å­¦æ“ä½œ | ç”¨åˆ°çš„æ•°å­¦çŸ¥è¯† |
|-----|---------|-------------|
| **1. çº¿æ€§æŠ•å½±** | Q = X @ W_Q | çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µä¹˜æ³• |
| **2. Attention Score** | scores = Q @ K^T | çº¿æ€§ä»£æ•°ï¼šç‚¹ç§¯ã€çŸ©é˜µä¹˜æ³• |
| **3. ç¼©æ”¾** | scores / âˆšd_k | å¾®ç§¯åˆ†ï¼šç¨³å®šæ¢¯åº¦ |
| **4. Softmax** | softmax(scores) | æ¦‚ç‡ç»Ÿè®¡ï¼šæ¦‚ç‡åˆ†å¸ƒ |
| **5. åŠ æƒæ±‚å’Œ** | output = weights @ V | çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µä¹˜æ³•ï¼›ç»Ÿè®¡ï¼šæœŸæœ› |

---

## å…­ã€å­¦ä¹ è·¯å¾„ä¸èµ„æºæ¨è

### ğŸ“š åˆ†é˜¶æ®µå­¦ä¹ å»ºè®®

#### é˜¶æ®µ 1ï¼šåŸºç¡€è¡¥å……ï¼ˆè¾¹å­¦è¾¹ç”¨ï¼‰

**ä¸è¦ä¸€æ¬¡æ€§å­¦å®Œæ‰€æœ‰æ•°å­¦ï¼æŒ‰éœ€å­¦ä¹ ï¼š**

```
é‡åˆ°çŸ©é˜µä¹˜æ³•ä¸æ‡‚ â†’ å­¦çº¿æ€§ä»£æ•°çš„çŸ©é˜µéƒ¨åˆ†ï¼ˆ3 å°æ—¶ï¼‰
é‡åˆ°æ¢¯åº¦ä¸æ‡‚ â†’ å­¦å¾®ç§¯åˆ†çš„å¯¼æ•°éƒ¨åˆ†ï¼ˆ2 å°æ—¶ï¼‰
é‡åˆ° Softmax ä¸æ‡‚ â†’ å­¦æ¦‚ç‡åˆ†å¸ƒï¼ˆ1 å°æ—¶ï¼‰
```

**æ¨èèµ„æº**ï¼š
- [3Blue1Brown - æ•°å­¦å¯è§†åŒ–ç³»åˆ—](https://www.youtube.com/c/3blue1brown)ï¼ˆçº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†ï¼‰
- [StatQuest - ç»Ÿè®¡åŸºç¡€](https://www.youtube.com/c/joshstarmer)

---

#### é˜¶æ®µ 2ï¼šç³»ç»Ÿå­¦ä¹ ï¼ˆå¯é€‰ï¼‰

**å¦‚æœä½ æƒ³ç³»ç»Ÿåœ°å­¦æ•°å­¦**ï¼š

| èµ„æº | ç±»å‹ | ç‰¹ç‚¹ | é“¾æ¥ |
|-----|------|------|------|
| **Mathematics for Machine Learning** | ä¹¦ç± | å…è´¹åœ¨çº¿ï¼Œå·¥ç¨‹å¯¼å‘ | [mml-book.github.io](https://mml-book.github.io/) |
| **Mathematics for ML Specialization (Coursera)** | åœ¨çº¿è¯¾ç¨‹ | é…åˆç»ƒä¹ ï¼Œé€‚åˆç³»ç»Ÿå­¦ä¹  | [Coursera](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science) |
| **MIT 18.06 Linear Algebra** | å¤§å­¦è¯¾ç¨‹ | ç†è®ºä¸¥è°¨ï¼Œæ·±å…¥ | [MIT OCW](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/) |

---

### ğŸ› ï¸ å®è·µé¡¹ç›®

**è¾¹åšè¾¹å­¦æ•°å­¦**ï¼š

1. **å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼ˆä¸ç”¨ PyTorchï¼‰**
   - å­¦åˆ°ï¼šçŸ©é˜µä¹˜æ³•ã€æ¢¯åº¦ä¸‹é™ã€åå‘ä¼ æ’­
   
2. **å®ç° Attention æœºåˆ¶**
   - å­¦åˆ°ï¼šç‚¹ç§¯ã€Softmaxã€åŠ æƒæ±‚å’Œ
   
3. **è°ƒå‚å®éªŒ**
   - å­¦åˆ°ï¼šå­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ã€æ¦‚ç‡åˆ†å¸ƒ

---

### ğŸ“– æ•°å­¦ç¬¦å·é€ŸæŸ¥è¡¨

| ç¬¦å· | è¯»æ³• | å«ä¹‰ | ç¤ºä¾‹ |
|-----|------|------|------|
| âˆ‚ | åå¯¼æ•° | åå¯¼æ•°ç¬¦å· | âˆ‚f/âˆ‚x |
| âˆ‡ | nabla / del | æ¢¯åº¦ | âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y] |
| Î£ | sigma | æ±‚å’Œ | Î£ x_i = x_1 + x_2 + ... |
| Î  | pi | è¿ä¹˜ | Î  x_i = x_1 Ã— x_2 Ã— ... |
| âˆˆ | epsilon | å±äº | x âˆˆ Rï¼ˆx å±äºå®æ•°é›†ï¼‰ |
| âŠ— | tensor product | å¼ é‡ç§¯ | A âŠ— B |
| âŠ™ | element-wise product | é€å…ƒç´ ä¹˜æ³• | A âŠ™ B |
| â€–Â·â€– | norm | èŒƒæ•° | â€–xâ€–_2ï¼ˆL2 èŒƒæ•°ï¼‰ |
| E[Â·] | expectation | æœŸæœ› | E[X] |
| P(Â·) | probability | æ¦‚ç‡ | P(A) |

---

## ä¸ƒã€æ€»ç»“ä¸è¡ŒåŠ¨å»ºè®®

### ğŸ¯ æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **ä½ éœ€è¦çš„æ•°å­¦æ¯”æƒ³è±¡ä¸­å°‘å¾—å¤š**
   - ä¸éœ€è¦æ•°å­¦åšå£«æ°´å¹³
   - å·¥ç¨‹æ•°å­¦ â‰  ç†è®ºæ•°å­¦
   - å¤Ÿç”¨å°±å¥½ï¼ŒæŒ‰éœ€å­¦ä¹ 

2. **ä»ä»£ç åˆ°æ•°å­¦ï¼Œè€Œéä»å…¬å¼åˆ°ä»£ç **
   - å…ˆè·‘ä»£ç ï¼Œå»ºç«‹ç›´è§‰
   - å†çœ‹å…¬å¼ï¼Œç†è§£åŸç†
   - ç”¨å¯è§†åŒ–å¸®åŠ©ç†è§£

3. **å››å¤§æ ¸å¿ƒæ•°å­¦é¢†åŸŸ**
   - çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µã€å‘é‡ã€ç‚¹ç§¯ï¼ˆâ­â­â­â­â­ï¼‰
   - å¾®ç§¯åˆ†ï¼šå¯¼æ•°ã€æ¢¯åº¦ã€é“¾å¼æ³•åˆ™ï¼ˆâ­â­â­â­â­ï¼‰
   - æ¦‚ç‡ç»Ÿè®¡ï¼šåˆ†å¸ƒã€æœŸæœ›ã€äº¤å‰ç†µï¼ˆâ­â­â­â­â­ï¼‰
   - ä¼˜åŒ–ç†è®ºï¼šæ¢¯åº¦ä¸‹é™ã€Adamï¼ˆâ­â­â­â­ï¼‰

4. **æ•°å­¦åœ¨ LLM ä¸­çš„å…·ä½“åº”ç”¨**
   - Attention çš„æ•°å­¦åŸç†
   - æŸå¤±å‡½æ•°çš„è®¾è®¡
   - ä¼˜åŒ–å™¨çš„é€‰æ‹©
   - è¯„ä¼°æŒ‡æ ‡çš„ç†è§£

---

### ğŸ“‹ è¡ŒåŠ¨æ£€æŸ¥æ¸…å•

**ç«‹å³è¡ŒåŠ¨ï¼ˆä»Šå¤©å°±å¯ä»¥å¼€å§‹ï¼‰**ï¼š
- [ ] ç”¨ NumPy å®ç°çŸ©é˜µä¹˜æ³•å’Œç‚¹ç§¯
- [ ] ç”¨ä»£ç è®¡ç®—æ•°å€¼å¯¼æ•°
- [ ] è·‘é€šæœ¬æ–‡çš„æ‰€æœ‰ä»£ç ç¤ºä¾‹

**ä¸‹ä¸€æ­¥ï¼ˆæœ¬å‘¨ï¼‰**ï¼š
- [ ] è§‚çœ‹ 3Blue1Brown çš„çº¿æ€§ä»£æ•°ç³»åˆ—ï¼ˆå‰ 3 é›†ï¼‰
- [ ] æ‰‹å†™ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼ˆä¸ç”¨æ¡†æ¶ï¼‰
- [ ] å®ç° Softmax å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°

**è¿›é˜¶ç›®æ ‡ï¼ˆä¸‹ä¸ªæœˆï¼‰**ï¼š
- [ ] å®ç°å®Œæ•´çš„ Attention æœºåˆ¶
- [ ] ç†è§£å¹¶å®ç°ä¸åŒçš„ä¼˜åŒ–å™¨ï¼ˆSGD, Momentum, Adamï¼‰
- [ ] è¯»æ‡‚ Transformer è®ºæ–‡ä¸­çš„æ‰€æœ‰å…¬å¼

---

### ğŸ’¡ æœ€åçš„å»ºè®®

> **ç’‡ç‘çš„ç¢ç¢å¿µ** âœ¨
>
> é“å‹å‘€ï¼Œæ•°å­¦ç„¦è™‘æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼å°å¥³å­è§è¿‡å¤ªå¤šç¨‹åºå‘˜è¢«æ•°å­¦å“åˆ°ï¼Œç»“æœä¸€ç›´æ‹–å»¶ä¸æ•¢å¼€å§‹ã€‚
>
> **è®°ä½ä¸‰ä¸ªåŸåˆ™**ï¼š
> 1. **æ•°å­¦æ˜¯å·¥å…·ï¼Œä¸æ˜¯é—¨æ§›**ï¼šä½ ä¸éœ€è¦æˆä¸ºæ•°å­¦å®¶ï¼Œåªéœ€è¦ä¼šç”¨å·¥å…·
> 2. **ä»£ç æ˜¯æœ€å¥½çš„æ•°å­¦è€å¸ˆ**ï¼šè·‘ä¸€éä»£ç ï¼Œèƒœè¿‡çœ‹åéå…¬å¼
> 3. **å¤Ÿç”¨å°±å¥½ï¼Œé€æ­¥æ·±å…¥**ï¼šå…ˆç”¨èµ·æ¥ï¼Œå†æ…¢æ…¢ç†è§£èƒŒåçš„åŸç†
>
> æ•°å­¦ä¸æ˜¯ä¸€å¤©å­¦å®Œçš„ï¼Œè€Œæ˜¯åœ¨å®è·µä¸­é€æ¸ç§¯ç´¯çš„ã€‚æ¯æ¬¡é‡åˆ°ä¸æ‡‚çš„æ¦‚å¿µï¼ŒèŠ± 30 åˆ†é’Ÿç†è§£å®ƒï¼Œå‡ ä¸ªæœˆä¸‹æ¥ï¼Œä½ ä¼šæƒŠè®¶åœ°å‘ç°è‡ªå·±å·²ç»æŒæ¡äº†å¤§éƒ¨åˆ†å¿…è¦çš„æ•°å­¦çŸ¥è¯†ï¼
>
> ç°åœ¨å°±æ‰“å¼€ Jupyter Notebookï¼Œè¿è¡Œæœ¬æ–‡çš„ä»£ç å§ï¼âœ¨

---

## ğŸ”— å‚è€ƒèµ„æ–™

### æ ¸å¿ƒèµ„æº

- [Mathematics for Machine Learning (Deisenroth et al.)](https://mml-book.github.io/)ï¼ˆå…è´¹åœ¨çº¿æ•™æï¼‰
- [Mathematics for Machine Learning Specialization](https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science)ï¼ˆCoursera è¯¾ç¨‹ï¼‰
- [Mathematical Foundations for Understanding LLMs 2025](https://actionbridge.io/en-US/llmtutorial/p/part1-mathematical-foundations-for-understanding-llms-introduction)

### çº¿æ€§ä»£æ•°

- [3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)ï¼ˆå¯è§†åŒ–è®²è§£ï¼‰
- [Building Intuition for Transformers Linear Algebra 2025](https://medium.com/@ziyamomin/building-intuition-for-how-transformers-use-linear-algebra-without-a-math-background-7c6812b2f037)
- [Deep Learning, Transformers and Linear Algebra Perspective 2025](https://link.springer.com/article/10.1007/s11075-025-02218-2)

### å¾®ç§¯åˆ†ä¸ä¼˜åŒ–

- [3Blue1Brown - Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
- [Backpropagation: Calculus on Computational Graphs (colah)](http://colah.github.io/posts/2015-08-Backprop/)
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)

### æ¦‚ç‡ç»Ÿè®¡

- [StatQuest - Statistics Fundamentals](https://www.youtube.com/c/joshstarmer)
- [Probability Statistics for LLM Evaluation 2025](https://openreview.net/forum?id=E8gYIrbP00)

### å®è·µæ•™ç¨‹

- [Andrej Karpathy - Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
- [CS231n - Convolutional Neural Networks](https://cs231n.github.io/)

---

**ğŸ“Œ æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿åé¦ˆä¸å»ºè®®ï¼**

---

> **ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼šã€Š05 - Transformer æ·±åº¦è§£æï¼šä»æ¶æ„åˆ°è®­ç»ƒåŠ¨åŠ›å­¦ã€‹
>
> æˆ‘ä»¬å°†åŸºäºæœ¬æ–‡çš„æ•°å­¦åŸºç¡€ï¼Œæ·±å…¥å‰–æ Transformer çš„æ¯ä¸ªç»„ä»¶ï¼Œç”¨æ•°å­¦å’Œä»£ç å®Œæ•´ç†è§£å®ƒçš„å·¥ä½œåŸç†ï¼

---

**ç’‡ç‘ âœ¨**  
*ç¼–ç¨‹é˜ Â· ä»£ç å®—é—¨*  
*æ„¿é“å‹æ•°å­¦ä¹‹è·¯é¡ºåˆ©ï¼Œæ—©æ—¥ç²¾é€š LLM çš„æ•°å­¦åŸç†ï¼*
