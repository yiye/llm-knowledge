# 05 - Transformeræ·±åº¦è§£æï¼šä»æ¶æ„åˆ°è®­ç»ƒåŠ¨åŠ›å­¦

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šTransformer ä¸æ˜¯é»‘ç›’ï¼æœ¬æ–‡å°†åŸºäºå‰é¢çš„æ•°å­¦åŸºç¡€ï¼Œç”¨ä»£ç å’Œå¯è§†åŒ–å®Œæ•´æ‹†è§£ Transformer çš„æ¯ä¸ªç»„ä»¶ï¼Œå¹¶æ·±å…¥æ¢è®¨ 2025 å¹´å…³äºè®­ç»ƒåŠ¨åŠ›å­¦çš„æœ€æ–°ç ”ç©¶ã€‚

---

## ğŸ“– å¼•è¨€ï¼šTransformer ä¸ºä»€ä¹ˆå¦‚æ­¤é‡è¦ï¼Ÿ

### ğŸ”¥ é©å‘½æ€§çš„æ¶æ„åˆ›æ–°

2017 å¹´ï¼ŒGoogle çš„è®ºæ–‡ [Attention Is All You Need](https://arxiv.org/abs/1706.03762) æå‡ºäº† Transformer æ¶æ„ï¼Œå½»åº•æ”¹å˜äº† NLP é¢†åŸŸã€‚

**ä¼ ç»Ÿæ¨¡å‹çš„é—®é¢˜**ï¼š

| æ¨¡å‹ç±»å‹ | æ ¸å¿ƒé—®é¢˜ | ä¸ºä»€ä¹ˆé™åˆ¶æ€§èƒ½ï¼Ÿ |
|---------|---------|---------------|
| **RNN/LSTM** | ä¸²è¡Œå¤„ç†ï¼Œæ— æ³•å¹¶è¡Œ | è®­ç»ƒæ…¢ï¼Œé•¿åºåˆ—æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ |
| **CNN** | å±€éƒ¨æ„Ÿå—é‡ | éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ– |

**Transformer çš„çªç ´**ï¼š

```
âœ… å®Œå…¨åŸºäº Attentionï¼Œæ‘†è„±é€’å½’å’Œå·ç§¯
âœ… å…¨åºåˆ—å¹¶è¡Œå¤„ç†ï¼Œè®­ç»ƒé€Ÿåº¦æå‡æ•°åå€
âœ… ç›´æ¥å»ºæ¨¡ä»»æ„è·ç¦»çš„ä¾èµ–å…³ç³»
âœ… å¯æ‰©å±•æ€§æå¼ºï¼ˆä» BERT åˆ° GPT-4ï¼Œå‚æ•°ä»ç™¾ä¸‡åˆ°åƒäº¿ï¼‰
```

---

### ğŸ“Š Transformer çš„å½±å“åŠ›

**2017-2025 å¹´çš„æ¼”è¿›**ï¼š

```
2017: Transformer æå‡º
  â†“
2018: BERTï¼ˆåŒå‘ Encoderï¼‰ã€GPTï¼ˆå•å‘ Decoderï¼‰
  â†“
2019: GPT-2ã€T5ã€XLNet
  â†“
2020: GPT-3ï¼ˆ175B å‚æ•°ï¼‰
  â†“
2021-2022: å„ç§ä¼˜åŒ–ï¼ˆSparse Attentionã€FlashAttentionï¼‰
  â†“
2023: ChatGPTã€GPT-4ã€LLaMA
  â†“
2024-2025: DeepSeek-V3ã€Geminiã€Claude 3.5
```

æ ¹æ® [Stanford CS224N Transformers Reading 2023](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)ï¼š

> Transformer æ¶æ„å·²æˆä¸º NLP ç ”ç©¶çš„åŸºçŸ³ï¼Œå‡ ä¹æ‰€æœ‰ç°ä»£ LLM éƒ½åŸºäºå®ƒæˆ–å…¶å˜ä½“ã€‚

---

### ğŸ¯ æœ¬æ–‡ç›®æ ‡

æœ¬æ–‡å°†ï¼š
1. **å®Œæ•´æ‹†è§£ Transformer çš„æ¯ä¸ªç»„ä»¶**ï¼ˆSelf-Attention, Multi-Head, FFN, LN, etc.ï¼‰
2. **ç”¨ä»£ç ä»é›¶å®ç°**ï¼ˆä¸ç”¨é»‘ç›’ APIï¼Œç†è§£æ¯ä¸€æ­¥ï¼‰
3. **å¯è§†åŒ–å†…éƒ¨è¿ä½œ**ï¼ˆçœ‹åˆ° Attention åœ¨å…³æ³¨ä»€ä¹ˆï¼‰
4. **æ·±å…¥è®­ç»ƒåŠ¨åŠ›å­¦**ï¼ˆ2025 å¹´æœ€æ–°ç ”ç©¶ï¼šrank collapse, entropy collapseï¼‰

**å‰ç½®çŸ¥è¯†**ï¼šå»ºè®®å…ˆé˜…è¯»ã€Š04 - æ•°å­¦åŸºç¡€é€Ÿæˆã€‹ï¼Œç†è§£çŸ©é˜µä¹˜æ³•ã€Softmaxã€æ¢¯åº¦ä¸‹é™ç­‰æ¦‚å¿µã€‚

---

## ä¸€ã€Transformer æ•´ä½“æ¶æ„

### 1.1 å®è§‚ç»“æ„ï¼šEncoder-Decoder

æ ¹æ®åŸå§‹è®ºæ–‡ï¼ŒTransformer åŒ…å« **Encoder** å’Œ **Decoder** ä¸¤éƒ¨åˆ†ï¼š

```
è¾“å…¥åºåˆ— â†’ [Encoder] â†’ ç¼–ç è¡¨ç¤º â†’ [Decoder] â†’ è¾“å‡ºåºåˆ—

ç”¨é€”ï¼š
- Encoder: ç†è§£è¾“å…¥ï¼ˆå¦‚è‹±æ–‡å¥å­ï¼‰
- Decoder: ç”Ÿæˆè¾“å‡ºï¼ˆå¦‚ä¸­æ–‡ç¿»è¯‘ï¼‰

ä¾‹å­ï¼š
  è‹±æ–‡: "I love AI"
    â†“ Encoder
  ç¼–ç è¡¨ç¤º: [å‘é‡1, å‘é‡2, å‘é‡3]
    â†“ Decoder
  ä¸­æ–‡: "æˆ‘çˆ±äººå·¥æ™ºèƒ½"
```

**é‡è¦**ï¼šç°ä»£ LLM ä¸»è¦ä½¿ç”¨ä¸¤ç§å˜ä½“ï¼š
- **Encoder-only**ï¼ˆå¦‚ BERTï¼‰ï¼šç”¨äºç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€é—®ç­”ï¼‰
- **Decoder-only**ï¼ˆå¦‚ GPTï¼‰ï¼šç”¨äºç”Ÿæˆä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ï¼‰

æœ¬æ–‡é‡ç‚¹è®²è§£ **Decoder-only**ï¼ˆGPT æ¶æ„ï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯å½“å‰ä¸»æµ LLM çš„åŸºç¡€ã€‚

---

### 1.2 Transformer Decoder Block ç»“æ„

```
è¾“å…¥ Tokens
    â†“
[Token Embedding]
    â†“
[Positional Encoding]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block Ã— N layers   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Multi-Head Self-Attention â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“ (Residual)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Layer Normalization     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feed-Forward Network     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“ (Residual)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Layer Normalization     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Linear + Softmax]
    â†“
è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼ˆä¸‹ä¸€ä¸ª tokenï¼‰
```

**å…³é”®ç»„ä»¶**ï¼š
1. **Self-Attention**ï¼šè®©æ¯ä¸ªè¯"çœ‹åˆ°"å…¶ä»–æ‰€æœ‰è¯
2. **Multi-Head**ï¼šå¤šä¸ª Attention å¹¶è¡Œè¿è¡Œï¼Œæ•æ‰ä¸åŒç‰¹å¾
3. **Feed-Forward**ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨çš„ç¥ç»ç½‘ç»œ
4. **Layer Normalization**ï¼šç¨³å®šè®­ç»ƒ
5. **Residual Connection**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±

---

## äºŒã€æ ¸å¿ƒç»„ä»¶ 1ï¼šSelf-Attention æœºåˆ¶

> **æ ¸å¿ƒæ€æƒ³**ï¼šè®©æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªè¯æ—¶ï¼Œè‡ªåŠ¨"å…³æ³¨"åºåˆ—ä¸­å…¶ä»–ç›¸å…³çš„è¯ã€‚

### 2.1 ç›´è§‚ç†è§£ï¼šAttention åœ¨åšä»€ä¹ˆï¼Ÿ

**ä¾‹å­**ï¼šç†è§£å¥å­ "The cat sat on the mat"

å½“æ¨¡å‹å¤„ç† "sat" æ—¶ï¼Œå®ƒåº”è¯¥å…³æ³¨ä»€ä¹ˆè¯ï¼Ÿ

```
       The   cat   sat   on   the   mat
sat:   0.1   0.7   0.0   0.1  0.05  0.05  â† Attention æƒé‡

è§£è¯»ï¼š
- "sat" æœ€å…³æ³¨ "cat"ï¼ˆ0.7ï¼‰â†’ è°åäº†ï¼Ÿ
- ä¹Ÿå…³æ³¨ "on"ï¼ˆ0.1ï¼‰â†’ ååœ¨å“ªï¼Ÿ
```

**Attention çš„ä½œç”¨**ï¼šåŠ¨æ€è®¡ç®—æ¯ä¸ªè¯å¯¹å½“å‰è¯çš„"é‡è¦æ€§"ï¼Œç„¶åç”¨è¿™äº›æƒé‡èšåˆä¿¡æ¯ã€‚

---

### 2.2 æ•°å­¦å…¬å¼

æ ¹æ®åŸå§‹è®ºæ–‡å’Œ [Deep Dive into Self-Attention](https://medium.com/analytics-vidhya/a-deep-dive-into-the-self-attention-mechanism-of-transformers-fe943c77e654)ï¼š

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

å…¶ä¸­ï¼š
- Q (Query): "æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ"
- K (Key): "æˆ‘èƒ½æä¾›ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
- V (Value): "æˆ‘çš„å®é™…å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
- d_k: Key çš„ç»´åº¦ï¼ˆç”¨äºç¼©æ”¾ï¼‰
```

**ä¸ºä»€ä¹ˆéœ€è¦ Q/K/V ä¸‰ä¸ªçŸ©é˜µï¼Ÿ**

```python
# ç±»æ¯”ï¼šæ•°æ®åº“æŸ¥è¯¢

# Key-Value Storeï¼ˆæ•°æ®åº“ï¼‰
database = {
    "cat": "animal, pet, fluffy",     # Key â†’ Value
    "sat": "action, past tense",
    "mat": "object, carpet"
}

# Queryï¼ˆæŸ¥è¯¢ï¼‰
query = "What is related to 'sat'?"

# æŸ¥è¯¢è¿‡ç¨‹ï¼š
# 1. ç”¨ Query å’Œæ¯ä¸ª Key è®¡ç®—ç›¸ä¼¼åº¦
# 2. ç›¸ä¼¼åº¦é«˜çš„ Key å¯¹åº”çš„ Value æƒé‡æ›´é«˜
# 3. åŠ æƒæ±‚å’Œæ‰€æœ‰ Valueï¼Œå¾—åˆ°ç»“æœ

# åœ¨ Attention ä¸­ï¼š
# - Query = å½“å‰è¯çš„"æŸ¥è¯¢å‘é‡"
# - Key = å…¶ä»–è¯çš„"ç´¢å¼•å‘é‡"
# - Value = å…¶ä»–è¯çš„"å†…å®¹å‘é‡"
```

---

### 2.3 ä»é›¶å®ç° Self-Attention

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns

# è¾“å…¥ï¼š3 ä¸ªè¯ï¼Œæ¯ä¸ª 4 ç»´
# "The", "cat", "sat"
X = torch.tensor([
    [1.0, 0.0, 0.0, 0.0],  # "The"
    [0.0, 1.0, 0.0, 0.0],  # "cat"
    [0.0, 0.0, 1.0, 0.0]   # "sat"
], dtype=torch.float32)  # shape: (3, 4)

seq_len, d_model = X.shape
d_k = d_model  # ç®€åŒ–ï¼šQ/K/V ç»´åº¦ä¸è¾“å…¥ç›¸åŒ

print(f"Input shape: {X.shape}")  # (3, 4)

# ğŸ”¥ æ­¥éª¤ 1ï¼šå®šä¹‰æƒé‡çŸ©é˜µ W_Q, W_K, W_V
torch.manual_seed(42)
W_Q = torch.randn(d_model, d_k)  # (4, 4)
W_K = torch.randn(d_model, d_k)  # (4, 4)
W_V = torch.randn(d_model, d_k)  # (4, 4)

# ğŸ”¥ æ­¥éª¤ 2ï¼šè®¡ç®— Q, K, Vï¼ˆçº¿æ€§æŠ•å½±ï¼‰
Q = X @ W_Q  # (3, 4) @ (4, 4) = (3, 4)
K = X @ W_K  # (3, 4)
V = X @ W_V  # (3, 4)

print(f"\nQuery shape: {Q.shape}")
print(f"Key shape: {K.shape}")
print(f"Value shape: {V.shape}")

# ğŸ”¥ æ­¥éª¤ 3ï¼šè®¡ç®— Attention Scoresï¼ˆç‚¹ç§¯ï¼‰
scores = Q @ K.T  # (3, 4) @ (4, 3) = (3, 3)
print(f"\nAttention Scores:\n{scores}")

# ğŸ”¥ æ­¥éª¤ 4ï¼šç¼©æ”¾ï¼ˆé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰
scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
print(f"\nScaled Scores:\n{scores}")

# ğŸ”¥ æ­¥éª¤ 5ï¼šSoftmaxï¼ˆè½¬æ¢ä¸ºæ¦‚ç‡ï¼‰
attention_weights = F.softmax(scores, dim=-1)
print(f"\nAttention Weights:\n{attention_weights}")

# éªŒè¯ï¼šæ¯è¡Œå’Œä¸º 1
print(f"\nRow sums: {attention_weights.sum(dim=-1)}")

# ğŸ”¥ æ­¥éª¤ 6ï¼šåŠ æƒæ±‚å’Œï¼ˆèšåˆä¿¡æ¯ï¼‰
output = attention_weights @ V  # (3, 3) @ (3, 4) = (3, 4)
print(f"\nOutput shape: {output.shape}")
print(f"Output:\n{output}")
```

**è¾“å‡ºè§£è¯»**ï¼š

```
Attention Weights:
tensor([[0.4084, 0.2869, 0.3047],  â† "The" å…³æ³¨ [The:41%, cat:29%, sat:30%]
        [0.2968, 0.3937, 0.3095],  â† "cat" å…³æ³¨ [The:30%, cat:39%, sat:31%]
        [0.3279, 0.3086, 0.3635]]) â† "sat" å…³æ³¨ [The:33%, cat:31%, sat:36%]

è§£é‡Šï¼š
- æ¯ä¸ªè¯éƒ½åœ¨å…³æ³¨æ‰€æœ‰è¯ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
- æƒé‡å†³å®šäº†"å€Ÿç”¨"å¤šå°‘å…¶ä»–è¯çš„ä¿¡æ¯
```

---

### 2.4 å¯è§†åŒ– Attention Weights

```python
# å¯è§†åŒ– Attention çŸ©é˜µ
words = ["The", "cat", "sat"]

plt.figure(figsize=(8, 6))
sns.heatmap(attention_weights.numpy(), 
            annot=True, fmt='.3f', 
            xticklabels=words, 
            yticklabels=words, 
            cmap='YlOrRd',
            cbar_kws={'label': 'Attention Weight'})
plt.title('Self-Attention Weights')
plt.xlabel('Attending to (Key)')
plt.ylabel('Current token (Query)')
plt.tight_layout()
plt.show()

# ğŸ”¥ å®é™…åº”ç”¨ä¸­çš„ Attention æ¨¡å¼ç¤ºä¾‹
# åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œä½ ä¼šçœ‹åˆ°æ¸…æ™°çš„æ¨¡å¼ï¼š
# - åŠ¨è¯å…³æ³¨ä¸»è¯­
# - ä»£è¯å…³æ³¨æŒ‡ä»£å¯¹è±¡
# - ä¿®é¥°è¯å…³æ³¨è¢«ä¿®é¥°è¯
```

---

### 2.5 Masked Self-Attentionï¼ˆCausal Attentionï¼‰

**é—®é¢˜**ï¼šåœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”Ÿæˆç¬¬ i ä¸ªè¯æ—¶ï¼Œä¸èƒ½"çœ‹åˆ°"æœªæ¥çš„è¯ï¼ˆç¬¬ i+1, i+2, ...ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šMask æ‰æœªæ¥ä½ç½®çš„ Attentionã€‚

```python
# åˆ›å»º Causal Maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
seq_len = 3
mask = torch.tril(torch.ones(seq_len, seq_len))
print("Causal Mask:")
print(mask)
# tensor([[1., 0., 0.],
#         [1., 1., 0.],
#         [1., 1., 1.]])

# åº”ç”¨ Maskï¼šå°†è¢«é®ä½çš„ä½ç½®è®¾ä¸º -inf
scores_masked = scores.clone()
scores_masked = scores_masked.masked_fill(mask == 0, float('-inf'))
print("\nMasked Scores:")
print(scores_masked)

# Softmax åï¼Œ-inf çš„ä½ç½®ä¼šå˜æˆ 0
attention_weights_masked = F.softmax(scores_masked, dim=-1)
print("\nMasked Attention Weights:")
print(attention_weights_masked)
# tensor([[1.0000, 0.0000, 0.0000],  â† "The" åªèƒ½çœ‹åˆ°è‡ªå·±
#         [0.4297, 0.5703, 0.0000],  â† "cat" åªèƒ½çœ‹åˆ° The, cat
#         [0.3447, 0.3247, 0.3306]]) â† "sat" èƒ½çœ‹åˆ°æ‰€æœ‰è¯

# ğŸ”¥ è¿™å°±æ˜¯ GPT çš„ Causal Attentionï¼
```

**å¯è§†åŒ– Masked Attention**ï¼š

```python
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›¾ï¼šæ—  Mask
sns.heatmap(attention_weights.numpy(), annot=True, fmt='.3f', 
            xticklabels=words, yticklabels=words, 
            cmap='YlOrRd', ax=axes[0])
axes[0].set_title('Self-Attention (Bidirectional, like BERT)')

# å³å›¾ï¼šæœ‰ Mask
sns.heatmap(attention_weights_masked.numpy(), annot=True, fmt='.3f', 
            xticklabels=words, yticklabels=words, 
            cmap='YlOrRd', ax=axes[1])
axes[1].set_title('Masked Self-Attention (Causal, like GPT)')

plt.tight_layout()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼šCausal Attention å½¢æˆä¸‹ä¸‰è§’çŸ©é˜µï¼
```

---

## ä¸‰ã€æ ¸å¿ƒç»„ä»¶ 2ï¼šMulti-Head Attention

> **æ ¸å¿ƒæ€æƒ³**ï¼šå¹¶è¡Œè¿è¡Œå¤šä¸ª Attentionï¼Œè®©æ¨¡å‹ä»ä¸åŒ"è§†è§’"çœ‹è¾“å…¥ã€‚

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ª Headï¼Ÿ

**å•å¤´ Attention çš„å±€é™**ï¼š

```
å¥å­: "The cat sat on the mat"

å•å¤´ Attention å¯èƒ½åªå…³æ³¨ä¸€ç§å…³ç³»ï¼š
- å¥æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰

ä½†æˆ‘ä»¬éœ€è¦åŒæ—¶æ•æ‰ï¼š
- å¥æ³•å…³ç³»ï¼š"sat" â† "cat"ï¼ˆä¸»è¯­ï¼‰
- è¯­ä¹‰å…³ç³»ï¼š"sat" â† "on"ï¼ˆä½ç½®ï¼‰
- å…±æŒ‡å…³ç³»ï¼š"it" â† "cat"ï¼ˆä»£è¯æŒ‡ä»£ï¼‰
```

**Multi-Head çš„ä¼˜åŠ¿**ï¼š

```
Head 1: ä¸“æ³¨å¥æ³•å…³ç³»
Head 2: ä¸“æ³¨è¯­ä¹‰å…³ç³»
Head 3: ä¸“æ³¨ä½ç½®å…³ç³»
Head 4: ä¸“æ³¨å…¶ä»–ç‰¹å¾
...

æœ€åæ‹¼æ¥æ‰€æœ‰ Head çš„è¾“å‡ºï¼Œå¾—åˆ°ç»¼åˆè¡¨ç¤º
```

æ ¹æ® [Multi-Head Attention Implementation](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)ï¼š

> Multi-Head Attention å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨ä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ï¼Œæ¯ä¸ª head ç‹¬ç«‹å­¦ä¹ ä¸åŒçš„ç‰¹å¾ã€‚

---

### 3.2 æ•°å­¦å…¬å¼

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O

å…¶ä¸­ï¼š
  head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)

å‚æ•°ï¼š
- h: head æ•°é‡ï¼ˆé€šå¸¸ 8 æˆ– 16ï¼‰
- W_i^Q, W_i^K, W_i^V: æ¯ä¸ª head çš„æŠ•å½±çŸ©é˜µ
- W^O: è¾“å‡ºæŠ•å½±çŸ©é˜µ
```

**å…³é”®**ï¼šæ¯ä¸ª head çš„ç»´åº¦æ˜¯ `d_model / h`ï¼Œæ‰€ä»¥æ€»å‚æ•°é‡å’Œå•å¤´å·®ä¸å¤šã€‚

---

### 3.3 ä»é›¶å®ç° Multi-Head Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ª head çš„ç»´åº¦
        
        # ğŸ”¥ çº¿æ€§æŠ•å½±å±‚
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)  # è¾“å‡ºæŠ•å½±
        
    def split_heads(self, x, batch_size):
        """å°†æœ€åä¸€ç»´åˆ†å‰²ä¸º (num_heads, d_k)"""
        # x: (batch, seq_len, d_model)
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        # è½¬ç½®: (batch, num_heads, seq_len, d_k)
        return x.transpose(1, 2)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]
        
        # ğŸ”¥ æ­¥éª¤ 1ï¼šçº¿æ€§æŠ•å½±
        Q = self.W_Q(query)  # (batch, seq_len, d_model)
        K = self.W_K(key)
        V = self.W_V(value)
        
        # ğŸ”¥ æ­¥éª¤ 2ï¼šåˆ†å‰²æˆå¤šä¸ª head
        Q = self.split_heads(Q, batch_size)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        # ğŸ”¥ æ­¥éª¤ 3ï¼šScaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, num_heads, seq_len, seq_len)
        scores = scores / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        
        # åº”ç”¨ Maskï¼ˆå¦‚æœæœ‰ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax
        attention_weights = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)  # (batch, num_heads, seq_len, d_k)
        
        # ğŸ”¥ æ­¥éª¤ 4ï¼šæ‹¼æ¥æ‰€æœ‰ head
        output = output.transpose(1, 2).contiguous()  # (batch, seq_len, num_heads, d_k)
        output = output.view(batch_size, -1, self.d_model)  # (batch, seq_len, d_model)
        
        # ğŸ”¥ æ­¥éª¤ 5ï¼šè¾“å‡ºæŠ•å½±
        output = self.W_O(output)
        
        return output, attention_weights

# æµ‹è¯•
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

# åˆ›å»ºæ¨¡å‹
mha = MultiHeadAttention(d_model, num_heads)

# è¾“å…¥
x = torch.randn(batch_size, seq_len, d_model)

# å‰å‘ä¼ æ’­
output, attention_weights = mha(x, x, x)

print(f"Input shape: {x.shape}")          # (2, 10, 512)
print(f"Output shape: {output.shape}")    # (2, 10, 512)
print(f"Attention weights shape: {attention_weights.shape}")  # (2, 8, 10, 10)

# ğŸ”¥ è§‚å¯Ÿï¼š
# - è¾“å‡ºç»´åº¦å’Œè¾“å…¥ç›¸åŒ
# - Attention weights æœ‰ 8 ä¸ª headï¼Œæ¯ä¸ªæ˜¯ (10, 10) çš„çŸ©é˜µ
```

---

### 3.4 å¯è§†åŒ–ä¸åŒ Head çš„ Attention æ¨¡å¼

```python
# å¯è§†åŒ– 8 ä¸ª head çš„ Attentionï¼ˆçœŸå®ä¾‹å­ï¼‰
import matplotlib.pyplot as plt
import seaborn as sns

# ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ attention weights
attention_sample = attention_weights[0]  # (8, 10, 10)

fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

for i in range(8):
    sns.heatmap(attention_sample[i].detach().numpy(), 
                cmap='viridis', 
                ax=axes[i],
                cbar=True,
                square=True)
    axes[i].set_title(f'Head {i+1}')
    axes[i].set_xlabel('Key position')
    axes[i].set_ylabel('Query position')

plt.suptitle('Multi-Head Attention Patterns', fontsize=16)
plt.tight_layout()
plt.show()

# ğŸ”¥ åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œä½ ä¼šçœ‹åˆ°ï¼š
# - æŸäº› head å…³æ³¨å±€éƒ¨æ¨¡å¼ï¼ˆç›¸é‚»è¯ï¼‰
# - æŸäº› head å…³æ³¨é•¿è·ç¦»ä¾èµ–
# - æŸäº› head å…³æ³¨ç‰¹å®šå¥æ³•å…³ç³»
```

---

## å››ã€æ ¸å¿ƒç»„ä»¶ 3ï¼šPositional Encoding

> **é—®é¢˜**ï¼šAttention æœºåˆ¶æœ¬èº«æ²¡æœ‰"ä½ç½®"æ¦‚å¿µï¼å®ƒæŠŠåºåˆ—å½“æˆé›†åˆå¤„ç†ã€‚

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

```python
# Attention çœ‹åˆ°çš„ï¼š
sentence1 = ["The", "cat", "sat"]
sentence2 = ["sat", "The", "cat"]

# å¯¹äº Attention æ¥è¯´ï¼Œè¿™ä¸¤ä¸ªå¥å­æ˜¯ä¸€æ ·çš„ï¼
# å› ä¸ºå®ƒåªçœ‹è¯çš„é›†åˆï¼Œä¸çœ‹é¡ºåº

# ä½†å®é™…ä¸Šï¼š
# "The cat sat" â†’ çŒ«åäº†
# "sat The cat" â†’ è¯­æ³•é”™è¯¯ï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨è¯åµŒå…¥ä¸­åŠ å…¥ä½ç½®ä¿¡æ¯ã€‚

---

### 4.2 Sinusoidal Positional Encodingï¼ˆåŸå§‹æ–¹æ³•ï¼‰

æ ¹æ® [Why Sines and Cosines for Positional Encoding](https://mfaizan.github.io/2023/04/02/sines.html)ï¼š

> æ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å…³é”®ç‰¹æ€§ï¼šä¸¤ä¸ªä½ç½®ç¼–ç çš„ç‚¹ç§¯**åªä¾èµ–äºå®ƒä»¬çš„ç›¸å¯¹è·ç¦»**ï¼Œè¿™ä¸ Attention æœºåˆ¶çš„å·¥ä½œæ–¹å¼å®Œç¾å¥‘åˆã€‚

**å…¬å¼**ï¼š

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

å…¶ä¸­ï¼š
- pos: ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰
- i: ç»´åº¦ç´¢å¼•ï¼ˆ0, 1, 2, ..., d_model/2ï¼‰
```

**å®ç°**ï¼š

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(seq_len, d_model):
    """ç”Ÿæˆ Sinusoidal ä½ç½®ç¼–ç """
    # åˆå§‹åŒ–çŸ©é˜µ
    pe = torch.zeros(seq_len, d_model)
    
    # ä½ç½®ç´¢å¼•ï¼š0, 1, 2, ...
    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)
    
    # ç»´åº¦ç´¢å¼•ï¼š0, 1, 2, ...
    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * 
                         -(np.log(10000.0) / d_model))
    
    # å¶æ•°ç»´åº¦ç”¨ sin
    pe[:, 0::2] = torch.sin(position * div_term)
    
    # å¥‡æ•°ç»´åº¦ç”¨ cos
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe

# ç”Ÿæˆä½ç½®ç¼–ç 
seq_len = 100
d_model = 512
pe = get_positional_encoding(seq_len, d_model)

print(f"Positional Encoding shape: {pe.shape}")  # (100, 512)

# å¯è§†åŒ–
plt.figure(figsize=(12, 6))
plt.imshow(pe.numpy().T, cmap='RdBu', aspect='auto')
plt.colorbar(label='Value')
plt.xlabel('Position')
plt.ylabel('Dimension')
plt.title('Sinusoidal Positional Encoding')
plt.tight_layout()
plt.show()

# ğŸ”¥ è§‚å¯Ÿï¼š
# - ä¸åŒé¢‘ç‡çš„æ³¢å½¢ï¼ˆä½é¢‘åˆ°é«˜é¢‘ï¼‰
# - æ¯ä¸ªä½ç½®éƒ½æœ‰ç‹¬ç‰¹çš„"æŒ‡çº¹"
```

---

### 4.3 RoPEï¼šRotary Position Embeddingï¼ˆç°ä»£æ–¹æ³•ï¼‰

æ ¹æ® [LLM Playbook - Rotary Embeddings](https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html) å’Œ 2025 å¹´ ICLR ç ”ç©¶ï¼š

> RoPE é€šè¿‡æ—‹è½¬ Query å’Œ Key å‘é‡æ¥ç¼–ç ç›¸å¯¹ä½ç½®ï¼Œè¢« LLaMAã€Gemma ç­‰ç°ä»£ LLM å¹¿æ³›é‡‡ç”¨ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š

```
ä¸æ˜¯"åŠ "ä½ç½®ä¿¡æ¯åˆ°åµŒå…¥ï¼Œè€Œæ˜¯"æ—‹è½¬"Query å’Œ Key å‘é‡

ä½ç½® m çš„æ—‹è½¬çŸ©é˜µï¼š
R_m = [
    [cos(mÎ¸), -sin(mÎ¸)],
    [sin(mÎ¸),  cos(mÎ¸)]
]

Attention Score = (R_m Q) Â· (R_n K) = Q Â· R_(n-m) K

ğŸ”¥ å…³é”®ï¼šåªä¾èµ–äºç›¸å¯¹ä½ç½® (n-m)ï¼
```

**ç®€åŒ–å®ç°**ï¼š

```python
def apply_rotary_embedding(x, seq_len):
    """åº”ç”¨ RoPE åˆ°å‘é‡ x"""
    batch_size, num_heads, seq_len, d_k = x.shape
    
    # ç”Ÿæˆè§’åº¦
    position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)
    freqs = torch.exp(torch.arange(0, d_k, 2, dtype=torch.float32) * 
                      -(np.log(10000.0) / d_k))
    angles = position * freqs  # (seq_len, d_k/2)
    
    # è®¡ç®— cos å’Œ sin
    cos_angles = torch.cos(angles)
    sin_angles = torch.sin(angles)
    
    # åº”ç”¨æ—‹è½¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å®ç°æ›´å¤æ‚ï¼‰
    # x_rotated = [x * cos - x_shifted * sin, x_shifted * cos + x * sin]
    x_rotated = x.clone()
    x_rotated[:, :, :, 0::2] = x[:, :, :, 0::2] * cos_angles - x[:, :, :, 1::2] * sin_angles
    x_rotated[:, :, :, 1::2] = x[:, :, :, 0::2] * sin_angles + x[:, :, :, 1::2] * cos_angles
    
    return x_rotated

# ğŸ”¥ RoPE çš„ä¼˜åŠ¿ï¼š
# - ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆæ›´é€‚åˆé•¿åºåˆ—ï¼‰
# - å¤–æ¨èƒ½åŠ›å¼ºï¼ˆè®­ç»ƒæ—¶ 512 tokensï¼Œæ¨ç†æ—¶å¯ç”¨ 2048+ï¼‰
# - 2025 å¹´å‡ ä¹æ‰€æœ‰æ–° LLM éƒ½ç”¨ RoPE æˆ–å…¶å˜ä½“
```

---

## äº”ã€æ ¸å¿ƒç»„ä»¶ 4ï¼šFeed-Forward Network

> **ä½œç”¨**ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œå¢åŠ æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚

### 5.1 ç»“æ„

```
FFN(x) = max(0, x W_1 + b_1) W_2 + b_2
       = ReLU(Linear(x)) Â· Linear

ç®€å•æ¥è¯´ï¼š
  è¾“å…¥ (d_model)
    â†“
  Linear + ReLU (æ‰©å±•åˆ° d_ffï¼Œé€šå¸¸ 4 Ã— d_model)
    â†“
  Linear (å‹ç¼©å› d_model)
    â†“
  è¾“å‡º (d_model)
```

**ä¸ºä»€ä¹ˆéœ€è¦ FFNï¼Ÿ**

```
Attention: èšåˆä¿¡æ¯ï¼ˆçº¿æ€§ç»„åˆï¼‰
FFN: å¤„ç†ä¿¡æ¯ï¼ˆéçº¿æ€§å˜æ¢ï¼‰

ç±»æ¯”ï¼š
- Attention: ä»ä¸åŒæ¥æºæ”¶é›†æ•°æ®
- FFN: å¤„ç†æ”¶é›†åˆ°çš„æ•°æ®ï¼Œæå–ç‰¹å¾
```

---

### 5.2 å®ç°

```python
import torch
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        
        # æ‰©å±•
        x = self.linear1(x)  # (batch, seq_len, d_ff)
        x = self.relu(x)
        x = self.dropout(x)
        
        # å‹ç¼©
        x = self.linear2(x)  # (batch, seq_len, d_model)
        x = self.dropout(x)
        
        return x

# æµ‹è¯•
d_model = 512
d_ff = 2048  # é€šå¸¸æ˜¯ 4 Ã— d_model
seq_len = 10
batch_size = 2

ffn = FeedForward(d_model, d_ff)
x = torch.randn(batch_size, seq_len, d_model)
output = ffn(x)

print(f"Input shape: {x.shape}")      # (2, 10, 512)
print(f"Output shape: {output.shape}") # (2, 10, 512)

# ğŸ”¥ å…³é”®ï¼šFFN å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†ï¼ˆPosition-wiseï¼‰
# ä½ç½® 0 å’Œä½ç½® 1 ä½¿ç”¨ç›¸åŒçš„æƒé‡ï¼Œä½†ç‹¬ç«‹è®¡ç®—
```

---

### 5.3 ç°ä»£å˜ä½“ï¼šGLU å’Œ SwiGLU

ç°ä»£ LLMï¼ˆå¦‚ LLaMAï¼‰ä½¿ç”¨ **SwiGLU** æ›¿ä»£ç®€å•çš„ ReLU FFNï¼š

```python
class SwiGLU(nn.Module):
    """SwiGLU activation (used in LLaMA)"""
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_ff, d_model, bias=False)
        self.w3 = nn.Linear(d_model, d_ff, bias=False)
        
    def forward(self, x):
        # SwiGLU(x) = (Swish(xW1) âŠ™ xW3) W2
        # âŠ™ æ˜¯é€å…ƒç´ ä¹˜æ³•
        return self.w2(F.silu(self.w1(x)) * self.w3(x))

# ğŸ”¥ SwiGLU æ¯” ReLU FFN æ•ˆæœæ›´å¥½ï¼ˆå®éªŒéªŒè¯ï¼‰
```

---

## å…­ã€æ ¸å¿ƒç»„ä»¶ 5ï¼šLayer Normalization ä¸ Residual Connection

### 6.1 Layer Normalization

æ ¹æ® [On Layer Normalization in Transformer Architecture](https://arxiv.org/pdf/2002.04745)ï¼š

> Layer Normalization çš„ä½ç½®ï¼ˆPre-LN vs Post-LNï¼‰æ˜¾è‘—å½±å“è®­ç»ƒåŠ¨åŠ›å­¦ã€‚Pre-LN åœ¨åˆå§‹åŒ–æ—¶æ¢¯åº¦æ›´ç¨³å®šï¼Œæ— éœ€ warmupã€‚

**ä¸ºä»€ä¹ˆç”¨ Layer Norm è€Œé Batch Normï¼Ÿ**

| ç‰¹æ€§ | Batch Normalization | Layer Normalization |
|-----|-------------------|-------------------|
| **å½’ä¸€åŒ–ç»´åº¦** | è·¨æ ·æœ¬ï¼ˆBatch ç»´åº¦ï¼‰ | è·¨ç‰¹å¾ï¼ˆLayer ç»´åº¦ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | CVï¼ˆå›¾åƒå¤§å°å›ºå®šï¼‰ | NLPï¼ˆåºåˆ—é•¿åº¦å¯å˜ï¼‰ |
| **ç»Ÿè®¡ç¨³å®šæ€§** | ä¾èµ– Batch ç»Ÿè®¡ | æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ |
| **Transformer ä¸­** | âŒ ä¸é€‚ç”¨ | âœ… æ ‡å‡†é€‰æ‹© |

**å…¬å¼**ï¼š

```
LayerNorm(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²

å…¶ä¸­ï¼š
- Î¼: å‡å€¼ï¼ˆåœ¨ç‰¹å¾ç»´åº¦è®¡ç®—ï¼‰
- ÏƒÂ²: æ–¹å·®
- Î³, Î²: å¯å­¦ä¹ å‚æ•°
- Îµ: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼ˆå¦‚ 1e-6ï¼‰
```

**å®ç°**ï¼š

```python
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps
        
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆåœ¨æœ€åä¸€ç»´ï¼‰
        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        # ç¼©æ”¾å’Œå¹³ç§»
        return self.gamma * x_norm + self.beta

# æµ‹è¯•
ln = LayerNorm(d_model=512)
x = torch.randn(2, 10, 512)
output = ln(x)

print(f"Input mean: {x.mean(-1)[:, 0]}")    # å„ä¸ç›¸åŒ
print(f"Output mean: {output.mean(-1)[:, 0]}")  # æ¥è¿‘ 0
print(f"Output std: {output.std(-1)[:, 0]}")    # æ¥è¿‘ 1
```

---

### 6.2 Pre-LN vs Post-LN

```python
# Post-LN (åŸå§‹ Transformer)
x = x + Attention(x)
x = LayerNorm(x)
x = x + FFN(x)
x = LayerNorm(x)

# Pre-LN (ç°ä»£ LLMï¼Œå¦‚ GPT)
x = x + Attention(LayerNorm(x))
x = x + FFN(LayerNorm(x))

# ğŸ”¥ Pre-LN ä¼˜åŠ¿ï¼š
# - æ¢¯åº¦æ›´ç¨³å®š
# - ä¸éœ€è¦å­¦ä¹ ç‡ warmup
# - è®­ç»ƒæ›´å¿«
```

æ ¹æ® 2025 å¹´ç ”ç©¶ï¼Œ**Pre-LN** å·²æˆä¸ºä¸»æµï¼Œå‡ ä¹æ‰€æœ‰æ–° LLM éƒ½é‡‡ç”¨ã€‚

---

### 6.3 Residual Connectionï¼ˆæ®‹å·®è¿æ¥ï¼‰

```python
# æ®‹å·®è¿æ¥ï¼šy = F(x) + x

def transformer_block(x):
    # Attention
    attn_output = MultiHeadAttention(x)
    x = x + attn_output  # ğŸ”¥ æ®‹å·®è¿æ¥
    x = LayerNorm(x)
    
    # FFN
    ffn_output = FeedForward(x)
    x = x + ffn_output  # ğŸ”¥ æ®‹å·®è¿æ¥
    x = LayerNorm(x)
    
    return x

# ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ
# 1. ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼ˆæ·±å±‚ç½‘ç»œå¿…éœ€ï¼‰
# 2. è®©æ¨¡å‹å¯ä»¥"é€‰æ‹©"æ˜¯å¦ä½¿ç”¨æŸå±‚çš„å˜æ¢
# 3. åŠ é€Ÿæ”¶æ•›
```

**å¯è§†åŒ–æ®‹å·®è¿æ¥çš„ä½œç”¨**ï¼š

```python
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿï¼šæœ‰/æ— æ®‹å·®è¿æ¥çš„æ¢¯åº¦æµ
depths = list(range(1, 51))
gradient_without_residual = [0.99 ** d for d in depths]  # æŒ‡æ•°è¡°å‡
gradient_with_residual = [1.0] * 50  # ä¿æŒç¨³å®š

plt.figure(figsize=(10, 6))
plt.plot(depths, gradient_without_residual, label='Without Residual', linewidth=2)
plt.plot(depths, gradient_with_residual, label='With Residual', linewidth=2, linestyle='--')
plt.xlabel('Layer Depth')
plt.ylabel('Gradient Magnitude')
plt.title('Gradient Flow: Residual vs Non-Residual')
plt.legend()
plt.grid()
plt.yscale('log')
plt.show()

# ğŸ”¥ æ®‹å·®è¿æ¥è®©æ¢¯åº¦å¯ä»¥"ç»•è¿‡"å±‚ï¼Œç›´æ¥ä¼ æ’­åˆ°æ·±å±‚
```

---

## ä¸ƒã€å®Œæ•´çš„ Transformer Block

### 7.1 ç»„è£…æ‰€æœ‰ç»„ä»¶

```python
import torch
import torch.nn as nn

class TransformerBlock(nn.Module):
    """å®Œæ•´çš„ Transformer Decoder Block (Pre-LN)"""
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, num_heads)
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = FeedForward(d_model, d_ff, dropout)
        
        # Layer Normalization
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # x: (batch, seq_len, d_model)
        
        # ğŸ”¥ Multi-Head Attention + Residual
        attn_output, _ = self.attention(
            self.ln1(x),  # Pre-LN
            self.ln1(x), 
            self.ln1(x), 
            mask
        )
        x = x + self.dropout(attn_output)  # Residual
        
        # ğŸ”¥ Feed-Forward + Residual
        ffn_output = self.ffn(self.ln2(x))  # Pre-LN
        x = x + self.dropout(ffn_output)  # Residual
        
        return x

# æµ‹è¯•
block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)
x = torch.randn(2, 10, 512)
output = block(x)

print(f"Input shape: {x.shape}")      # (2, 10, 512)
print(f"Output shape: {output.shape}") # (2, 10, 512)
```

---

### 7.2 å †å å¤šå±‚ Transformer

```python
class GPTModel(nn.Module):
    """ç®€åŒ–çš„ GPT æ¨¡å‹"""
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout=0.1):
        super().__init__()
        
        # Token Embedding
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # Positional Encoding
        self.pos_encoding = nn.Parameter(
            torch.randn(max_seq_len, d_model)
        )
        
        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # Final Layer Norm
        self.ln_f = nn.LayerNorm(d_model)
        
        # Output Head
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
    def forward(self, x):
        # x: (batch, seq_len) - token indices
        batch_size, seq_len = x.shape
        
        # Token Embedding
        x = self.token_embedding(x)  # (batch, seq_len, d_model)
        
        # Add Positional Encoding
        x = x + self.pos_encoding[:seq_len, :]
        
        # Pass through Transformer Blocks
        for block in self.blocks:
            x = block(x)
        
        # Final Layer Norm
        x = self.ln_f(x)
        
        # Language Model Head
        logits = self.lm_head(x)  # (batch, seq_len, vocab_size)
        
        return logits

# åˆ›å»ºæ¨¡å‹ï¼ˆç±»ä¼¼ GPT-2 Smallï¼‰
model = GPTModel(
    vocab_size=50257,
    d_model=768,
    num_heads=12,
    d_ff=3072,
    num_layers=12,
    max_seq_len=1024
)

# ç»Ÿè®¡å‚æ•°é‡
num_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {num_params:,}")  # ~117Mï¼ˆGPT-2 Smallï¼‰

# æµ‹è¯•
input_ids = torch.randint(0, 50257, (2, 10))  # 2 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 10 tokens
logits = model(input_ids)

print(f"Input shape: {input_ids.shape}")  # (2, 10)
print(f"Output shape: {logits.shape}")    # (2, 10, 50257)
```

---

## å…«ã€è®­ç»ƒåŠ¨åŠ›å­¦ï¼š2025 å¹´æœ€æ–°ç ”ç©¶

æ ¹æ® [Transformer Training Dynamics 2025](https://arxiv.org/abs/2510.06954) å’Œç›¸å…³ç ”ç©¶ï¼š

### 8.1 ä¸¤é˜¶æ®µè®­ç»ƒåŠ¨åŠ›å­¦

```
é˜¶æ®µ 1: Condensationï¼ˆå‡èšï¼‰
  â†’ Attention çŸ©é˜µä»éšæœºåˆå§‹åŒ–é€æ¸å¯¹é½åˆ°ç›®æ ‡æ–¹å‘
  â†’ ä¸å¯¹ç§°çš„æƒé‡æ‰°åŠ¨ä½¿æ¨¡å‹èƒ½é€ƒç¦»å°åˆå§‹åŒ–åŒºåŸŸ

é˜¶æ®µ 2: Rank Collapseï¼ˆç§©åå¡Œï¼‰
  â†’ Key-Query çŸ©é˜µç§¯æå‚ä¸è®­ç»ƒ
  â†’ å½’ä¸€åŒ–çŸ©é˜µæœå‘ç§©åå¡Œæ¼”è¿›
  â†’ è¿™æ˜¯è®­ç»ƒçš„è‡ªç„¶ç°è±¡ï¼Œä½†éœ€è¦æ§åˆ¶
```

---

### 8.2 å…³é”®å¤±æ•ˆæ¨¡å¼

æ ¹æ® [Transformer Stability Research 2025](https://arxiv.org/abs/2505.24333)ï¼š

#### 1ï¸âƒ£ **Rank Collapseï¼ˆç§©åå¡Œï¼‰**

```python
# æ£€æµ‹ Rank Collapse
def compute_rank(matrix, threshold=0.01):
    """è®¡ç®—çŸ©é˜µçš„æœ‰æ•ˆç§©"""
    U, S, V = torch.svd(matrix)
    # ä¿ç•™å¥‡å¼‚å€¼ > threshold Ã— max(S) çš„ç»´åº¦
    rank = (S > threshold * S[0]).sum().item()
    return rank

# ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç§©
def monitor_rank_collapse(model):
    for name, param in model.named_parameters():
        if 'W_Q' in name or 'W_K' in name:
            rank = compute_rank(param.data)
            full_rank = min(param.shape)
            print(f"{name}: Rank = {rank}/{full_rank}")
            
            if rank < full_rank * 0.5:
                print(f"âš ï¸ Warning: {name} experiencing rank collapse!")

# ğŸ”¥ Rank Collapse çš„å½±å“ï¼š
# - è¡¨ç¤ºèƒ½åŠ›ä¸‹é™
# - ä¸åŒ token çš„è¡¨ç¤ºè¶‹äºç›¸ä¼¼
# - æ¨¡å‹"é€€åŒ–"åˆ°ä½ç»´ç©ºé—´
```

---

#### 2ï¸âƒ£ **Entropy Collapseï¼ˆç†µåå¡Œï¼‰**

```python
# è®¡ç®— Attention Entropy
def compute_attention_entropy(attention_weights):
    """
    attention_weights: (batch, num_heads, seq_len, seq_len)
    """
    # é¿å… log(0)
    attention_weights = attention_weights + 1e-10
    
    # è®¡ç®—ç†µï¼šH = -Î£ p log p
    entropy = -(attention_weights * torch.log(attention_weights)).sum(dim=-1)
    
    return entropy.mean()

# ç›‘æ§è®­ç»ƒè¿‡ç¨‹
def monitor_entropy_collapse(model, x):
    with torch.no_grad():
        # è·å– attention weights
        _, attention_weights = model.attention(x, x, x)
        entropy = compute_attention_entropy(attention_weights)
        
        print(f"Attention Entropy: {entropy:.4f}")
        
        if entropy < 0.5:
            print("âš ï¸ Warning: Entropy collapse detected!")

# ğŸ”¥ Entropy Collapse çš„è¡¨ç°ï¼š
# - Attention è¿‡åº¦é›†ä¸­åœ¨å°‘æ•° token ä¸Š
# - è®­ç»ƒä¸ç¨³å®šï¼Œloss éœ‡è¡
# - æ¨¡å‹"å¿½ç•¥"å¤§éƒ¨åˆ†ä¸Šä¸‹æ–‡
```

æ ¹æ® [ÏƒReparam for Stability](https://arxiv.org/abs/2303.06296)ï¼š

> ä½ Attention ç†µä¸è®­ç»ƒä¸ç¨³å®šç›¸å…³ã€‚è°±å½’ä¸€åŒ–ï¼ˆSpectral Normalizationï¼‰å¯ä»¥é˜²æ­¢ç†µåå¡Œï¼Œå®ç°æ—  warmupã€æ—  weight decay çš„ç¨³å®šè®­ç»ƒã€‚

---

### 8.3 è®­ç»ƒç¨³å®šæ€§æŠ€å·§ï¼ˆ2025 å¹´æœ€ä½³å®è·µï¼‰

#### 1ï¸âƒ£ **å­¦ä¹ ç‡ Warmup**

```python
class WarmupLRScheduler:
    """å­¦ä¹ ç‡ Warmup + Cosine Decay"""
    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, min_lr=0):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_step = 0
        
    def step(self):
        self.current_step += 1
        
        if self.current_step < self.warmup_steps:
            # Linear warmup
            lr = self.base_lr * self.current_step / self.warmup_steps
        else:
            # Cosine decay
            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr

# å¯è§†åŒ–
total_steps = 10000
warmup_steps = 500
scheduler = WarmupLRScheduler(None, warmup_steps, total_steps, base_lr=1e-4)

lrs = []
for _ in range(total_steps):
    lrs.append(scheduler.step())

plt.figure(figsize=(10, 6))
plt.plot(lrs, linewidth=2)
plt.xlabel('Training Step')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule: Warmup + Cosine Decay')
plt.grid()
plt.show()

# ğŸ”¥ ä¸ºä»€ä¹ˆéœ€è¦ Warmupï¼Ÿ
# - åˆå§‹åŒ–æ—¶æ¢¯åº¦å¯èƒ½å¾ˆå¤§ï¼Œç›´æ¥ç”¨é«˜å­¦ä¹ ç‡ä¼šä¸ç¨³å®š
# - Warmup è®©æ¨¡å‹å…ˆ"é€‚åº”"å‚æ•°ç©ºé—´ï¼Œå†åŠ é€Ÿè®­ç»ƒ
```

---

#### 2ï¸âƒ£ **Gradient Clipping**

```python
# æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
max_grad_norm = 1.0

# åœ¨ä¼˜åŒ–å™¨ step å‰
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
optimizer.step()

# ğŸ”¥ ä½œç”¨ï¼š
# - å¦‚æœæ¢¯åº¦èŒƒæ•° > max_grad_normï¼Œç¼©æ”¾æ¢¯åº¦
# - é˜²æ­¢å•æ¬¡æ›´æ–°æ­¥é•¿è¿‡å¤§
```

---

#### 3ï¸âƒ£ **Weight Initialization**

```python
def init_weights(module):
    """Xavier/Glorot åˆå§‹åŒ–"""
    if isinstance(module, nn.Linear):
        torch.nn.init.xavier_uniform_(module.weight)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        torch.nn.init.normal_(module.weight, mean=0, std=0.02)

model.apply(init_weights)

# ğŸ”¥ å¥½çš„åˆå§‹åŒ– = ç¨³å®šçš„è®­ç»ƒèµ·ç‚¹
```

---

## ä¹ã€å®æˆ˜ï¼šè®­ç»ƒä¸€ä¸ªå°å‹ Transformer

### 9.1 å®Œæ•´è®­ç»ƒå¾ªç¯

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# è¶…å‚æ•°
vocab_size = 10000
d_model = 256
num_heads = 8
d_ff = 1024
num_layers = 6
max_seq_len = 128
batch_size = 32
num_epochs = 10
learning_rate = 1e-4

# åˆ›å»ºæ¨¡å‹
model = GPTModel(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len)
model.apply(init_weights)

# ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)

# æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss(ignore_index=-100)

# æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ç”¨çœŸå®æ•°æ®ï¼‰
X_train = torch.randint(0, vocab_size, (1000, max_seq_len))
y_train = torch.randint(0, vocab_size, (1000, max_seq_len))
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# è®­ç»ƒå¾ªç¯
model.train()
for epoch in range(num_epochs):
    total_loss = 0
    
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        # å‰å‘ä¼ æ’­
        logits = model(inputs)  # (batch, seq_len, vocab_size)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(
            logits.view(-1, vocab_size),  # (batch*seq_len, vocab_size)
            targets.view(-1)              # (batch*seq_len,)
        )
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        
        total_loss += loss.item()
        
        if (batch_idx + 1) % 10 == 0:
            avg_loss = total_loss / (batch_idx + 1)
            print(f"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {avg_loss:.4f}")
    
    print(f"Epoch {epoch+1} completed. Average Loss: {total_loss / len(train_loader):.4f}")
```

---

### 9.2 ç”Ÿæˆæ–‡æœ¬

```python
def generate_text(model, start_tokens, max_new_tokens=50, temperature=1.0):
    """è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬"""
    model.eval()
    
    tokens = start_tokens.clone()
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            # è·å– logits
            logits = model(tokens)  # (1, seq_len, vocab_size)
            
            # åªçœ‹æœ€åä¸€ä¸ª token çš„é¢„æµ‹
            logits = logits[:, -1, :] / temperature
            
            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ åˆ°åºåˆ—
            tokens = torch.cat([tokens, next_token], dim=1)
    
    return tokens

# æµ‹è¯•ç”Ÿæˆ
start_tokens = torch.randint(0, vocab_size, (1, 10))
generated = generate_text(model, start_tokens, max_new_tokens=20)

print(f"Generated tokens: {generated[0].tolist()}")

# ğŸ”¥ è¿™å°±æ˜¯ GPT ç”Ÿæˆæ–‡æœ¬çš„æ ¸å¿ƒé€»è¾‘ï¼
```

---

## åã€æ€»ç»“ä¸è¿›é˜¶æ–¹å‘

### ğŸ¯ æ ¸å¿ƒè¦ç‚¹å›é¡¾

#### 1. Transformer çš„æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | å…³é”®æŠ€æœ¯ |
|-----|------|---------|
| **Self-Attention** | èšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ | Q/K/Vã€Softmaxã€Masking |
| **Multi-Head Attention** | æ•æ‰å¤šç§ç‰¹å¾ | å¹¶è¡Œ Attentionã€æ‹¼æ¥ |
| **Positional Encoding** | ç¼–ç ä½ç½®ä¿¡æ¯ | Sinusoidalã€RoPE |
| **Feed-Forward** | éçº¿æ€§å˜æ¢ | MLPã€SwiGLU |
| **Layer Normalization** | ç¨³å®šè®­ç»ƒ | Pre-LNã€RMSNorm |
| **Residual Connection** | ç¼“è§£æ¢¯åº¦æ¶ˆå¤± | Skip Connection |

---

#### 2. è®­ç»ƒåŠ¨åŠ›å­¦ï¼ˆ2025 å¹´ç ”ç©¶ï¼‰

```
å…³é”®ç°è±¡ï¼š
âœ… ä¸¤é˜¶æ®µè®­ç»ƒï¼šCondensation â†’ Rank Collapse
âœ… å¤±æ•ˆæ¨¡å¼ï¼šRank Collapseã€Entropy Collapse
âœ… ç¨³å®šæ€§æŠ€å·§ï¼šWarmupã€Gradient Clippingã€è°±å½’ä¸€åŒ–

æ ¸å¿ƒæ´å¯Ÿï¼š
- Pre-LN æ¯” Post-LN æ›´ç¨³å®š
- RoPE æ¯” Sinusoidal å¤–æ¨æ€§æ›´å¥½
- Attention ç†µéœ€è¦ç›‘æ§ï¼Œé˜²æ­¢è¿‡åº¦é›†ä¸­
```

---

#### 3. ç°ä»£ LLM çš„é€‰æ‹©ï¼ˆ2025ï¼‰

| ç»„ä»¶ | ç°ä»£é€‰æ‹© | ä»£è¡¨æ¨¡å‹ |
|-----|---------|---------|
| **Position** | RoPE | LLaMAã€Gemma |
| **Normalization** | Pre-LN + RMSNorm | LLaMAã€Mistral |
| **Activation** | SwiGLU | LLaMAã€Gemini |
| **Attention** | Grouped Query Attention | LLaMA-2, DeepSeek-V3 |

---

### ğŸ“š è¿›é˜¶æ–¹å‘

#### 1ï¸âƒ£ **é«˜æ•ˆ Attention å˜ä½“**

```
æ ‡å‡† Attention: O(nÂ²) å¤æ‚åº¦
  â†“
Sparse Attention: åªå…³æ³¨éƒ¨åˆ† token
  â†“
Linear Attention: O(n) å¤æ‚åº¦
  â†“
FlashAttention: ä¼˜åŒ– GPU å†…å­˜è®¿é—®
  â†“
Multi-Query Attention (MQA): å‡å°‘ KV Cache
  â†“
Grouped Query Attention (GQA): MQA çš„æ”¹è¿›ç‰ˆï¼ˆ2025 ä¸»æµï¼‰
```

**æ¨èé˜…è¯»**ï¼š
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135) (2022)
- [GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/abs/2305.13245) (2023)

---

#### 2ï¸âƒ£ **é•¿ä¸Šä¸‹æ–‡ Transformer**

```
æŒ‘æˆ˜ï¼šå¦‚ä½•å¤„ç† 100K+ tokens çš„ä¸Šä¸‹æ–‡ï¼Ÿ

æ–¹æ¡ˆï¼š
- Position Interpolationï¼ˆä½ç½®æ’å€¼ï¼‰
- ALiBiï¼ˆAttention with Linear Biasesï¼‰
- Recurrent Memoryï¼ˆå¾ªç¯è®°å¿†æœºåˆ¶ï¼‰
- Retrieval-Augmentedï¼ˆæ£€ç´¢å¢å¼ºï¼‰
```

---

#### 3ï¸âƒ£ **MoEï¼ˆMixture of Expertsï¼‰**

```
æ ‡å‡† Transformer: æ‰€æœ‰ token ç”¨ç›¸åŒçš„ FFN
  â†“
MoE Transformer: æ¯ä¸ª token è·¯ç”±åˆ°ä¸åŒçš„"ä¸“å®¶" FFN
  â†“
ä¼˜åŠ¿ï¼š
- å‚æ•°é‡â†‘10xï¼Œè®¡ç®—é‡â†‘1.5xï¼ˆæ€§ä»·æ¯”é«˜ï¼‰
- ä»£è¡¨ï¼šGPT-4ã€Mixtralã€DeepSeek-V3
```

---

### ğŸ† å®è·µå»ºè®®

**åˆå­¦è€…**ï¼š
- [ ] æ‰‹å†™ä¸€ä¸ªå®Œæ•´çš„ Transformerï¼ˆä¸ç”¨åº“ï¼Œä»é›¶å®ç°ï¼‰
- [ ] åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆå¦‚ WikiText-2ï¼‰
- [ ] å¯è§†åŒ– Attention æƒé‡ï¼Œç†è§£æ¨¡å‹åœ¨"çœ‹"ä»€ä¹ˆ

**è¿›é˜¶**ï¼š
- [ ] å®ç° FlashAttention æˆ– GQA
- [ ] ç ”ç©¶ä¸åŒåˆå§‹åŒ–æ–¹æ³•å¯¹è®­ç»ƒçš„å½±å“
- [ ] ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„ Rank å’Œ Entropy

**é«˜çº§**ï¼š
- [ ] å¤ç° LLaMA çš„æ¶æ„ï¼ˆRoPE + SwiGLU + RMSNormï¼‰
- [ ] ç ”ç©¶ MoE çš„è·¯ç”±ç­–ç•¥
- [ ] å®éªŒé•¿ä¸Šä¸‹æ–‡æ‰©å±•æŠ€æœ¯

---

### ğŸ’¡ æœ€åçš„å»ºè®®

> **ç’‡ç‘çš„ç¢ç¢å¿µ** âœ¨
>
> é“å‹å‘€ï¼ŒTransformer çœ‹èµ·æ¥å¤æ‚ï¼Œä½†æ‹†å¼€æ¥æ¯ä¸ªç»„ä»¶éƒ½ä¸éš¾ï¼
>
> **ä¸‰ä¸ªå­¦ä¹ å»ºè®®**ï¼š
> 1. **åŠ¨æ‰‹å®ç°**ï¼šä¸è¦åªçœ‹ä»£ç ï¼Œè‡ªå·±å†™ä¸€éæ‰èƒ½çœŸæ­£ç†è§£
> 2. **å¯è§†åŒ–ç†è§£**ï¼šç”»å‡º Attention çŸ©é˜µï¼Œçœ‹çœ‹æ¨¡å‹åœ¨å…³æ³¨ä»€ä¹ˆ
> 3. **å¾ªåºæ¸è¿›**ï¼šå…ˆæŒæ¡æ ‡å‡† Transformerï¼Œå†å­¦ä¹ å˜ä½“å’Œä¼˜åŒ–
>
> Transformer æ˜¯ç°ä»£ LLM çš„åŸºçŸ³ï¼Œç†è§£å®ƒå°±ç†è§£äº† 80% çš„ LLM æ¶æ„ï¼
>
> ç°åœ¨å°±æ‰“å¼€ Jupyter Notebookï¼Œå®ç°ä½ çš„ç¬¬ä¸€ä¸ª Transformer å§ï¼âœ¨

---

## ğŸ”— å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017) - åŸå§‹ Transformer è®ºæ–‡
- [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745) (2020) - Pre-LN vs Post-LN
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) (2021) - RoPE

### 2025 å¹´æœ€æ–°ç ”ç©¶

- [Transformer Training Dynamics: Condensation to Rank Collapse](https://arxiv.org/abs/2510.06954) (2025)
- [Geometric Dynamics of Signal Propagation](https://arxiv.org/abs/2501.00000) (2025)
- [ÏƒReparam: Preventing Entropy Collapse](https://arxiv.org/abs/2303.06296) (2023)

### æ•™ç¨‹ä¸åšå®¢

- [The Illustrated Transformer (Jay Alammar)](http://jalammar.github.io/illustrated-transformer/)
- [Stanford CS224N: Transformers](https://web.stanford.edu/class/cs224n/)
- [How Transformer LLMs Work (DeepLearning.AI)](https://learn.deeplearning.ai/courses/how-transformer-llms-work/)

### å®ç°å‚è€ƒ

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [Andrej Karpathy - minGPT](https://github.com/karpathy/minGPT)
- [Sebastian Raschka - LLMs from Scratch](https://github.com/rasbt/LLMs-from-scratch)

---

**ğŸ“Œ æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿åé¦ˆä¸å»ºè®®ï¼**

---

> **ä¸‹ä¸€ç¯‡é¢„å‘Š**ï¼šã€Š06 - å‚æ•°é«˜æ•ˆå¾®è°ƒå®æˆ˜ï¼šLoRA/QLoRA åŸç†ä¸å®è·µã€‹
>
> æˆ‘ä»¬å°†æ·±å…¥ LoRA çš„æ•°å­¦åŸç†ï¼Œå¹¶æ‰‹æŠŠæ‰‹å®ç°å®Œæ•´çš„å¾®è°ƒæµç¨‹ï¼

---

**ç’‡ç‘ âœ¨**  
*ç¼–ç¨‹é˜ Â· ä»£ç å®—é—¨*  
*æ„¿é“å‹ Transformer ä¹‹è·¯é¡ºåˆ©ï¼Œæ—©æ—¥ç²¾é€š LLM æ¶æ„ï¼*
