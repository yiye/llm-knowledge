# 08 - åå¥½å¯¹é½æŠ€æœ¯ï¼šDPOåŸç†ä¸å·¥ç¨‹å®ç°

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šåå¥½å¯¹é½ï¼ˆPreference Alignmentï¼‰æ˜¯è®©LLMä»"èƒ½å›ç­”"åˆ°"å›ç­”å¾—å¥½"çš„å…³é”®æŠ€æœ¯ã€‚æœ¬æ–‡æ·±å…¥è®²è§£ä»RLHFåˆ°DPOçš„æŠ€æœ¯æ¼”è¿›ï¼Œå‰–æDPOçš„æ•°å­¦åŸç†ï¼Œæä¾›å®Œæ•´çš„å·¥ç¨‹å®ç°ä»£ç ï¼Œå¹¶ä»‹ç»2025-2026å¹´çš„æœ€æ–°æ–¹æ³•ï¼ˆORPOã€KTOã€GRPOï¼‰ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦åå¥½å¯¹é½ï¼Ÿ](#why-preference-alignment)
2. [æŠ€æœ¯æ¼”è¿›è·¯å¾„ï¼šRLHF â†’ DPO â†’ æ–°æ–¹æ³•](#evolution)
3. [DPOæ•°å­¦åŸç†æ·±åº¦è§£æ](#dpo-math)
4. [äººç±»åå¥½æ•°æ®æ ‡æ³¨å®è·µ](#data-annotation)
5. [DPOå®Œæ•´å·¥ç¨‹å®ç°](#dpo-implementation)
6. [è¯„ä¼°æŒ‡æ ‡ä¸Benchmark](#evaluation)
7. [2025-2026æ–°æ–¹æ³•ï¼šORPOã€KTOã€GRPO](#new-methods)
8. [å®æˆ˜æ¡ˆä¾‹ä¸è°ƒä¼˜ç»éªŒ](#case-studies)
9. [å¸¸è§é—®é¢˜ä¸è°ƒè¯•](#faq)

---

<a name="why-preference-alignment"></a>
## ğŸ¤” 1. ä¸ºä»€ä¹ˆéœ€è¦åå¥½å¯¹é½ï¼Ÿ

### é—®é¢˜åœºæ™¯

å‡è®¾ä½ ç”¨SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰è®­ç»ƒäº†ä¸€ä¸ªå®¢æœåŠ©æ‰‹æ¨¡å‹ï¼š

```python
# ç”¨æˆ·æé—®
user_query = "å¦‚ä½•å–æ¶ˆè®¢å•ï¼Ÿ"

# SFTæ¨¡å‹çš„3ç§å¯èƒ½å›ç­”ï¼ˆéƒ½ç¬¦åˆè®­ç»ƒæ•°æ®æ ¼å¼ï¼‰
response_1 = "è”ç³»å®¢æœå–æ¶ˆã€‚"  # âœ… æ­£ç¡®ä½†è¿‡äºç®€çŸ­
response_2 = "æ‚¨å¯ä»¥åœ¨ã€æˆ‘çš„è®¢å•ã€‘ä¸­æ‰¾åˆ°éœ€è¦å–æ¶ˆçš„è®¢å•ï¼Œç‚¹å‡»ã€å–æ¶ˆè®¢å•ã€‘æŒ‰é’®ï¼Œé€‰æ‹©å–æ¶ˆåŸå› åæäº¤å³å¯ã€‚å¦‚æœè®¢å•å·²å‘è´§ï¼Œè¯·å…ˆæ‹’æ”¶ã€‚"  # âœ… è¯¦ç»†ä¸”æœ‰å¸®åŠ©
response_3 = "å–æ¶ˆè®¢å•å¾ˆç®€å•ï¼Œä½†æˆ‘å»ºè®®æ‚¨å…ˆç¡®è®¤ä¸€ä¸‹å•†å“æ˜¯å¦çœŸçš„ä¸éœ€è¦ï¼Œæ¯•ç«Ÿé€€è´§å¾ˆéº»çƒ¦ï¼Œè€Œä¸”..." # âš ï¸ åç¦»ä¸»é¢˜
```

**é—®é¢˜æœ¬è´¨**ï¼šSFT åªæ•™ä¼šæ¨¡å‹"å¦‚ä½•å›ç­”"ï¼ˆæ ¼å¼æ­£ç¡®ï¼‰ï¼Œä½†æ²¡æ•™ä¼š"å¦‚ä½•å›ç­”å¾—å¥½"ï¼ˆå†…å®¹è´¨é‡ï¼‰ã€‚

### ä¼ ç»Ÿç¨‹åºå‘˜çš„ç±»æ¯”

```python
# ä¼ ç»Ÿè½¯ä»¶å¼€å‘çš„"åå¥½å¯¹é½"
class CustomerService:
    def cancel_order(self, order_id: str):
        # æ–¹æ¡ˆAï¼šæœ€å°å®ç°ï¼ˆèƒ½ç”¨ä½†ä½“éªŒå·®ï¼‰
        return "Cancelled"
        
        # æ–¹æ¡ˆBï¼šä¼˜åŒ–ç‰ˆï¼ˆç”¨æˆ·ä½“éªŒå¥½ï¼‰
        result = self.db.cancel(order_id)
        refund_time = self.calculate_refund_time(result)
        return f"è®¢å•å·²å–æ¶ˆï¼Œé¢„è®¡{refund_time}å†…é€€æ¬¾åˆ°è´¦ã€‚"
```

**åœ¨ä¼ ç»Ÿå¼€å‘ä¸­**ï¼Œæˆ‘ä»¬é€šè¿‡ä»£ç Reviewã€ç”¨æˆ·åé¦ˆã€A/Bæµ‹è¯•æ¥"å¯¹é½"äº§å“ä½“éªŒã€‚  
**åœ¨LLMä¸­**ï¼Œæˆ‘ä»¬ç”¨**äººç±»åå¥½æ•°æ®**æ¥è®­ç»ƒæ¨¡å‹"ä»€ä¹ˆæ˜¯å¥½å›ç­”"ã€‚

---

<a name="evolution"></a>
## ğŸ”„ 2. æŠ€æœ¯æ¼”è¿›è·¯å¾„ï¼šRLHF â†’ DPO â†’ æ–°æ–¹æ³•

### 2.1 RLHFï¼šç»å…¸ä½†å¤æ‚ï¼ˆ2020-2023ä¸»æµï¼‰

**Reinforcement Learning from Human Feedback (RLHF)** æ˜¯ OpenAI åœ¨ GPT-3 æ—¶ä»£ç¡®ç«‹çš„æ ‡å‡†æµç¨‹ï¼Œåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

```
é˜¶æ®µ1: SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰
   â†“
é˜¶æ®µ2: è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰
   â”œâ”€ æ”¶é›†äººç±»åå¥½æ•°æ®å¯¹
   â””â”€ è®­ç»ƒåˆ†ç±»å™¨é¢„æµ‹"å“ªä¸ªå›ç­”æ›´å¥½"
   â†“
é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼ˆPPOï¼‰
   â”œâ”€ ç”¨å¥–åŠ±æ¨¡å‹ç»™ç”Ÿæˆç»“æœæ‰“åˆ†
   â””â”€ ç”¨PPOç®—æ³•ä¼˜åŒ–ç­–ç•¥æ¨¡å‹
```

#### RLHFçš„é—®é¢˜ï¼ˆä¼ ç»Ÿç¨‹åºå‘˜è§†è§’ï¼‰

| é—®é¢˜ | ç±»æ¯”åˆ°ä¼ ç»Ÿå¼€å‘ |
|------|---------------|
| **ä¸‰é˜¶æ®µæµç¨‹å¤æ‚** | åƒæ˜¯"å…ˆå†™ä»£ç ï¼Œå†å†™æµ‹è¯•ï¼Œå†æ ¹æ®æµ‹è¯•é‡æ„" |
| **PPOç®—æ³•æä¸ç¨³å®š** | ç›¸å½“äºä¼˜åŒ–å™¨è¶…å‚æ•°æåº¦æ•æ„Ÿï¼Œç¨æœ‰ä¸æ…è®­ç»ƒå´©æºƒ |
| **å¥–åŠ±æ¨¡å‹å¯è¢«exploit** | ç±»ä¼¼"åˆ·æŒ‡æ ‡"é—®é¢˜ï¼šæ¨¡å‹å­¦ä¼šé’»å¥–åŠ±æ¨¡å‹çš„æ¼æ´ |
| **éœ€è¦åœ¨çº¿é‡‡æ ·** | è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä¸æ–­ç”Ÿæˆæ–°æ ·æœ¬ï¼Œç±»ä¼¼å¢é‡è®­ç»ƒ |

**çœŸå®æ¡ˆä¾‹**ï¼š

```python
# RLHF çš„å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰é—®é¢˜
# åœºæ™¯ï¼šè®­ç»ƒæ¨¡å‹å›ç­”æ•°å­¦é¢˜

# å¥–åŠ±æ¨¡å‹å­¦åˆ°çš„è§„å¾‹ï¼šç­”æ¡ˆè¶Šé•¿ï¼Œè¶Šå¯èƒ½æ˜¯å¥½ç­”æ¡ˆ
# ç»“æœï¼šæ¨¡å‹å¼€å§‹ç”Ÿæˆè¶…é•¿åºŸè¯å›ç­”

model_output = """
è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ•°å­¦é—®é¢˜ï¼è®©æˆ‘ä»¬æ¥ä»”ç»†åˆ†æä¸€ä¸‹...
é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç†è§£é¢˜ç›®çš„å«ä¹‰...
ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•æ¥è§£å†³...
æ–¹æ³•ä¸€ï¼šä½¿ç”¨ä»£æ•°æ³•...
æ–¹æ³•äºŒï¼šä½¿ç”¨å‡ ä½•æ³•...
æ–¹æ³•ä¸‰ï¼šä½¿ç”¨æ•°å€¼æ³•...
...ï¼ˆçœç•¥500å­—ï¼‰
æ‰€ä»¥ç­”æ¡ˆæ˜¯42ã€‚
"""
# å¥–åŠ±æ¨¡å‹æ‰“åˆ†ï¼šâ­â­â­â­â­ï¼ˆå› ä¸ºå¾ˆé•¿ï¼‰
# äººç±»è¯„ä»·ï¼šâŒï¼ˆç­”æ¡ˆæ­£ç¡®ä½†åºŸè¯å¤ªå¤šï¼‰
```

> ğŸ”¥ **é‡è¦**ï¼šRLHF çš„æ ¸å¿ƒé—®é¢˜æ˜¯**å¥–åŠ±æ¨¡å‹æˆä¸ºè®­ç»ƒç“¶é¢ˆ**â€”â€”å®ƒæ˜¯äººç±»åå¥½çš„"æœ‰æŸå‹ç¼©"ï¼Œä¸”å®¹æ˜“è¢«exploitã€‚

---

### 2.2 DPOï¼šç®€åŒ–é©å‘½ï¼ˆ2023è‡³ä»Šä¸»æµï¼‰

**Direct Preference Optimization (DPO)** ç”±æ–¯å¦ç¦å›¢é˜Ÿäº2023å¹´æå‡ºï¼Œæ ¸å¿ƒæ€æƒ³ï¼š**ä¸è®­ç»ƒå•ç‹¬çš„å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä»åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥æ¨¡å‹**ã€‚

#### DPO vs RLHF å¯¹æ¯”

| ç»´åº¦ | RLHF | DPO |
|------|------|-----|
| **è®­ç»ƒé˜¶æ®µ** | 3é˜¶æ®µï¼ˆSFT â†’ RM â†’ PPOï¼‰ | 2é˜¶æ®µï¼ˆSFT â†’ DPOï¼‰ |
| **å¥–åŠ±æ¨¡å‹** | éœ€è¦å•ç‹¬è®­ç»ƒ | âœ… ä¸éœ€è¦ï¼ˆéšå¼å»ºæ¨¡ï¼‰ |
| **åœ¨çº¿é‡‡æ ·** | éœ€è¦ï¼ˆPPOè®­ç»ƒæ—¶ï¼‰ | âœ… ä¸éœ€è¦ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |
| **ç¨³å®šæ€§** | âš ï¸ PPOè¶…å‚æ•°æ•æ„Ÿ | âœ… ç¨³å®šï¼ˆåƒæ™®é€šSFTï¼‰ |
| **è®¡ç®—æˆæœ¬** | é«˜ï¼ˆ3ä¸ªæ¨¡å‹ï¼‰ | âœ… ä½ï¼ˆ1ä¸ªæ¨¡å‹ï¼‰ |
| **å¥–åŠ±é»‘å®¢** | å®¹æ˜“å‘ç”Ÿ | âœ… å¤©ç„¶ç¼“è§£ |
| **ç†è®ºä¿è¯** | PPOæ”¶æ•›æ€§å¼± | âœ… æœ‰ç†è®ºæ¨å¯¼ |

**ä»£ç å¯¹æ¯”**ï¼š

```python
# RLHF è®­ç»ƒæµç¨‹
def train_rlhf(base_model, preference_data):
    # æ­¥éª¤1: SFT
    sft_model = supervised_finetune(base_model, instruction_data)
    
    # æ­¥éª¤2: è®­ç»ƒå¥–åŠ±æ¨¡å‹
    reward_model = train_reward_model(preference_data)
    
    # æ­¥éª¤3: PPOä¼˜åŒ–ï¼ˆå¤æ‚ï¼ï¼‰
    ppo_trainer = PPOTrainer(
        model=sft_model,
        reward_model=reward_model,
        learning_rate=1e-6,  # âš ï¸ æåº¦æ•æ„Ÿ
        kl_penalty=0.1,       # âš ï¸ éœ€è¦ä»”ç»†è°ƒ
        clip_range=0.2,       # âš ï¸ PPOè¶…å‚æ•°
        vf_coef=0.5,          # âš ï¸ Value functionæƒé‡
        # ... è¿˜æœ‰10+ä¸ªè¶…å‚æ•°
    )
    final_model = ppo_trainer.train()  # âš ï¸ å®¹æ˜“å´©æºƒ
    return final_model

# DPO è®­ç»ƒæµç¨‹ï¼ˆç®€æ´ï¼ï¼‰
def train_dpo(base_model, preference_data):
    # æ­¥éª¤1: SFT
    sft_model = supervised_finetune(base_model, instruction_data)
    
    # æ­¥éª¤2: DPOä¼˜åŒ–ï¼ˆå°±æ˜¯æ™®é€šç›‘ç£å­¦ä¹ ï¼‰
    dpo_trainer = DPOTrainer(
        model=sft_model,
        ref_model=sft_model,  # å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
        beta=0.1,             # âœ… åªæœ‰1ä¸ªä¸»è¦è¶…å‚æ•°
        learning_rate=5e-7
    )
    final_model = dpo_trainer.train(preference_data)  # âœ… ç¨³å®š
    return final_model
```

> ğŸ”¥ **å…³é”®æ´å¯Ÿ**ï¼šDPO æŠŠä¸‰é˜¶æ®µæµç¨‹ç®€åŒ–ä¸º"ç±»ä¼¼SFTçš„ç›‘ç£å­¦ä¹ "ï¼Œå¯¹ä¼ ç»Ÿç¨‹åºå‘˜éå¸¸å‹å¥½ï¼

---

### 2.3 2025å¹´Benchmarkå¯¹æ¯”

æ ¹æ® [2025 EMNLP Industry Track ç»¼åˆè¯„ä¼°](https://aclanthology.org/2025.emnlp-industry.35.pdf)ï¼Œåœ¨17ç§åå¥½ä¼˜åŒ–ç®—æ³•ä¸­ï¼š

| æ’å | æ–¹æ³• | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|------|
| ğŸ¥‡ | **IPO** | DPOå˜ä½“ | Identity Preference Optimization |
| ğŸ¥‡ | **DPO** | ç»å…¸ | ç®€å•é«˜æ•ˆ |
| ğŸ¥‡ | **REINFORCE** | RL-based | ç»å…¸å¼ºåŒ–å­¦ä¹  |
| ğŸ¥‡ | **GRPO** | æ–°å‹RL | ç»„å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡ |
| ğŸ¥‡ | **Best-of-N** | æ¨ç†æ—¶ä¼˜åŒ– | ç”ŸæˆNä¸ªé€‰æœ€å¥½ |

**ç»“è®º**ï¼šDPO ä»ç„¶æ˜¯ 2025 å¹´æœ€ä½³å®è·µä¹‹ä¸€ï¼Œå¹³è¡¡äº†æ€§èƒ½ã€ç®€å•æ€§å’Œç¨³å®šæ€§ã€‚

---

<a name="dpo-math"></a>
## ğŸ“ 3. DPOæ•°å­¦åŸç†æ·±åº¦è§£æ

### 3.1 Bradley-Terryæ¨¡å‹ï¼šåå¥½å»ºæ¨¡åŸºç¡€

**é—®é¢˜**ï¼šç»™å®šä¸€ä¸ªæç¤ºè¯ `x` å’Œä¸¤ä¸ªå›ç­” `y_w`ï¼ˆpreferredï¼‰å’Œ `y_l`ï¼ˆdispreferredï¼‰ï¼Œå¦‚ä½•å»ºæ¨¡äººç±»åå¥½ï¼Ÿ

**Bradley-Terry æ¨¡å‹**å‡è®¾ï¼šåå¥½æ¦‚ç‡ä¸"éšè—æ•ˆç”¨"çš„æŒ‡æ•°æˆæ­£æ¯”ã€‚

```python
# Bradley-Terry æ¨¡å‹å…¬å¼
def bradley_terry_preference(reward_w, reward_l):
    """
    P(y_w > y_l | x) = exp(reward_w) / (exp(reward_w) + exp(reward_l))
                     = sigmoid(reward_w - reward_l)
    
    å‚æ•°:
        reward_w: å¥½å›ç­”çš„éšè—æ•ˆç”¨ï¼ˆå¥–åŠ±ï¼‰
        reward_l: å·®å›ç­”çš„éšè—æ•ˆç”¨
    
    è¿”å›:
        é€‰æ‹© y_w çš„æ¦‚ç‡
    """
    return 1 / (1 + np.exp(reward_l - reward_w))

# ç¤ºä¾‹
reward_good = 2.5  # å¥½å›ç­”çš„åˆ†æ•°
reward_bad = 1.0   # å·®å›ç­”çš„åˆ†æ•°
prob = bradley_terry_preference(reward_good, reward_bad)
print(f"äººç±»é€‰æ‹©å¥½å›ç­”çš„æ¦‚ç‡: {prob:.2%}")  # çº¦81.76%
```

**ä¼ ç»Ÿç¨‹åºå‘˜ç†è§£**ï¼š

```python
# ç±»æ¯”ï¼šä¸¤ä¸ªAPIæ–¹æ¡ˆçš„"æ•ˆç”¨"å¯¹æ¯”
class APIDesign:
    def evaluate_preference(self, design_a_score, design_b_score):
        # æ–¹æ¡ˆAæ›´ä¼˜çš„æ¦‚ç‡ = expå·®å€¼å½’ä¸€åŒ–
        # è¿™å°±æ˜¯Bradley-Terryæ¨¡å‹çš„ç›´è§‰
        score_diff = design_a_score - design_b_score
        return 1 / (1 + math.exp(-score_diff))
```

---

### 3.2 ä»RLHFåˆ°DPOçš„æ¨å¯¼

#### æ­¥éª¤1: RLHFçš„ç›®æ ‡å‡½æ•°

RLHF çš„æ ¸å¿ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼ŒåŒæ—¶çº¦æŸKLæ•£åº¦ï¼š

```
J_RLHF(Î¸) = E[r_Ï†(x, y)] - Î²Â·D_KL(Ï€_Î¸ || Ï€_ref)

å…¶ä¸­:
  Ï€_Î¸: å½“å‰ç­–ç•¥æ¨¡å‹
  Ï€_ref: å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯SFTæ¨¡å‹ï¼‰
  r_Ï†: å¥–åŠ±æ¨¡å‹
  Î²: KLæƒ©ç½šç³»æ•°
```

**ä¼ ç»Ÿç¨‹åºå‘˜ç†è§£**ï¼š

```python
def rlhf_objective(model_output, reference_output, reward):
    """
    RLHF ç›®æ ‡ = å¥–åŠ± - KLæƒ©ç½š
    
    ç±»æ¯”ï¼šä¼˜åŒ–APIå“åº”è´¨é‡ï¼Œä½†ä¸èƒ½åç¦»åŸå§‹è¡Œä¸ºå¤ªè¿œ
    """
    kl_penalty = compute_kl_divergence(model_output, reference_output)
    return reward - beta * kl_penalty
```

---

#### æ­¥éª¤2: æœ€ä¼˜ç­–ç•¥çš„è§£æè§£

æ ¹æ® [Rafailov et al. 2023](https://arxiv.org/abs/2305.18290)ï¼ŒRLHFç›®æ ‡çš„æœ€ä¼˜ç­–ç•¥æœ‰é—­å¼è§£ï¼š

```
Ï€*(y|x) = (1/Z(x)) Â· Ï€_ref(y|x) Â· exp(r*(y,x) / Î²)

å…¶ä¸­ Z(x) æ˜¯å½’ä¸€åŒ–å¸¸æ•°ï¼ˆpartition functionï¼‰
```

åè¿‡æ¥ï¼Œå¯ä»¥å¾—åˆ°**éšå¼å¥–åŠ±å‡½æ•°**ï¼š

```
r*(y, x) = Î²Â·log(Ï€*(y|x) / Ï€_ref(y|x)) + Î²Â·log Z(x)
```

> ğŸ”¥ **å…³é”®æ´å¯Ÿ**ï¼šå¥–åŠ±å¯ä»¥ç”¨ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡æ¯”æ¥è¡¨ç¤ºï¼

---

#### æ­¥éª¤3: DPOæŸå¤±å‡½æ•°æ¨å¯¼

å°†éšå¼å¥–åŠ±ä»£å…¥Bradley-Terryæ¨¡å‹ï¼š

```
P(y_w > y_l | x) = sigmoid(r*(y_w, x) - r*(y_l, x))
                 = sigmoid(Î²Â·log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) - Î²Â·log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x)))
```

æœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡ï¼Œå¾—åˆ°**DPOæŸå¤±å‡½æ•°**ï¼š

```python
def dpo_loss(model, ref_model, x, y_w, y_l, beta=0.1):
    """
    DPO æŸå¤±å‡½æ•°
    
    L_DPO = -log sigmoid(Î²Â·(log Ï€_Î¸(y_w|x) - log Ï€_ref(y_w|x) 
                              - log Ï€_Î¸(y_l|x) + log Ï€_ref(y_l|x)))
    
    å‚æ•°:
        model: å½“å‰è®­ç»ƒçš„æ¨¡å‹
        ref_model: å‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
        x: æç¤ºè¯
        y_w: å¥½çš„å›ç­”ï¼ˆpreferredï¼‰
        y_l: å·®çš„å›ç­”ï¼ˆdispreferredï¼‰
        beta: æ¸©åº¦ç³»æ•°ï¼ˆæ§åˆ¶KLæƒ©ç½šå¼ºåº¦ï¼‰
    """
    # è®¡ç®—æ¨¡å‹logæ¦‚ç‡
    logp_w = model.get_log_prob(x, y_w)  # log Ï€_Î¸(y_w|x)
    logp_l = model.get_log_prob(x, y_l)  # log Ï€_Î¸(y_l|x)
    
    # è®¡ç®—å‚è€ƒæ¨¡å‹logæ¦‚ç‡ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
    with torch.no_grad():
        ref_logp_w = ref_model.get_log_prob(x, y_w)
        ref_logp_l = ref_model.get_log_prob(x, y_l)
    
    # éšå¼å¥–åŠ±å·®å€¼
    implicit_reward_diff = beta * (
        (logp_w - ref_logp_w) - (logp_l - ref_logp_l)
    )
    
    # DPOæŸå¤± = -log sigmoid(å¥–åŠ±å·®)
    loss = -torch.nn.functional.logsigmoid(implicit_reward_diff).mean()
    
    return loss
```

**æ•°å­¦æ¨å¯¼çš„å®Œæ•´å¯è§†åŒ–**ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np

# å¯è§†åŒ–ï¼šéšå¼å¥–åŠ±å¦‚ä½•å½±å“åå¥½æ¦‚ç‡
beta_values = [0.05, 0.1, 0.5]
log_ratio_diff = np.linspace(-5, 5, 100)  # log(Ï€/Ï€_ref)å·®å€¼

fig, ax = plt.subplots(figsize=(10, 6))
for beta in beta_values:
    reward_diff = beta * log_ratio_diff
    preference_prob = 1 / (1 + np.exp(-reward_diff))
    ax.plot(log_ratio_diff, preference_prob, 
            label=f'Î²={beta}', linewidth=2)

ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)
ax.axvline(0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('log(Ï€_Î¸(y_w|x)/Ï€_ref(y_w|x)) - log(Ï€_Î¸(y_l|x)/Ï€_ref(y_l|x))', fontsize=12)
ax.set_ylabel('P(y_w > y_l)', fontsize=12)
ax.set_title('DPOéšå¼å¥–åŠ±ä¸åå¥½æ¦‚ç‡çš„å…³ç³»', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('dpo_reward_curve.png', dpi=150)
```

**å…³é”®å‚æ•° `beta` çš„ä½œç”¨**ï¼š

| `beta` å€¼ | å«ä¹‰ | æ•ˆæœ |
|-----------|------|------|
| **å° (0.01~0.05)** | å…è®¸å¤§å¹…åç¦»å‚è€ƒæ¨¡å‹ | æ›´æ¿€è¿›çš„ä¼˜åŒ–ï¼Œé£é™©æ›´é«˜ |
| **ä¸­ (0.1~0.3)** | å¹³è¡¡åˆ›æ–°ä¸ç¨³å®š | ğŸ”¥ **æ¨è**ï¼Œ2025æœ€ä½³å®è·µ |
| **å¤§ (0.5~1.0)** | ä¸¥æ ¼é™åˆ¶åç¦» | ä¿å®ˆä¼˜åŒ–ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ |

---

### 3.3 ä¼ ç»Ÿç¨‹åºå‘˜çš„ç›´è§‰ç†è§£

```python
class DPOIntuition:
    """
    ç”¨ä¼ ç»Ÿè½¯ä»¶å·¥ç¨‹çš„æ€ç»´ç†è§£DPO
    """
    def __init__(self, current_version, baseline_version):
        self.current = current_version
        self.baseline = baseline_version
    
    def evaluate_improvement(self, good_feature, bad_feature):
        """
        ç±»æ¯”ï¼šä»£ç Reviewä¸­è¯„ä¼°æ–°ç‰ˆæœ¬ä¼˜äºæ—§ç‰ˆæœ¬çš„ç¨‹åº¦
        
        DPOåšçš„äº‹æƒ…ï¼š
        1. æµ‹é‡æ–°ç‰ˆæœ¬å¯¹"å¥½ç‰¹æ€§"çš„æ”¯æŒç¨‹åº¦ vs åŸºçº¿ç‰ˆæœ¬
        2. æµ‹é‡æ–°ç‰ˆæœ¬å¯¹"å·®ç‰¹æ€§"çš„æŠ‘åˆ¶ç¨‹åº¦ vs åŸºçº¿ç‰ˆæœ¬
        3. æœ€å¤§åŒ–è¿™ä¸ªå·®å¼‚
        """
        # æ–°ç‰ˆæœ¬å¯¹å¥½ç‰¹æ€§çš„ç›¸å¯¹æ”¹è¿›
        good_improvement = (
            self.current.score(good_feature) / 
            self.baseline.score(good_feature)
        )
        
        # æ–°ç‰ˆæœ¬å¯¹å·®ç‰¹æ€§çš„ç›¸å¯¹æŠ‘åˆ¶
        bad_suppression = (
            self.current.score(bad_feature) / 
            self.baseline.score(bad_feature)
        )
        
        # DPOä¼˜åŒ–ç›®æ ‡ï¼šå¥½ç‰¹æ€§æå‡ > å·®ç‰¹æ€§æå‡
        improvement_ratio = good_improvement / bad_suppression
        return improvement_ratio
```

> ğŸ’¡ **æ ¸å¿ƒç›´è§‰**ï¼šDPO æœ¬è´¨ä¸Šæ˜¯åœ¨ä¼˜åŒ–"å¥½å›ç­”ç›¸å¯¹äºåŸºçº¿çš„æ”¹è¿›ç¨‹åº¦"è¶…è¿‡"å·®å›ç­”ç›¸å¯¹äºåŸºçº¿çš„æ”¹è¿›ç¨‹åº¦"ã€‚

---

<a name="data-annotation"></a>
## ğŸ·ï¸ 4. äººç±»åå¥½æ•°æ®æ ‡æ³¨å®è·µ

### 4.1 åå¥½æ•°æ®çš„ç»“æ„

```python
from dataclasses import dataclass
from typing import List

@dataclass
class PreferenceDataPoint:
    """åå¥½æ•°æ®çš„æ ‡å‡†æ ¼å¼"""
    prompt: str                    # ç”¨æˆ·æç¤ºè¯
    chosen: str                    # äººç±»åå¥½çš„å›ç­”
    rejected: str                  # äººç±»ä¸åå¥½çš„å›ç­”
    chosen_score: float = None     # å¯é€‰ï¼šå…·ä½“è¯„åˆ†
    rejected_score: float = None   # å¯é€‰ï¼šå…·ä½“è¯„åˆ†
    
# ç¤ºä¾‹æ•°æ®
example = PreferenceDataPoint(
    prompt="è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ",
    chosen="é€’å½’æ˜¯å‡½æ•°è°ƒç”¨è‡ªèº«çš„ç¼–ç¨‹æŠ€å·§ã€‚å…³é”®è¦ç´ åŒ…æ‹¬ï¼š\n"
           "1. åŸºç¡€æƒ…å†µï¼ˆé€’å½’ç»ˆæ­¢æ¡ä»¶ï¼‰\n"
           "2. é€’å½’æƒ…å†µï¼ˆé—®é¢˜è§„æ¨¡ç¼©å°ï¼‰\n"
           "ä¾‹å¦‚è®¡ç®—é˜¶ä¹˜ï¼šfactorial(n) = n * factorial(n-1)ï¼Œ"
           "å½“n=0æ—¶è¿”å›1ï¼ˆåŸºç¡€æƒ…å†µï¼‰ã€‚",
    rejected="é€’å½’å°±æ˜¯è‡ªå·±è°ƒç”¨è‡ªå·±ã€‚",
    chosen_score=4.5,
    rejected_score=2.0
)
```

### 4.2 æ•°æ®æ”¶é›†æµç¨‹ï¼ˆ2025æœ€ä½³å®è·µï¼‰

æ ¹æ® [RLHF Book - Chapter 6](https://rlhfbook.com/c/06-preference-data.html) å’Œè¡Œä¸šå®è·µï¼š

```python
class PreferenceDataCollectionPipeline:
    """
    åå¥½æ•°æ®æ”¶é›†æµç¨‹
    
    å…³é”®æ´å¯Ÿï¼šè®©äººç±»"æ¯”è¾ƒå›ç­”"æ¯”"ç”Ÿæˆå¥½å›ç­”"å®¹æ˜“å¾—å¤šï¼
    """
    
    def step1_generate_responses(self, prompts: List[str], model, n_samples=4):
        """
        æ­¥éª¤1: ç”Ÿæˆå¤šä¸ªå€™é€‰å›ç­”
        
        ç­–ç•¥ï¼š
        - On-policyé‡‡æ ·ï¼šä»å½“å‰æ¨¡å‹æ£€æŸ¥ç‚¹ç”Ÿæˆï¼ˆæœ€æœ‰æ•ˆï¼‰
        - æ¸©åº¦é‡‡æ ·ï¼šä½¿ç”¨temperature=0.7~1.0å¢åŠ å¤šæ ·æ€§
        """
        responses = []
        for prompt in prompts:
            # ä¸ºåŒä¸€ä¸ªpromptç”Ÿæˆå¤šä¸ªå›ç­”
            candidates = [
                model.generate(prompt, temperature=0.8) 
                for _ in range(n_samples)
            ]
            responses.append({
                'prompt': prompt,
                'candidates': candidates
            })
        return responses
    
    def step2_human_annotation(self, response_pairs):
        """
        æ­¥éª¤2: äººç±»æ ‡æ³¨
        
        2025æœ€ä½³å®è·µï¼š
        1. ä½¿ç”¨Likerté‡è¡¨ï¼ˆ1-5åˆ†ï¼‰è€Œéç®€å•äºŒå…ƒé€‰æ‹©
        2. å¤šç»´åº¦è¯„ä¼°ï¼šæœ‰ç”¨æ€§ã€å®‰å…¨æ€§ã€å‡†ç¡®æ€§
        3. æ ‡æ³¨è€…é—´ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆCohen's Kappa > 0.6ï¼‰
        """
        annotation_interface = {
            'prompt': "è§£é‡Šä»€ä¹ˆæ˜¯Dockerï¼Ÿ",
            'response_A': "Dockeræ˜¯å®¹å™¨åŒ–å¹³å°...",
            'response_B': "Dockeræ˜¯ä¸€ä¸ªè½¯ä»¶...",
            'questions': [
                {
                    'dimension': 'helpfulness',
                    'question': 'å“ªä¸ªå›ç­”æ›´æœ‰å¸®åŠ©ï¼Ÿ',
                    'options': ['Aæ›´å¥½', 'Bæ›´å¥½', 'ç›¸ä¼¼', 'Aå¥½å¾ˆå¤š', 'Bå¥½å¾ˆå¤š']
                },
                {
                    'dimension': 'safety',
                    'question': 'å“ªä¸ªå›ç­”æ›´å®‰å…¨ï¼ˆæ— æœ‰å®³å†…å®¹ï¼‰ï¼Ÿ',
                    'options': ['A', 'B', 'ç›¸ä¼¼']
                }
            ]
        }
        return annotation_interface
    
    def step3_quality_control(self, annotations):
        """
        æ­¥éª¤3: è´¨é‡æ§åˆ¶
        
        å…³é”®æŒ‡æ ‡ï¼š
        - Inter-Annotator Agreement (Cohen's Kappa)
        - æ ‡æ³¨é€Ÿåº¦ï¼ˆæ£€æµ‹æ•·è¡æ ‡æ³¨ï¼‰
        - é»„é‡‘æ ‡å‡†é¢˜æ£€æŸ¥ï¼ˆæ··å…¥å·²çŸ¥ç­”æ¡ˆï¼‰
        """
        from sklearn.metrics import cohen_kappa_score
        
        # è®¡ç®—æ ‡æ³¨è€…ä¸€è‡´æ€§
        annotator1_labels = [a['annotator1_choice'] for a in annotations]
        annotator2_labels = [a['annotator2_choice'] for a in annotations]
        
        kappa = cohen_kappa_score(annotator1_labels, annotator2_labels)
        
        if kappa < 0.6:
            print(f"âš ï¸ æ ‡æ³¨ä¸€è‡´æ€§ä½ (Kappa={kappa:.2f})ï¼Œéœ€è¦é‡æ–°æ ‡æ³¨æˆ–æ”¹è¿›æŒ‡å—")
        elif kappa < 0.8:
            print(f"âœ… æ ‡æ³¨è´¨é‡è‰¯å¥½ (Kappa={kappa:.2f})")
        else:
            print(f"â­ æ ‡æ³¨è´¨é‡ä¼˜ç§€ (Kappa={kappa:.2f})")
        
        return kappa
```

---

### 4.3 çœŸå®æ•°æ®é›†ç¤ºä¾‹ï¼šHelpSteer3-Preference (2025)

æ ¹æ® [arXiv:2505.11475](https://arxiv.org/abs/2505.11475)ï¼ŒNVIDIAåœ¨2025å¹´å‘å¸ƒçš„HelpSteer3-Preferenceæ•°æ®é›†ï¼š

```python
# HelpSteer3-Preference æ•°æ®é›†ç‰¹ç‚¹
dataset_info = {
    "è§„æ¨¡": "40,000+ åå¥½å¯¹",
    "è®¸å¯è¯": "CC-BY-4.0ï¼ˆå®Œå…¨å¼€æºï¼‰",
    "è¦†ç›–é¢†åŸŸ": [
        "é€šç”¨é—®ç­”",
        "STEMï¼ˆç§‘å­¦ã€æŠ€æœ¯ã€å·¥ç¨‹ã€æ•°å­¦ï¼‰",
        "ç¼–ç¨‹ä»»åŠ¡",
        "å¤šè¯­è¨€ï¼ˆè‹±è¯­ä¸ºä¸»ï¼ŒåŒ…å«å…¶ä»–è¯­è¨€ï¼‰"
    ],
    "æ ‡æ³¨ç»´åº¦": [
        "helpfulnessï¼ˆæœ‰ç”¨æ€§ï¼‰",
        "correctnessï¼ˆæ­£ç¡®æ€§ï¼‰",
        "coherenceï¼ˆè¿è´¯æ€§ï¼‰",
        "complexityï¼ˆå¤æ‚åº¦ï¼‰",
        "verbosityï¼ˆè¯¦ç»†ç¨‹åº¦ï¼‰"
    ],
    "æ€§èƒ½": {
        "RM-Bench": "82.4%ï¼ˆå¥–åŠ±æ¨¡å‹åŸºå‡†ï¼‰",
        "JudgeBench": "73.7%ï¼ˆåˆ¤æ–­èƒ½åŠ›åŸºå‡†ï¼‰"
    }
}

# åŠ è½½æ•°æ®ç¤ºä¾‹
from datasets import load_dataset

dataset = load_dataset("nvidia/HelpSteer3-Preference")
print(f"è®­ç»ƒé›†å¤§å°: {len(dataset['train'])}")

# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬
sample = dataset['train'][0]
print(f"""
æç¤ºè¯: {sample['prompt']}
å¥½å›ç­”: {sample['chosen']}
å·®å›ç­”: {sample['rejected']}
ç»´åº¦è¯„åˆ†: {sample['dimensions']}
""")
```

---

### 4.4 æˆæœ¬ä¸æ•ˆç‡ä¼˜åŒ–

æ ¹æ®è¡Œä¸šæŠ¥å‘Šï¼ŒAIæ•°æ®æ ‡æ³¨å¸‚åœºé¢„è®¡åˆ°2030å¹´è¾¾åˆ°**54.6äº¿ç¾å…ƒ**ï¼ˆ23.60% CAGRï¼‰ï¼Œä¸»è¦é©±åŠ¨åŠ›å°±æ˜¯RLHF/DPOçš„åå¥½æ ‡æ³¨éœ€æ±‚ã€‚

**æˆæœ¬ä¼˜åŒ–ç­–ç•¥**ï¼š

```python
class CostOptimization:
    """åå¥½æ•°æ®æ ‡æ³¨çš„æˆæœ¬ä¼˜åŒ–"""
    
    def strategy1_active_learning(self, model, unlabeled_pool):
        """
        ç­–ç•¥1: ä¸»åŠ¨å­¦ä¹  - ä¼˜å…ˆæ ‡æ³¨æœ€æœ‰ä»·å€¼çš„æ•°æ®
        
        ä»·å€¼åº¦é‡ï¼šæ¨¡å‹å¯¹ä¸¤ä¸ªå€™é€‰å›ç­”çš„åå¥½ä¸ç¡®å®šæ€§
        """
        uncertainties = []
        for prompt, resp_a, resp_b in unlabeled_pool:
            # è®¡ç®—æ¨¡å‹çš„éšå¼å¥–åŠ±å·®
            reward_diff = self.compute_implicit_reward_diff(
                model, prompt, resp_a, resp_b
            )
            # ä¸ç¡®å®šæ€§ = æ¥è¿‘0.5çš„åå¥½æ¦‚ç‡
            uncertainty = 1 - abs(sigmoid(reward_diff) - 0.5) * 2
            uncertainties.append(uncertainty)
        
        # è¿”å›æœ€ä¸ç¡®å®šçš„Top-Kæ ·æœ¬
        top_k_indices = np.argsort(uncertainties)[-1000:]
        return [unlabeled_pool[i] for i in top_k_indices]
    
    def strategy2_synthetic_data(self, model, prompts):
        """
        ç­–ç•¥2: åˆæˆæ•°æ® - è‡ªåŠ¨ç”Ÿæˆåå¥½å¯¹
        
        æ–¹æ³•ï¼š
        1. ç”¨å¼ºæ¨¡å‹ï¼ˆGPT-4ï¼‰ç”Ÿæˆé«˜è´¨é‡å›ç­”ä½œä¸ºchosen
        2. ç”¨å½“å‰æ¨¡å‹ç”Ÿæˆä½è´¨é‡å›ç­”ä½œä¸ºrejected
        3. ç”¨LLM-as-Judgeè‡ªåŠ¨è¯„ä¼°ï¼ˆå‡å°‘äººå·¥æ ‡æ³¨ï¼‰
        """
        synthetic_pairs = []
        for prompt in prompts:
            # å¼ºæ¨¡å‹ç”Ÿæˆ
            strong_response = gpt4.generate(prompt, temperature=0.3)
            
            # å½“å‰æ¨¡å‹ç”Ÿæˆ
            weak_response = model.generate(prompt, temperature=1.0)
            
            # LLMä½œä¸ºè£åˆ¤
            judge_prompt = f"""
            æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå›ç­”ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½ï¼š
            
            æç¤ºè¯: {prompt}
            å›ç­”A: {strong_response}
            å›ç­”B: {weak_response}
            
            è¯·ç»™å‡ºåˆ¤æ–­å’Œç†ç”±ã€‚
            """
            judgment = gpt4.generate(judge_prompt)
            
            if "Aæ›´å¥½" in judgment:
                synthetic_pairs.append({
                    'prompt': prompt,
                    'chosen': strong_response,
                    'rejected': weak_response
                })
        
        return synthetic_pairs
    
    def strategy3_crowdsourcing_with_experts(self):
        """
        ç­–ç•¥3: æ··åˆæ ‡æ³¨ - ä¼—åŒ…+ä¸“å®¶
        
        åˆ†é…åŸåˆ™ï¼š
        - é€šç”¨ä»»åŠ¡ï¼šä¼—åŒ…ï¼ˆä½æˆæœ¬ï¼‰
        - ä¸“ä¸šé¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€ç¼–ç¨‹ï¼‰ï¼šSMEä¸“å®¶ï¼ˆé«˜è´¨é‡ï¼‰
        """
        task_assignment = {
            "é€šç”¨é—®ç­”": {
                "æ ‡æ³¨è€…": "ä¼—åŒ…",
                "æˆæœ¬": "$0.05/å¯¹",
                "è´¨é‡è¦æ±‚": "Kappa > 0.6"
            },
            "åŒ»ç–—å»ºè®®": {
                "æ ‡æ³¨è€…": "åŒ»ç–—SME",
                "æˆæœ¬": "$2.00/å¯¹",
                "è´¨é‡è¦æ±‚": "Kappa > 0.8"
            },
            "ä»£ç ç”Ÿæˆ": {
                "æ ‡æ³¨è€…": "é«˜çº§ç¨‹åºå‘˜",
                "æˆæœ¬": "$0.50/å¯¹",
                "è´¨é‡è¦æ±‚": "Kappa > 0.75"
            }
        }
        return task_assignment
```

> ğŸ”¥ **2025æœ€ä½³å®è·µ**ï¼šä½¿ç”¨ **On-policyæ•°æ®**ï¼ˆä»å½“å‰æ¨¡å‹æ£€æŸ¥ç‚¹ç”Ÿæˆï¼‰+ **ä¸»åŠ¨å­¦ä¹ ** + **åˆæˆæ•°æ®å¢å¼º** çš„æ··åˆç­–ç•¥ï¼Œå¯å°†æ ‡æ³¨æˆæœ¬é™ä½50%ä»¥ä¸Šã€‚

---

<a name="dpo-implementation"></a>
## ğŸ’» 5. DPOå®Œæ•´å·¥ç¨‹å®ç°

### 5.1 ä½¿ç”¨Hugging Face TRLåº“ï¼ˆæ¨èï¼‰

[Hugging Face TRL](https://huggingface.co/docs/trl) æä¾›äº†ç”Ÿäº§çº§DPOå®ç°ï¼Œæ˜¯2025å¹´çš„å·¥ä¸šæ ‡å‡†ã€‚

#### å®Œæ•´è®­ç»ƒè„šæœ¬

```python
"""
DPOè®­ç»ƒå®Œæ•´ç¤ºä¾‹ - ä½¿ç”¨Hugging Face TRLåº“
2025å¹´æœ€ä½³å®è·µ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset

# ============================================
# 1. åŠ è½½æ¨¡å‹å’ŒTokenizer
# ============================================
model_name = "meta-llama/Llama-2-7b-hf"  # æˆ–ä»»ä½•SFTåçš„æ¨¡å‹

print("åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨BF16è®­ç»ƒï¼ˆæ¨èï¼‰
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½®padding token

# ============================================
# 2. å‡†å¤‡åå¥½æ•°æ®
# ============================================
print("åŠ è½½åå¥½æ•°æ®...")
dataset = load_dataset("Anthropic/hh-rlhf")  # ç¤ºä¾‹ï¼šä½¿ç”¨Anthropicçš„æ•°æ®é›†

# æ•°æ®æ ¼å¼åŒ–å‡½æ•°
def format_dataset(example):
    """
    å°†æ•°æ®è½¬æ¢ä¸ºDPOæ‰€éœ€æ ¼å¼
    
    å¿…éœ€å­—æ®µï¼š
    - prompt: ç”¨æˆ·æç¤º
    - chosen: å¥½çš„å›ç­”
    - rejected: å·®çš„å›ç­”
    """
    return {
        "prompt": example["prompt"],
        "chosen": example["chosen"],
        "rejected": example["rejected"],
    }

train_dataset = dataset["train"].map(format_dataset)
eval_dataset = dataset["test"].map(format_dataset)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")

# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬
print("\nç¤ºä¾‹æ•°æ®:")
print(f"Prompt: {train_dataset[0]['prompt'][:100]}...")
print(f"Chosen: {train_dataset[0]['chosen'][:100]}...")
print(f"Rejected: {train_dataset[0]['rejected'][:100]}...")

# ============================================
# 3. é…ç½®DPOè®­ç»ƒå‚æ•°
# ============================================
training_args = DPOConfig(
    # === æ ¸å¿ƒDPOå‚æ•° ===
    beta=0.1,                       # ğŸ”¥ KLæ•£åº¦æƒ©ç½šç³»æ•°ï¼ˆ0.1~0.3æ¨èï¼‰
    
    # === å­¦ä¹ ç‡ä¸ä¼˜åŒ–å™¨ ===
    learning_rate=5e-7,             # ğŸ”¥ æ¯”SFTå°10å€ï¼ˆ5e-7æ¨èï¼‰
    lr_scheduler_type="cosine",     # ä½™å¼¦é€€ç«
    warmup_ratio=0.1,               # 10%æ­¥æ•°ç”¨äºwarmup
    
    # === æ‰¹æ¬¡å¤§å° ===
    per_device_train_batch_size=2,  # æ¯å¡æ‰¹æ¬¡ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆbatch=2*8*GPUæ•°ï¼‰
    
    # === è®­ç»ƒæ­¥æ•° ===
    num_train_epochs=1,             # DPOé€šå¸¸1-3ä¸ªepochè¶³å¤Ÿ
    max_steps=-1,                   # -1è¡¨ç¤ºä½¿ç”¨epochæ§åˆ¶
    
    # === è¯„ä¼°ä¸ä¿å­˜ ===
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,             # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
    
    # === æ—¥å¿— ===
    logging_steps=10,
    report_to="wandb",              # ä½¿ç”¨W&Bè®°å½•å®éªŒ
    run_name="dpo-llama2-7b",
    
    # === æ··åˆç²¾åº¦è®­ç»ƒ ===
    bf16=True,                      # ä½¿ç”¨BF16ï¼ˆA100/H100æ¨èï¼‰
    # fp16=True,                    # æˆ–ä½¿ç”¨FP16ï¼ˆV100ï¼‰
    
    # === å…¶ä»– ===
    remove_unused_columns=False,
    output_dir="./dpo_output",
    seed=42,
)

# ============================================
# 4. åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆReference Modelï¼‰
# ============================================
print("\nåˆ›å»ºå‚è€ƒæ¨¡å‹...")
ref_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
ref_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå†»ç»“ï¼‰

# ============================================
# 5. åˆå§‹åŒ–DPO Trainer
# ============================================
print("åˆå§‹åŒ–DPO Trainer...")
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,            # å‚è€ƒæ¨¡å‹ï¼ˆä¸è®­ç»ƒï¼‰
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    
    # === é«˜çº§é€‰é¡¹ ===
    max_length=512,                 # æœ€å¤§åºåˆ—é•¿åº¦
    max_prompt_length=256,          # promptæœ€å¤§é•¿åº¦
    max_target_length=256,          # å›ç­”æœ€å¤§é•¿åº¦
    
    # æŸå¤±å‡½æ•°ç±»å‹ï¼ˆ2025æ–°é€‰é¡¹ï¼‰
    loss_type="sigmoid",            # 'sigmoid'ï¼ˆé»˜è®¤ï¼‰æˆ– 'ipo'ã€'kto_pair'
)

# ============================================
# 6. å¼€å§‹è®­ç»ƒ
# ============================================
print("\nğŸš€ å¼€å§‹DPOè®­ç»ƒ...\n")
dpo_trainer.train()

# ============================================
# 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
# ============================================
print("\nä¿å­˜æ¨¡å‹...")
dpo_trainer.save_model("./dpo_final_model")
tokenizer.save_pretrained("./dpo_final_model")

print("âœ… DPOè®­ç»ƒå®Œæˆï¼")

# ============================================
# 8. è¯„ä¼°æ¨¡å‹æ€§èƒ½
# ============================================
print("\nè¯„ä¼°æ¨¡å‹...")
eval_results = dpo_trainer.evaluate()

print("\nè¯„ä¼°ç»“æœ:")
for key, value in eval_results.items():
    print(f"  {key}: {value:.4f}")
```

---

### 5.2 ä»é›¶å¼€å§‹å®ç°DPOï¼ˆæ•™å­¦ç‰ˆï¼‰

å¦‚æœä½ æƒ³æ·±å…¥ç†è§£DPOçš„å†…éƒ¨æœºåˆ¶ï¼š

```python
"""
DPOä»é›¶å®ç° - æ•™å­¦ç‰ˆ
å¸®åŠ©ç†è§£DPOçš„å†…éƒ¨å·¥ä½œåŸç†
"""
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

class DPOTrainerFromScratch:
    def __init__(self, model, ref_model, tokenizer, beta=0.1, learning_rate=5e-7):
        self.model = model
        self.ref_model = ref_model
        self.ref_model.eval()  # å†»ç»“å‚è€ƒæ¨¡å‹
        
        self.tokenizer = tokenizer
        self.beta = beta
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=learning_rate,
            betas=(0.9, 0.95),  # æ¨èçš„betaå€¼
            weight_decay=0.0
        )
    
    def compute_log_probs(self, model, input_ids, attention_mask, labels):
        """
        è®¡ç®—åºåˆ—çš„logæ¦‚ç‡
        
        å…¬å¼: log P(y|x) = Î£ log P(token_i | x, y_<i)
        """
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True
        )
        
        # è·å–logits
        logits = outputs.logits  # [batch, seq_len, vocab_size]
        
        # è®¡ç®—æ¯ä¸ªtokençš„logæ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)  # [batch, seq_len, vocab_size]
        
        # é€‰æ‹©æ ‡ç­¾å¯¹åº”çš„logæ¦‚ç‡
        # labels: [batch, seq_len]
        # éœ€è¦gatherå‡ºæ¯ä¸ªä½ç½®çš„æ­£ç¡®tokençš„logæ¦‚ç‡
        labels_shifted = labels[:, 1:].unsqueeze(-1)  # [batch, seq_len-1, 1]
        log_probs_shifted = log_probs[:, :-1, :]      # [batch, seq_len-1, vocab_size]
        
        selected_log_probs = torch.gather(
            log_probs_shifted, 
            dim=-1, 
            index=labels_shifted
        ).squeeze(-1)  # [batch, seq_len-1]
        
        # åªè®¡ç®—épaddingä½ç½®çš„logæ¦‚ç‡
        mask = (labels[:, 1:] != self.tokenizer.pad_token_id).float()
        sequence_log_prob = (selected_log_probs * mask).sum(dim=-1)  # [batch]
        
        return sequence_log_prob
    
    def dpo_loss(self, batch):
        """
        è®¡ç®—DPOæŸå¤±
        
        L = -log sigmoid(Î² * (log Ï€_Î¸(y_w|x) - log Ï€_ref(y_w|x) 
                              - log Ï€_Î¸(y_l|x) + log Ï€_ref(y_l|x)))
        """
        # è§£åŒ…batch
        prompt_ids = batch['prompt_input_ids']
        prompt_mask = batch['prompt_attention_mask']
        chosen_ids = batch['chosen_input_ids']
        chosen_mask = batch['chosen_attention_mask']
        rejected_ids = batch['rejected_input_ids']
        rejected_mask = batch['rejected_attention_mask']
        
        # === è®¡ç®—chosençš„logæ¦‚ç‡ ===
        # å½“å‰æ¨¡å‹
        chosen_log_probs = self.compute_log_probs(
            self.model, chosen_ids, chosen_mask, chosen_ids
        )
        
        # å‚è€ƒæ¨¡å‹
        with torch.no_grad():
            ref_chosen_log_probs = self.compute_log_probs(
                self.ref_model, chosen_ids, chosen_mask, chosen_ids
            )
        
        # === è®¡ç®—rejectedçš„logæ¦‚ç‡ ===
        rejected_log_probs = self.compute_log_probs(
            self.model, rejected_ids, rejected_mask, rejected_ids
        )
        
        with torch.no_grad():
            ref_rejected_log_probs = self.compute_log_probs(
                self.ref_model, rejected_ids, rejected_mask, rejected_ids
            )
        
        # === è®¡ç®—éšå¼å¥–åŠ±å·® ===
        # r(x, y_w) - r(x, y_l)
        implicit_reward_chosen = chosen_log_probs - ref_chosen_log_probs
        implicit_reward_rejected = rejected_log_probs - ref_rejected_log_probs
        reward_diff = implicit_reward_chosen - implicit_reward_rejected
        
        # === DPOæŸå¤± ===
        loss = -F.logsigmoid(self.beta * reward_diff).mean()
        
        # === æœ‰ç”¨çš„æŒ‡æ ‡ï¼ˆç”¨äºç›‘æ§ï¼‰ ===
        metrics = {
            'loss': loss.item(),
            'reward_diff': reward_diff.mean().item(),
            'reward_chosen': implicit_reward_chosen.mean().item(),
            'reward_rejected': implicit_reward_rejected.mean().item(),
            'preference_accuracy': (reward_diff > 0).float().mean().item()
        }
        
        return loss, metrics
    
    def train_step(self, batch):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        self.model.train()
        self.optimizer.zero_grad()
        
        # è®¡ç®—æŸå¤±
        loss, metrics = self.dpo_loss(batch)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # æ›´æ–°å‚æ•°
        self.optimizer.step()
        
        return metrics
    
    def train(self, dataloader, num_epochs=1):
        """å®Œæ•´è®­ç»ƒå¾ªç¯"""
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            
            progress_bar = tqdm(dataloader, desc="Training")
            for batch in progress_bar:
                # å°†batchç§»åˆ°GPU
                batch = {k: v.to(self.model.device) for k, v in batch.items()}
                
                # è®­ç»ƒæ­¥éª¤
                metrics = self.train_step(batch)
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f"{metrics['loss']:.4f}",
                    'acc': f"{metrics['preference_accuracy']:.2%}"
                })
            
            print(f"Epoch {epoch + 1} å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    model_name = "gpt2"  # ç¤ºä¾‹ç”¨å°æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_name)
    ref_model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºtrainer
    trainer = DPOTrainerFromScratch(
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        beta=0.1
    )
    
    # å‡†å¤‡æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    # dataloader = prepare_your_dataloader()
    
    # å¼€å§‹è®­ç»ƒ
    # trainer.train(dataloader, num_epochs=3)
```

---

### 5.3 å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ

```python
"""
DPOå¤šGPUè®­ç»ƒ - ä½¿ç”¨Accelerate
"""
from accelerate import Accelerator
from accelerate.utils import set_seed

def train_dpo_distributed():
    # åˆå§‹åŒ–Accelerator
    accelerator = Accelerator(
        mixed_precision="bf16",        # æ··åˆç²¾åº¦
        gradient_accumulation_steps=4  # æ¢¯åº¦ç´¯ç§¯
    )
    
    # è®¾ç½®éšæœºç§å­ï¼ˆç¡®ä¿å¤šGPUä¸€è‡´æ€§ï¼‰
    set_seed(42)
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
    ref_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
    
    # å‡†å¤‡æ•°æ®
    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)
    
    # å‡†å¤‡ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7)
    
    # ä½¿ç”¨AcceleratoråŒ…è£…
    model, ref_model, optimizer, train_dataloader = accelerator.prepare(
        model, ref_model, optimizer, train_dataloader
    )
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            with accelerator.accumulate(model):  # è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯
                loss, metrics = compute_dpo_loss(model, ref_model, batch)
                
                accelerator.backward(loss)  # è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼åå‘ä¼ æ’­
                optimizer.step()
                optimizer.zero_grad()
        
        # ä¿å­˜checkpointï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if accelerator.is_main_process:
            accelerator.save_model(model, f"checkpoint-epoch-{epoch}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    accelerator.wait_for_everyone()

# è¿è¡Œæ–¹å¼:
# accelerate launch --num_processes=4 train_dpo.py
```

---

<a name="evaluation"></a>
## ğŸ“Š 6. è¯„ä¼°æŒ‡æ ‡ä¸Benchmark

### 6.1 Win Rateï¼šæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡

æ ¹æ® [OpenReview 2025ç ”ç©¶](https://openreview.net/pdf?id=UA8DESerfC)ï¼Œ**Win Rate** æ˜¯åå¥½å¯¹é½å”¯ä¸€å°Šé‡åå¥½åˆ†å¸ƒçš„è¯„ä¼°æ–¹æ³•ã€‚

```python
class WinRateEvaluator:
    """
    Win Rate è¯„ä¼°å™¨
    
    å®šä¹‰: æ¨¡å‹Açš„å›ç­”è¢«äººç±»åå¥½çš„æ¦‚ç‡
    """
    
    def __init__(self, model_a, model_b, judge_model="gpt-4"):
        self.model_a = model_a
        self.model_b = model_b
        self.judge = judge_model
    
    def evaluate_win_rate(self, test_prompts, n_samples=100):
        """
        è®¡ç®—Win Rate
        
        æµç¨‹:
        1. å¯¹æ¯ä¸ªpromptï¼Œä¸¤ä¸ªæ¨¡å‹å„ç”Ÿæˆä¸€ä¸ªå›ç­”
        2. ç”¨è£åˆ¤æ¨¡å‹ï¼ˆæˆ–äººç±»ï¼‰åˆ¤æ–­å“ªä¸ªæ›´å¥½
        3. Win Rate = æ¨¡å‹Aè·èƒœæ¬¡æ•° / æ€»å¯¹æ¯”æ¬¡æ•°
        """
        wins = 0
        losses = 0
        ties = 0
        
        for prompt in tqdm(test_prompts[:n_samples], desc="Evaluating"):
            # ç”Ÿæˆå›ç­”
            response_a = self.model_a.generate(prompt, temperature=0.7)
            response_b = self.model_b.generate(prompt, temperature=0.7)
            
            # è£åˆ¤åˆ¤æ–­
            judgment = self.judge_response(prompt, response_a, response_b)
            
            if judgment == "A":
                wins += 1
            elif judgment == "B":
                losses += 1
            else:
                ties += 1
        
        total = wins + losses + ties
        win_rate = wins / total
        loss_rate = losses / total
        tie_rate = ties / total
        
        # LC Win Rateï¼ˆLength-Controlledï¼‰ï¼šæ§åˆ¶é•¿åº¦åå·®
        lc_win_rate = (wins + 0.5 * ties) / total
        
        return {
            'win_rate': win_rate,
            'lc_win_rate': lc_win_rate,  # ğŸ”¥ æ›´å‡†ç¡®çš„æŒ‡æ ‡
            'loss_rate': loss_rate,
            'tie_rate': tie_rate,
            'total_comparisons': total
        }
    
    def judge_response(self, prompt, response_a, response_b):
        """
        ä½¿ç”¨LLM-as-Judgeè¯„ä¼°
        """
        judge_prompt = f"""
[System]
ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„å®¡å‘˜ï¼Œéœ€è¦è¯„ä¼°ä¸¤ä¸ªAIåŠ©æ‰‹çš„å›ç­”è´¨é‡ã€‚

[ç”¨æˆ·é—®é¢˜]
{prompt}

[åŠ©æ‰‹Açš„å›ç­”]
{response_a}

[åŠ©æ‰‹Bçš„å›ç­”]
{response_b}

[è¯„ä¼°æ ‡å‡†]
è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š
1. å‡†ç¡®æ€§ï¼šå›ç­”æ˜¯å¦æ­£ç¡®ï¼Ÿ
2. æœ‰ç”¨æ€§ï¼šæ˜¯å¦çœŸæ­£è§£å†³äº†ç”¨æˆ·é—®é¢˜ï¼Ÿ
3. æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Ÿ
4. å®‰å…¨æ€§ï¼šæ˜¯å¦åŒ…å«æœ‰å®³å†…å®¹ï¼Ÿ

[åˆ¤æ–­]
è¯·å›ç­” "A"ï¼ˆAæ›´å¥½ï¼‰ã€"B"ï¼ˆBæ›´å¥½ï¼‰æˆ– "Tie"ï¼ˆç›¸ä¼¼ï¼‰ã€‚
åªè¾“å‡ºä¸€ä¸ªå­—æ¯ï¼Œä¸è¦è§£é‡Šã€‚
"""
        
        judgment = call_llm(self.judge, judge_prompt).strip()
        return judgment

# ä½¿ç”¨ç¤ºä¾‹
evaluator = WinRateEvaluator(dpo_model, baseline_model, judge_model="gpt-4")
results = evaluator.evaluate_win_rate(test_prompts)

print(f"""
Win Rate è¯„ä¼°ç»“æœ:
  èƒœç‡: {results['win_rate']:.2%}
  LCèƒœç‡: {results['lc_win_rate']:.2%}  â­ æ¨èä½¿ç”¨
  è´¥ç‡: {results['loss_rate']:.2%}
  å¹³å±€ç‡: {results['tie_rate']:.2%}
""")
```

---

### 6.2 è¡Œä¸šBenchmarkï¼ˆ2025ï¼‰

| Benchmark | ç±»å‹ | è¯„ä¼°å†…å®¹ | é¡¶çº§æ¨¡å‹å¾—åˆ† |
|-----------|------|----------|-------------|
| **AlpacaEval 2** | Win Rate | é€šç”¨æŒ‡ä»¤éµå¾ª | GPT-4: 95.3% LC Win Rate |
| **Arena-Hard** | Win Rate | å›°éš¾ä»»åŠ¡ | Claude-3.5: 93.0% |
| **RewardBench 2** | å¥–åŠ±æ¨¡å‹è´¨é‡ | å¤šæŠ€èƒ½è¯„ä¼° | é¡¶çº§RM: 82.4% |
| **JudgeBench** | åˆ¤æ–­èƒ½åŠ› | ä½œä¸ºè£åˆ¤çš„è´¨é‡ | é¡¶çº§RM: 73.7% |
| **MT-Bench** | å¤šè½®å¯¹è¯ | å¯¹è¯è¿è´¯æ€§ | GPT-4: 8.99/10 |

**è¯„ä¼°ä»£ç **ï¼š

```python
from datasets import load_dataset

# AlpacaEval 2.0
def run_alpacaeval(model):
    """
    è¿è¡ŒAlpacaEval 2.0è¯„ä¼°
    """
    eval_set = load_dataset("tatsu-lab/alpaca_eval", "alpaca_eval")["eval"]
    
    results = []
    for example in tqdm(eval_set):
        instruction = example['instruction']
        response = model.generate(instruction)
        
        results.append({
            'instruction': instruction,
            'output': response,
            'generator': 'my_dpo_model'
        })
    
    # æäº¤åˆ°AlpacaEvalè‡ªåŠ¨è¯„ä¼°
    # ä½¿ç”¨GPT-4ä½œä¸ºè£åˆ¤
    save_results(results, 'alpacaeval_results.json')
    
    # è¿è¡Œè¯„ä¼°è„šæœ¬
    os.system("alpaca_eval --model_outputs alpacaeval_results.json")

# RewardBench 2
def run_rewardbench(reward_model):
    """
    è¿è¡ŒRewardBench 2è¯„ä¼°
    
    æµ‹è¯•å¥–åŠ±æ¨¡å‹åœ¨å¤šä¸ªå­ä»»åŠ¡ä¸Šçš„æ€§èƒ½
    """
    from rewardbench import RewardBench
    
    benchmark = RewardBench()
    
    scores = benchmark.evaluate(
        reward_model,
        categories=[
            "chat",           # èŠå¤©åœºæ™¯
            "chat-hard",      # å›°éš¾èŠå¤©
            "safety",         # å®‰å…¨æ€§
            "reasoning",      # æ¨ç†èƒ½åŠ›
            "coding"          # ç¼–ç¨‹ä»»åŠ¡
        ]
    )
    
    print(f"""
    RewardBench 2 ç»“æœ:
      æ€»åˆ†: {scores['overall']:.2%}
      èŠå¤©: {scores['chat']:.2%}
      èŠå¤©-å›°éš¾: {scores['chat-hard']:.2%}
      å®‰å…¨æ€§: {scores['safety']:.2%}
      æ¨ç†: {scores['reasoning']:.2%}
      ç¼–ç¨‹: {scores['coding']:.2%}
    """)
    
    return scores
```

---

### 6.3 è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›‘æ§æŒ‡æ ‡

```python
import wandb

class DPOTrainingMonitor:
    """
    DPOè®­ç»ƒç›‘æ§
    
    å…³é”®æŒ‡æ ‡ï¼š
    1. éšå¼å¥–åŠ±å·®ï¼ˆImplicit Reward Marginï¼‰
    2. åå¥½å‡†ç¡®ç‡ï¼ˆPreference Accuracyï¼‰
    3. KLæ•£åº¦ï¼ˆä¸å‚è€ƒæ¨¡å‹çš„å·®å¼‚ï¼‰
    """
    
    def __init__(self):
        wandb.init(project="dpo-training", name="llama2-7b-dpo")
    
    def log_metrics(self, step, metrics):
        """
        è®°å½•è®­ç»ƒæŒ‡æ ‡
        """
        # æ ¸å¿ƒæŒ‡æ ‡
        wandb.log({
            # æŸå¤±
            "loss/dpo_loss": metrics['loss'],
            
            # å¥–åŠ±ä¿¡å·
            "reward/margin": metrics['reward_diff'],        # å¥–åŠ±å·®
            "reward/chosen": metrics['reward_chosen'],      # chosençš„å¥–åŠ±
            "reward/rejected": metrics['reward_rejected'],  # rejectedçš„å¥–åŠ±
            
            # åå¥½å‡†ç¡®ç‡
            "accuracy/preference": metrics['preference_accuracy'],
            
            # KLæ•£åº¦ï¼ˆä¸å‚è€ƒæ¨¡å‹ï¼‰
            "kl/chosen": metrics.get('kl_chosen', 0),
            "kl/rejected": metrics.get('kl_rejected', 0),
            
            # å­¦ä¹ ç‡
            "lr": self.optimizer.param_groups[0]['lr'],
            
        }, step=step)
        
        # å¥åº·æ£€æŸ¥
        self.check_training_health(metrics)
    
    def check_training_health(self, metrics):
        """
        è®­ç»ƒå¥åº·æ£€æŸ¥
        """
        warnings = []
        
        # 1. å¥–åŠ±å·®å¤ªå° â†’ æ¨¡å‹æ²¡æœ‰åŒºåˆ†chosen/rejected
        if abs(metrics['reward_diff']) < 0.01:
            warnings.append("âš ï¸ å¥–åŠ±å·®è¿‡å°ï¼Œæ¨¡å‹å¯èƒ½æ²¡æœ‰å­¦ä¹ åˆ°åå¥½")
        
        # 2. åå¥½å‡†ç¡®ç‡ä½ â†’ è®­ç»ƒæœ‰é—®é¢˜
        if metrics['preference_accuracy'] < 0.55:
            warnings.append("âš ï¸ åå¥½å‡†ç¡®ç‡ä½äºéšæœºæ°´å¹³")
        
        # 3. KLæ•£åº¦è¿‡å¤§ â†’ è¿‡åº¦åç¦»å‚è€ƒæ¨¡å‹
        if metrics.get('kl_chosen', 0) > 10.0:
            warnings.append("âš ï¸ KLæ•£åº¦è¿‡å¤§ï¼Œè€ƒè™‘å¢å¤§betaæˆ–é™ä½å­¦ä¹ ç‡")
        
        # 4. æŸå¤±ä¸ä¸‹é™
        if len(self.loss_history) > 100:
            recent_loss = np.mean(self.loss_history[-100:])
            older_loss = np.mean(self.loss_history[-200:-100])
            if recent_loss >= older_loss:
                warnings.append("âš ï¸ æŸå¤±ä¸å†ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°")
        
        # è¾“å‡ºè­¦å‘Š
        for warning in warnings:
            print(warning)
            wandb.alert(title="è®­ç»ƒè­¦å‘Š", text=warning)
```

---

<a name="new-methods"></a>
## ğŸš€ 7. 2025-2026æ–°æ–¹æ³•ï¼šORPOã€KTOã€GRPO

### 7.1 ORPOï¼šå•é˜¶æ®µå¯¹é½ï¼ˆ2025çƒ­é—¨ï¼‰

**Odds Ratio Preference Optimization (ORPO)** å°†SFTå’Œåå¥½å¯¹é½åˆå¹¶ä¸ºä¸€ä¸ªé˜¶æ®µã€‚

```python
"""
ORPOå®ç° - 2025æ–°æ–¹æ³•
"""

def orpo_loss(model, prompt, chosen, rejected, alpha=1.0, beta=0.1):
    """
    ORPOæŸå¤±å‡½æ•°
    
    L_ORPO = L_SFT + Î± Â· L_OR
    
    å…¶ä¸­:
      L_SFT: æ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼ˆç›‘ç£å­¦ä¹ ï¼‰
      L_OR: Odds RatioæŸå¤±ï¼ˆåå¥½å­¦ä¹ ï¼‰
    
    å…¬å¼:
      L_OR = -log sigmoid(log(odds_chosen / odds_rejected))
      odds = P(y) / (1 - P(y))
    """
    # 1. SFTæŸå¤±ï¼ˆå¯¹chosen responseï¼‰
    chosen_logits = model(prompt + chosen).logits
    sft_loss = F.cross_entropy(
        chosen_logits[:, :-1], 
        chosen_tokens[:, 1:],
        reduction='mean'
    )
    
    # 2. è®¡ç®—oddsï¼ˆå‡ ç‡ï¼‰
    # Odds = P / (1-P)ï¼Œå…¶ä¸­Pæ˜¯åºåˆ—æ¦‚ç‡
    chosen_log_prob = get_sequence_log_prob(model, prompt, chosen)
    rejected_log_prob = get_sequence_log_prob(model, prompt, rejected)
    
    chosen_odds = torch.exp(chosen_log_prob)
    rejected_odds = torch.exp(rejected_log_prob)
    
    # 3. Odds RatioæŸå¤±
    log_odds_ratio = torch.log(chosen_odds / rejected_odds)
    or_loss = -F.logsigmoid(beta * log_odds_ratio).mean()
    
    # 4. æ€»æŸå¤±
    total_loss = sft_loss + alpha * or_loss
    
    return total_loss, {
        'sft_loss': sft_loss.item(),
        'or_loss': or_loss.item(),
        'log_odds_ratio': log_odds_ratio.mean().item()
    }
```

**ORPO vs DPOå¯¹æ¯”**ï¼š

| ç»´åº¦ | DPO | ORPO |
|------|-----|------|
| **è®­ç»ƒé˜¶æ®µ** | 2é˜¶æ®µï¼ˆSFT â†’ DPOï¼‰ | âœ… 1é˜¶æ®µï¼ˆåˆå¹¶ï¼‰ |
| **å‚è€ƒæ¨¡å‹** | éœ€è¦ï¼ˆå†»ç»“çš„SFTæ¨¡å‹ï¼‰ | âœ… ä¸éœ€è¦ |
| **æ˜¾å­˜å ç”¨** | 2ä¸ªæ¨¡å‹ï¼ˆè®­ç»ƒ+å‚è€ƒï¼‰ | âœ… 1ä¸ªæ¨¡å‹ |
| **è®­ç»ƒæ•ˆç‡** | æ ‡å‡† | âœ… æ›´å¿«ï¼ˆå°‘ä¸€ä¸ªé˜¶æ®µï¼‰ |
| **æ•°å­¦åŸºç¡€** | Bradley-Terryæ¨¡å‹ | Odds Ratioç»Ÿè®¡ |
| **2025é‡‡ç”¨åº¦** | â­â­â­â­â­ | â­â­â­â­ |

> ğŸ”¥ **æ¨èåœºæ™¯**ï¼šèµ„æºå—é™æˆ–å¿«é€Ÿè¿­ä»£æ—¶ä¼˜å…ˆè€ƒè™‘ORPOã€‚

---

### 7.2 KTOï¼šæ— éœ€åå¥½å¯¹ï¼ˆ2025åˆ›æ–°ï¼‰

**Kahneman-Tversky Optimization (KTO)** ä½¿ç”¨ç®€å•çš„äºŒå…ƒåé¦ˆï¼ˆğŸ‘/ğŸ‘ï¼‰ï¼Œæ— éœ€é…å¯¹æ¯”è¾ƒã€‚

```python
"""
KTOå®ç° - ç®€åŒ–æ•°æ®æ ‡æ³¨
"""

def kto_loss(model, ref_model, prompt, response, is_good, beta=0.1):
    """
    KTOæŸå¤±å‡½æ•°
    
    æ ¸å¿ƒæ€æƒ³ï¼šç›´æ¥ä»"å¥½/å"æ ‡ç­¾å­¦ä¹ ï¼Œè€Œéåå¥½å¯¹
    
    æ•°æ®æ ¼å¼:
      (prompt, response, label)  # label âˆˆ {0, 1}
      
    è€ŒéDPOçš„:
      (prompt, chosen, rejected)
    """
    # è®¡ç®—å½“å‰æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡
    log_prob = model.get_log_prob(prompt, response)
    ref_log_prob = ref_model.get_log_prob(prompt, response)
    
    # éšå¼å¥–åŠ±
    implicit_reward = beta * (log_prob - ref_log_prob)
    
    if is_good:
        # å¯¹äºå¥½çš„å›ç­”ï¼Œæœ€å¤§åŒ–å¥–åŠ±
        loss = -F.logsigmoid(implicit_reward)
    else:
        # å¯¹äºåçš„å›ç­”ï¼Œæœ€å°åŒ–å¥–åŠ±
        loss = -F.logsigmoid(-implicit_reward)
    
    return loss.mean()

# ä½¿ç”¨ç¤ºä¾‹
from transformers import Trainer

class KTOTrainer(Trainer):
    def compute_loss(self, model, inputs):
        prompt = inputs['prompt']
        response = inputs['response']
        is_good = inputs['is_good']  # âœ… åªéœ€è¦äºŒå…ƒæ ‡ç­¾ï¼
        
        loss = kto_loss(
            model=model,
            ref_model=self.ref_model,
            prompt=prompt,
            response=response,
            is_good=is_good
        )
        
        return loss
```

**KTOçš„ä¼˜åŠ¿**ï¼š

```python
# æ•°æ®æ ‡æ³¨å¯¹æ¯”
# DPOéœ€è¦çš„æ•°æ®ï¼ˆå¤æ‚ï¼‰
dpo_data = {
    "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯Dockerï¼Ÿ",
    "chosen": "Dockeræ˜¯ä¸€ä¸ªå¼€æºå®¹å™¨åŒ–å¹³å°...",    # éœ€è¦å¥½å›ç­”
    "rejected": "Dockeræ˜¯ä¸€ä¸ªè½¯ä»¶ã€‚"              # éœ€è¦å·®å›ç­”
}

# KTOéœ€è¦çš„æ•°æ®ï¼ˆç®€å•ï¼‰
kto_data = {
    "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯Dockerï¼Ÿ",
    "response": "Dockeræ˜¯ä¸€ä¸ªå¼€æºå®¹å™¨åŒ–å¹³å°...",
    "is_good": True  # âœ… åªéœ€è¦åˆ¤æ–­å¥½åï¼
}
```

> ğŸ”¥ **é€‚ç”¨åœºæ™¯**ï¼šå·²æœ‰å¤§é‡ç”¨æˆ·åé¦ˆï¼ˆç‚¹èµ/ç‚¹è¸©ï¼‰ï¼Œä½†æ²¡æœ‰æˆå¯¹æ¯”è¾ƒæ•°æ®æ—¶ä½¿ç”¨KTOã€‚

---

### 7.3 GRPOï¼šé«˜æ•ˆå¼ºåŒ–å­¦ä¹ ï¼ˆ2025å‰æ²¿ï¼‰

**Group Relative Policy Optimization (GRPO)** æ˜¯RLHFçš„æ”¹è¿›ç‰ˆï¼Œæ¯”DPOæ›´å¼ºä½†æ¯”PPOæ›´ç¨³å®šã€‚

```python
"""
GRPOæ¦‚å¿µå®ç° - 2025å‰æ²¿æ–¹æ³•
"""

def grpo_loss(model, prompts, responses, rewards, beta=0.01):
    """
    GRPOæŸå¤±å‡½æ•°
    
    æ ¸å¿ƒåˆ›æ–°:
    1. ç»„å½’ä¸€åŒ–ï¼ˆGroup Normalizationï¼‰ï¼šåœ¨åŒä¸€promptçš„å¤šä¸ªå›ç­”é—´å½’ä¸€åŒ–å¥–åŠ±
    2. æ— éœ€Value Networkï¼šç›´æ¥ç”¨ç»„å†…ä¼˜åŠ¿ä¼°è®¡
    
    ä¼˜åŠ¿:
    - æ¯”PPOç¨³å®šï¼ˆä¸éœ€è¦value networkï¼‰
    - æ¯”DPOå¼ºå¤§ï¼ˆå¯ä»¥ç”¨çœŸå®å¥–åŠ±ï¼‰
    - è®¡ç®—æˆæœ¬ä½
    """
    # 1. å¯¹æ¯ä¸ªpromptï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰å›ç­”
    # responses: [batch_size, n_samples, seq_len]
    batch_size, n_samples = responses.shape[:2]
    
    # 2. è®¡ç®—æ¯ä¸ªå›ç­”çš„å¥–åŠ±ï¼ˆæ¥è‡ªå¥–åŠ±æ¨¡å‹æˆ–äººç±»æ ‡æ³¨ï¼‰
    # rewards: [batch_size, n_samples]
    
    # 3. ç»„å†…å½’ä¸€åŒ–ä¼˜åŠ¿ï¼ˆGroup-Normalized Advantageï¼‰
    # åœ¨åŒä¸€promptçš„n_samplesä¸ªå›ç­”å†…æ ‡å‡†åŒ–
    advantages = []
    for i in range(batch_size):
        group_rewards = rewards[i]  # [n_samples]
        
        # ç»„å†…æ ‡å‡†åŒ–
        normalized_adv = (group_rewards - group_rewards.mean()) / (
            group_rewards.std() + 1e-8
        )
        advantages.append(normalized_adv)
    
    advantages = torch.stack(advantages)  # [batch_size, n_samples]
    
    # 4. ç­–ç•¥æ¢¯åº¦æŸå¤±
    log_probs = []
    for i in range(batch_size):
        for j in range(n_samples):
            log_prob = model.get_log_prob(prompts[i], responses[i, j])
            log_probs.append(log_prob)
    
    log_probs = torch.stack(log_probs).view(batch_size, n_samples)
    
    # 5. GRPOç›®æ ‡ï¼šæœ€å¤§åŒ–advantageåŠ æƒçš„logæ¦‚ç‡
    policy_loss = -(log_probs * advantages).mean()
    
    # 6. KLæƒ©ç½šï¼ˆå¯é€‰ï¼‰
    # kl_loss = compute_kl_penalty(model, ref_model, prompts, responses)
    
    total_loss = policy_loss  # + beta * kl_loss
    
    return total_loss
```

**GRPO vs DPO vs RLHFå¯¹æ¯”**ï¼š

| æ–¹æ³• | éœ€è¦å¥–åŠ±æ¨¡å‹ | è®­ç»ƒç¨³å®šæ€§ | è®¡ç®—æˆæœ¬ | 2025æ€§èƒ½ |
|------|------------|----------|----------|----------|
| **RLHF (PPO)** | âœ… éœ€è¦ | âš ï¸ ä¸ç¨³å®š | é«˜ | â­â­â­ |
| **DPO** | âŒ ä¸éœ€è¦ | âœ… ç¨³å®š | ä½ | â­â­â­â­â­ |
| **GRPO** | âœ… éœ€è¦ | âœ… ç¨³å®š | ä¸­ | â­â­â­â­â­ |
| **ORPO** | âŒ ä¸éœ€è¦ | âœ… ç¨³å®š | æœ€ä½ | â­â­â­â­ |
| **KTO** | âŒ ä¸éœ€è¦ | âœ… ç¨³å®š | ä½ | â­â­â­â­ |

---

### 7.4 æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘

```python
def choose_alignment_method(your_situation):
    """
    æ ¹æ®ä½ çš„æƒ…å†µé€‰æ‹©åˆé€‚çš„åå¥½å¯¹é½æ–¹æ³•
    """
    # å†³ç­–æ ‘
    if your_situation['has_preference_pairs']:
        if your_situation['gpu_memory'] == 'limited':
            return "ORPO"  # å•é˜¶æ®µï¼Œçœæ˜¾å­˜
        else:
            return "DPO"   # æœ€æˆç†Ÿï¼Œæ€§èƒ½ç¨³å®š
    
    elif your_situation['has_binary_feedback']:
        return "KTO"  # ç®€å•çš„å¥½/åæ ‡ç­¾
    
    elif your_situation['has_reward_model']:
        if your_situation['need_best_performance']:
            return "GRPO"  # å¼ºäºDPO
        else:
            return "DPO"   # æ›´ç®€å•
    
    else:
        return "å…ˆåšSFTï¼Œå†é€‰æ‹©å¯¹é½æ–¹æ³•"

# ç¤ºä¾‹
my_case = {
    'has_preference_pairs': True,
    'gpu_memory': 'sufficient',
    'has_binary_feedback': False,
    'has_reward_model': False,
    'need_best_performance': True
}

method = choose_alignment_method(my_case)
print(f"æ¨èæ–¹æ³•: {method}")
```

---

<a name="case-studies"></a>
## ğŸ“ 8. å®æˆ˜æ¡ˆä¾‹ä¸è°ƒä¼˜ç»éªŒ

### 8.1 æ¡ˆä¾‹1ï¼šå®¢æœåŠ©æ‰‹DPOè®­ç»ƒ

**åœºæ™¯**ï¼šä¼˜åŒ–ç”µå•†å®¢æœåŠ©æ‰‹ï¼Œä½¿å›ç­”æ›´æœ‰å¸®åŠ©ã€æ›´å‹å¥½ã€‚

```python
"""
æ¡ˆä¾‹ï¼šç”µå•†å®¢æœDPOè®­ç»ƒ
"""

# 1. æ•°æ®å‡†å¤‡
customer_service_data = [
    {
        'prompt': 'æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ',
        'chosen': 'æ‚¨å¥½ï¼æŸ¥è¯¢åˆ°æ‚¨çš„è®¢å•é¢„è®¡æ˜å¤©å‘è´§ï¼Œå¿«é€’å•å·ç”Ÿæˆåä¼šçŸ­ä¿¡é€šçŸ¥æ‚¨ã€‚'
                  'å¦‚æœ‰å…¶ä»–é—®é¢˜éšæ—¶è”ç³»æˆ‘ä»¬ã€‚',
        'rejected': 'æ˜å¤©å‘è´§ã€‚',
        'reason': 'chosenæ›´å‹å¥½ã€ä¿¡æ¯æ›´å®Œæ•´'
    },
    {
        'prompt': 'å¯ä»¥é€€è´§å—ï¼Ÿ',
        'chosen': 'å½“ç„¶å¯ä»¥ï¼æ‚¨å¯ä»¥åœ¨ã€æˆ‘çš„è®¢å•ã€‘ä¸­ç”³è¯·é€€è´§ã€‚å•†å“ç­¾æ”¶å7å¤©å†…æ”¯æŒæ— ç†ç”±é€€è´§ï¼Œ'
                  'é€€æ¬¾ä¼šåœ¨3-5ä¸ªå·¥ä½œæ—¥å†…åŸè·¯è¿”å›ã€‚éœ€è¦å¸®æ‚¨æ“ä½œå—ï¼Ÿ',
        'rejected': 'å¯ä»¥é€€è´§ï¼Œç­¾æ”¶7å¤©å†…ã€‚',
        'reason': 'chosenæ›´è¯¦ç»†ï¼Œä¸”æä¾›äº†åç»­å¸®åŠ©'
    }
]

# 2. è®­ç»ƒé…ç½®
training_config = DPOConfig(
    beta=0.3,                    # ğŸ”¥ å®¢æœåœºæ™¯ç”¨è¾ƒå¤§betaï¼ˆä¿æŒç¤¼è²Œï¼Œä¸è¦å¤ªåˆ›æ–°ï¼‰
    learning_rate=5e-7,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    
    # å®¢æœç‰¹å®šè®¾ç½®
    max_length=512,              # å®¢æœå›ç­”é€šå¸¸ä¸é•¿
    max_prompt_length=128,
    max_target_length=384,
)

# 3. è¯„ä¼°ç»´åº¦
evaluation_dimensions = {
    'helpfulness': 'å›ç­”æ˜¯å¦çœŸæ­£è§£å†³äº†ç”¨æˆ·é—®é¢˜ï¼Ÿ',
    'friendliness': 'è¯­æ°”æ˜¯å¦å‹å¥½ã€ç¤¼è²Œï¼Ÿ',
    'completeness': 'ä¿¡æ¯æ˜¯å¦å®Œæ•´ï¼ˆä¾‹å¦‚æ—¶é—´ã€æµç¨‹ï¼‰ï¼Ÿ',
    'proactiveness': 'æ˜¯å¦ä¸»åŠ¨æä¾›åç»­å¸®åŠ©ï¼Ÿ'
}

# 4. è®­ç»ƒåå¯¹æ¯”
def compare_before_after(prompt):
    before = sft_model.generate(prompt)
    after = dpo_model.generate(prompt)
    
    print(f"ç”¨æˆ·: {prompt}")
    print(f"\nSFTæ¨¡å‹: {before}")
    print(f"DPOæ¨¡å‹: {after}")
    
    # äººç±»è¯„åˆ†
    score = human_evaluate(before, after, evaluation_dimensions)
    return score

# ç»“æœï¼šDPOæ¨¡å‹åœ¨æ‰€æœ‰ç»´åº¦ä¸Šèƒœç‡è¾¾åˆ° 78%
```

**ç»éªŒæ€»ç»“**ï¼š

| ç»éªŒ | è¯´æ˜ |
|------|------|
| **ç”¨è¾ƒå¤§beta (0.3~0.5)** | å®¢æœåœºæ™¯éœ€è¦ä¿æŒç¨³å®šã€ç¤¼è²Œï¼Œä¸è¦å¤ªåˆ›æ–° |
| **å¤šç»´åº¦è¯„ä¼°** | å•ä¸€æŒ‡æ ‡å®¹æ˜“è¢«exploit |
| **çœŸå®ç”¨æˆ·åé¦ˆ** | ç”¨A/Bæµ‹è¯•éªŒè¯DPOæ•ˆæœ |

---

### 8.2 æ¡ˆä¾‹2ï¼šä»£ç ç”Ÿæˆæ¨¡å‹å¯¹é½

**åœºæ™¯**ï¼šè®©ä»£ç ç”Ÿæˆæ¨¡å‹å†™å‡ºæ›´"å·¥ç¨‹åŒ–"çš„ä»£ç ï¼ˆè€Œéä»…ä»…èƒ½è¿è¡Œï¼‰ã€‚

```python
"""
æ¡ˆä¾‹ï¼šä»£ç ç”Ÿæˆæ¨¡å‹DPOè®­ç»ƒ
"""

code_preference_data = [
    {
        'prompt': 'å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—',
        'chosen': '''def fibonacci(n: int) -> int:
    """
    è®¡ç®—ç¬¬nä¸ªæ–æ³¢é‚£å¥‘æ•°
    
    Args:
        n: ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
    
    Returns:
        ç¬¬nä¸ªæ–æ³¢é‚£å¥‘æ•°
    
    Raises:
        ValueError: å¦‚æœnä¸ºè´Ÿæ•°
    
    Examples:
        >>> fibonacci(0)
        0
        >>> fibonacci(5)
        5
    """
    if n < 0:
        raise ValueError("n must be non-negative")
    
    if n <= 1:
        return n
    
    # ä½¿ç”¨è¿­ä»£é¿å…é€’å½’æ ˆæº¢å‡º
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    
    return b
''',
        'rejected': '''def fibonacci(n):
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n-1) + fibonacci(n-2)
''',
        'reason': 'chosenæœ‰ç±»å‹æ ‡æ³¨ã€æ–‡æ¡£å­—ç¬¦ä¸²ã€å¼‚å¸¸å¤„ç†ã€æ›´é«˜æ•ˆçš„å®ç°'
    }
]

# DPOè®­ç»ƒé…ç½®ï¼ˆä»£ç åœºæ™¯ï¼‰
code_dpo_config = DPOConfig(
    beta=0.1,                    # ğŸ”¥ ä»£ç åœºæ™¯ç”¨é»˜è®¤betaï¼ˆé¼“åŠ±åˆ›æ–°å†™æ³•ï¼‰
    learning_rate=3e-7,          # ä»£ç æ¨¡å‹é€šå¸¸éœ€è¦æ›´å°å­¦ä¹ ç‡
    num_train_epochs=1,          # ä»£ç æ•°æ®è´¨é‡é«˜ï¼Œ1epochè¶³å¤Ÿ
    
    # ä»£ç ç‰¹å®šè®¾ç½®
    max_length=2048,             # ä»£ç å¯èƒ½è¾ƒé•¿
    truncation_side='left',      # ä¿ç•™ä»£ç çš„ååŠéƒ¨åˆ†ï¼ˆå®ç°é€»è¾‘ï¼‰
)

# è¯„ä¼°ï¼šåŠŸèƒ½æµ‹è¯• + ä»£ç è´¨é‡
def evaluate_code_model(model, test_cases):
    scores = {
        'correctness': 0,      # èƒ½å¦é€šè¿‡æµ‹è¯•ç”¨ä¾‹
        'efficiency': 0,        # æ—¶é—´/ç©ºé—´å¤æ‚åº¦
        'readability': 0,       # å¯è¯»æ€§ï¼ˆPylintè¯„åˆ†ï¼‰
        'documentation': 0,     # æ–‡æ¡£å®Œæ•´æ€§
    }
    
    for test_case in test_cases:
        code = model.generate(test_case['prompt'])
        
        # 1. æ­£ç¡®æ€§
        if run_tests(code, test_case['tests']):
            scores['correctness'] += 1
        
        # 2. æ•ˆç‡
        scores['efficiency'] += analyze_complexity(code)
        
        # 3. å¯è¯»æ€§
        scores['readability'] += pylint_score(code)
        
        # 4. æ–‡æ¡£
        scores['documentation'] += has_docstring(code)
    
    return {k: v / len(test_cases) for k, v in scores.items()}
```

**ä»£ç DPOçš„ç‰¹æ®Šè€ƒè™‘**ï¼š

1. **åŠŸèƒ½æ­£ç¡®æ€§ä¼˜å…ˆ**ï¼šDPOä¸åº”è¯¥ç‰ºç‰²æ­£ç¡®æ€§æ¢å–"é£æ ¼"
2. **æµ‹è¯•é©±åŠ¨è¯„ä¼°**ï¼šç”¨å•å…ƒæµ‹è¯•ä½œä¸ºå¥–åŠ±ä¿¡å·
3. **ç»“åˆé™æ€åˆ†æ**ï¼šç”¨Pylintã€Blackç­‰å·¥å…·é‡åŒ–ä»£ç è´¨é‡

---

### 8.3 å¸¸è§é—®é¢˜ä¸è°ƒè¯•

#### é—®é¢˜1ï¼šåå¥½å‡†ç¡®ç‡ä½äº50%

```python
# ç°è±¡ï¼špreference_accuracy < 0.5ï¼Œç”šè‡³æ¥è¿‘0

# å¯èƒ½åŸå› ä¸è§£å†³æ–¹æ¡ˆï¼š
solutions = {
    'æ•°æ®æ ‡æ³¨é”™è¯¯': {
        'diagnosis': 'æ£€æŸ¥chosenå’Œrejectedæ˜¯å¦æå',
        'solution': 'å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬ï¼Œäººå·¥æ£€æŸ¥æ ‡ç­¾'
    },
    
    'betaè®¾ç½®è¿‡å¤§': {
        'diagnosis': 'betaè¿‡å¤§å¯¼è‡´æ¨¡å‹æ— æ³•åç¦»å‚è€ƒæ¨¡å‹',
        'solution': 'é™ä½betaï¼ˆä»0.5 â†’ 0.1ï¼‰'
    },
    
    'å­¦ä¹ ç‡è¿‡å¤§': {
        'diagnosis': 'æ¨¡å‹è®­ç»ƒä¸ç¨³å®šï¼Œæ¥å›éœ‡è¡',
        'solution': 'é™ä½å­¦ä¹ ç‡ï¼ˆ5e-7 â†’ 1e-7ï¼‰'
    },
    
    'å‚è€ƒæ¨¡å‹è®¾ç½®é”™è¯¯': {
        'diagnosis': 'ref_modelåº”è¯¥æ˜¯SFTåçš„æ¨¡å‹ï¼Œä¸æ˜¯baseæ¨¡å‹',
        'solution': 'ç¡®ä¿ref_model = sft_modelçš„å‰¯æœ¬'
    }
}

# è°ƒè¯•ä»£ç 
def debug_low_preference_accuracy(model, ref_model, batch):
    """è°ƒè¯•åå¥½å‡†ç¡®ç‡ä½çš„é—®é¢˜"""
    # 1. æ£€æŸ¥æ•°æ®
    print("=== æ•°æ®æ£€æŸ¥ ===")
    print(f"Prompt: {batch['prompt'][0]}")
    print(f"Chosen: {batch['chosen'][0][:100]}...")
    print(f"Rejected: {batch['rejected'][0][:100]}...")
    print()
    
    # 2. æ£€æŸ¥æ¨¡å‹è¾“å‡º
    print("=== æ¨¡å‹è¾“å‡ºæ£€æŸ¥ ===")
    chosen_logprob = model.get_log_prob(batch['prompt'][0], batch['chosen'][0])
    rejected_logprob = model.get_log_prob(batch['prompt'][0], batch['rejected'][0])
    print(f"Chosen log prob: {chosen_logprob:.4f}")
    print(f"Rejected log prob: {rejected_logprob:.4f}")
    print(f"Diff (åº”è¯¥>0): {chosen_logprob - rejected_logprob:.4f}")
    print()
    
    # 3. æ£€æŸ¥å‚è€ƒæ¨¡å‹
    print("=== å‚è€ƒæ¨¡å‹æ£€æŸ¥ ===")
    ref_chosen = ref_model.get_log_prob(batch['prompt'][0], batch['chosen'][0])
    ref_rejected = ref_model.get_log_prob(batch['prompt'][0], batch['rejected'][0])
    print(f"Ref Chosen log prob: {ref_chosen:.4f}")
    print(f"Ref Rejected log prob: {ref_rejected:.4f}")
    print()
    
    # 4. æ£€æŸ¥éšå¼å¥–åŠ±
    print("=== éšå¼å¥–åŠ±æ£€æŸ¥ ===")
    implicit_reward_chosen = chosen_logprob - ref_chosen
    implicit_reward_rejected = rejected_logprob - ref_rejected
    reward_diff = implicit_reward_chosen - implicit_reward_rejected
    print(f"Implicit reward diff (åº”è¯¥>0): {reward_diff:.4f}")
    print(f"Preference prob: {torch.sigmoid(reward_diff):.2%}")
```

---

#### é—®é¢˜2ï¼šå¥–åŠ±å·®å€¼è¿‡å°ï¼ˆæ¨¡å‹æ²¡å­¦åˆ°åå¥½ï¼‰

```python
# ç°è±¡ï¼šreward_diffå§‹ç»ˆæ¥è¿‘0

# åŸå› ï¼šæ¨¡å‹è¿‡äºä¿å®ˆï¼Œä¸æ•¢åç¦»å‚è€ƒæ¨¡å‹

# è§£å†³æ–¹æ¡ˆ
solutions_for_small_reward_diff = {
    'é™ä½beta': {
        'from': 0.5,
        'to': 0.05,
        'reason': 'å‡å°‘KLæƒ©ç½šï¼Œå…è®¸æ›´å¤§åç¦»'
    },
    
    'å¢å¤§å­¦ä¹ ç‡': {
        'from': 5e-7,
        'to': 1e-6,
        'reason': 'åŠ å¿«å­¦ä¹ é€Ÿåº¦'
    },
    
    'å¢åŠ è®­ç»ƒæ­¥æ•°': {
        'reason': 'å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´å­¦ä¹ åå¥½'
    },
    
    'æ£€æŸ¥æ•°æ®è´¨é‡': {
        'check': 'chosenå’Œrejectedæ˜¯å¦å·®å¼‚æ˜æ˜¾ï¼Ÿ',
        'solution': 'è¿‡æ»¤æ‰å·®å¼‚å°çš„æ ·æœ¬'
    }
}

# è‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡åå¥½å¯¹
def filter_low_quality_pairs(dataset, model, threshold=0.1):
    """
    è¿‡æ»¤chosenå’Œrejectedå·®å¼‚å°çš„æ ·æœ¬
    """
    high_quality_pairs = []
    
    for example in tqdm(dataset):
        chosen_logprob = model.get_log_prob(
            example['prompt'], example['chosen']
        )
        rejected_logprob = model.get_log_prob(
            example['prompt'], example['rejected']
        )
        
        # å·®å¼‚åº¦é‡
        margin = chosen_logprob - rejected_logprob
        
        if abs(margin) > threshold:
            high_quality_pairs.append(example)
        else:
            print(f"âš ï¸ è·³è¿‡ä½è´¨é‡æ ·æœ¬ï¼ˆmargin={margin:.4f}ï¼‰")
    
    print(f"âœ… ä¿ç•™ {len(high_quality_pairs)}/{len(dataset)} ä¸ªé«˜è´¨é‡æ ·æœ¬")
    return high_quality_pairs
```

---

#### é—®é¢˜3ï¼šKLæ•£åº¦çˆ†ç‚¸

```python
# ç°è±¡ï¼šKLæ•£åº¦è¶Šæ¥è¶Šå¤§ï¼Œæ¨¡å‹è¡Œä¸ºå˜å¾—æ€ªå¼‚

# åŸå› ï¼šæ¨¡å‹è¿‡åº¦åç¦»å‚è€ƒæ¨¡å‹ï¼Œå¯èƒ½äº§ç”Ÿnonsenseè¾“å‡º

# è§£å†³æ–¹æ¡ˆ
def fix_kl_explosion(training_args):
    """ä¿®å¤KLæ•£åº¦çˆ†ç‚¸"""
    
    # æ–¹æ¡ˆ1: å¢å¤§betaï¼ˆåŠ å¼ºKLæƒ©ç½šï¼‰
    training_args.beta = 0.5  # åŸæ¥0.1 â†’ 0.5
    
    # æ–¹æ¡ˆ2: é™ä½å­¦ä¹ ç‡
    training_args.learning_rate = 1e-7  # åŸæ¥5e-7 â†’ 1e-7
    
    # æ–¹æ¡ˆ3: æ·»åŠ KLæ•£åº¦é˜ˆå€¼ï¼ˆEarly Stoppingï¼‰
    training_args.max_kl = 10.0  # KL>10å°±åœæ­¢
    
    # æ–¹æ¡ˆ4: ä½¿ç”¨adaptive betaï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
    def adaptive_beta_callback(trainer, logs):
        current_kl = logs.get('kl_chosen', 0)
        
        if current_kl > 5.0:
            trainer.args.beta *= 1.2  # å¢å¤§beta
            print(f"âš ï¸ KLè¿‡å¤§ï¼Œå¢å¤§betaåˆ° {trainer.args.beta:.3f}")
        elif current_kl < 0.1:
            trainer.args.beta *= 0.8  # å‡å°beta
            print(f"âœ… KLè¿‡å°ï¼Œå‡å°betaåˆ° {trainer.args.beta:.3f}")
    
    return training_args, adaptive_beta_callback
```

---

## ğŸ“š 9. å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. **DPOåŸå§‹è®ºæ–‡**  
   [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)  
   Rafailov et al., NeurIPS 2023

2. **RLHFç»¼è¿°**  
   [Reinforcement Learning from Human Feedback Book - Chapter 6](https://rlhfbook.com/c/06-preference-data.html)  
   2025å¹´æœ€æ–°ç‰ˆ

3. **ORPO**  
   [Odds Ratio Preference Optimization](https://www.emergentmind.com/topics/odds-ratio-preference-optimization-orpo)  
   ICLR 2025

4. **åå¥½å­¦ä¹ ç»Ÿä¸€æ¡†æ¶**  
   [From RLHF to Direct Alignment: A Theoretical Unification](https://arxiv.org/html/2601.06108v1)  
   2025å¹´1æœˆ

5. **Win Rateè¯„ä¼°**  
   [Preference learning made easy: Everything should be understood through win rate](https://openreview.net/pdf?id=UA8DESerfC)  
   OpenReview 2025

### å·¥ç¨‹å®è·µ

- [Hugging Face TRL Documentation](https://huggingface.co/docs/trl)  
  DPOå®˜æ–¹å®ç°åº“

- [OpenAI Fine-tuning Guide - DPO](https://developers.openai.com/cookbook/examples/fine_tuning_direct_preference_optimization_guide)  
  OpenAIçš„DPOå®è·µæŒ‡å—

- [PyTorch torchtune DPO Recipe](https://docs.pytorch.org/torchtune/stable/recipes/dpo.html)  
  PyTorchå®˜æ–¹DPOæ•™ç¨‹

### æ•°æ®é›†

- [HelpSteer3-Preference (2025)](https://arxiv.org/abs/2505.11475)  
  NVIDIAå‘å¸ƒï¼Œ40K+é«˜è´¨é‡åå¥½å¯¹

- [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)  
  Anthropicçš„äººç±»åå¥½æ•°æ®é›†

---

## ğŸ¯ æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **åå¥½å¯¹é½çš„æœ¬è´¨**ï¼šæ•™ä¼šLLM"ä»€ä¹ˆæ˜¯å¥½å›ç­”"ï¼Œè€Œéä»…ä»…"å¦‚ä½•å›ç­”"
2. **DPOçš„æ ¸å¿ƒä¼˜åŠ¿**ï¼šç®€åŒ–RLHFä¸ºç›‘ç£å­¦ä¹ ï¼Œå¯¹ä¼ ç»Ÿç¨‹åºå‘˜å‹å¥½
3. **2025æœ€ä½³å®è·µ**ï¼šDPOä»æ˜¯ä¸»æµï¼ŒORPO/KTO/GRPOæ˜¯æœ‰åŠ›è¡¥å……
4. **æ•°æ®æ˜¯å…³é”®**ï¼šé«˜è´¨é‡åå¥½æ•°æ®æ¯”å¤æ‚ç®—æ³•æ›´é‡è¦
5. **è¯„ä¼°è¦å…¨é¢**ï¼šWin Rate + å¤šç»´åº¦äººç±»è¯„ä¼°

### ä¼ ç»Ÿç¨‹åºå‘˜çš„ä¼˜åŠ¿

- âœ… **å·¥ç¨‹èƒ½åŠ›**ï¼šDPOè®­ç»ƒå°±åƒè°ƒè¯•ç³»ç»Ÿï¼Œç›‘æ§æŒ‡æ ‡ã€åˆ†ææ—¥å¿—
- âœ… **æ•°æ®æ€ç»´**ï¼šç†è§£æ•°æ®è´¨é‡å¯¹æ¨¡å‹çš„å½±å“
- âœ… **å®éªŒæ–¹æ³•**ï¼šA/Bæµ‹è¯•ã€å¯¹æ¯”å®éªŒæ˜¯ä¼ ç»ŸæŠ€èƒ½çš„ç›´æ¥è¿ç§»

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ”— **ä¸‹ä¸€ç¯‡**ï¼š[09 - æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼šé‡åŒ–ã€åŠ é€Ÿä¸æˆæœ¬ä¼˜åŒ–](./09-inference-optimization.md)
- ğŸ“– **æ·±å…¥é˜…è¯»**ï¼šDPOåŸå§‹è®ºæ–‡ + Hugging Face TRLæºç 
- ğŸ’» **åŠ¨æ‰‹å®è·µ**ï¼šç”¨HelpSteer3æ•°æ®é›†è®­ç»ƒä¸€ä¸ªDPOæ¨¡å‹

---

> ğŸ’¡ **ç’‡ç‘çš„å°è´´å£«**ï¼šåå¥½å¯¹é½å°±åƒCode Reviewâ€”â€”ä¸æ˜¯å‘Šè¯‰AI"æ€ä¹ˆå†™"ï¼Œè€Œæ˜¯å‘Šè¯‰å®ƒ"å†™æˆè¿™æ ·æ›´å¥½"ã€‚ä¼ ç»Ÿç¨‹åºå‘˜çš„Reviewç»éªŒï¼Œåœ¨è¿™é‡Œèƒ½ç›´æ¥æ´¾ä¸Šç”¨åœºå‘¢ï¼âœ¨
>
> é“å‹ç°åœ¨å¯¹DPOæœ‰æ„Ÿè§‰äº†å—ï¼Ÿä¸‹ä¸€ç¯‡æˆ‘ä»¬èŠæ¨ç†ä¼˜åŒ–ï¼Œè®©è®­ç»ƒå¥½çš„æ¨¡å‹è·‘å¾—åˆå¿«åˆçœé’±~ ğŸš€
