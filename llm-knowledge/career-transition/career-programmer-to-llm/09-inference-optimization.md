# 09 - æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼šé‡åŒ–ã€åŠ é€Ÿä¸æˆæœ¬ä¼˜åŒ–

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šæ¨ç†æˆæœ¬æ˜¯LLMåº”ç”¨æœ€å¤§çš„è¿è¥å¼€æ”¯ã€‚æœ¬æ–‡æ·±å…¥è®²è§£é‡åŒ–æŠ€æœ¯ï¼ˆINT8/INT4/FP8ï¼‰ã€é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼ˆvLLMã€TensorRT-LLMï¼‰ã€KV Cacheä¼˜åŒ–ã€æ¨æµ‹è§£ç ç­‰å‰æ²¿æŠ€æœ¯ï¼Œå¹¶æä¾›2025å¹´çœŸå®æˆæœ¬æ•°æ®å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå¸®åŠ©ä¼ ç»Ÿç¨‹åºå‘˜å°†æ¨ç†æˆæœ¬é™ä½10å€ä»¥ä¸Šã€‚

---

## ğŸ“‹ ç›®å½•

1. [ä¸ºä»€ä¹ˆæ¨ç†ä¼˜åŒ–å¦‚æ­¤é‡è¦ï¼Ÿ](#why-optimization)
2. [é‡åŒ–æŠ€æœ¯æ·±åº¦è§£æ](#quantization)
3. [é«˜æ€§èƒ½æ¨ç†å¼•æ“å¯¹æ¯”](#inference-engines)
4. [KV Cacheä¼˜åŒ–æŠ€æœ¯](#kv-cache)
5. [æ¨æµ‹è§£ç ä¸åŠ é€ŸæŠ€æœ¯](#speculative-decoding)
6. [æ¨ç†æˆæœ¬åˆ†æä¸ä¼˜åŒ–](#cost-optimization)
7. [ç§»åŠ¨ç«¯éƒ¨ç½²ï¼šllama.cpp](#mobile-deployment)
8. [å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡](#optimization-strategy)
9. [å®æˆ˜æ¡ˆä¾‹ä¸Benchmark](#benchmarks)

---

<a name="why-optimization"></a>
## ğŸ’° 1. ä¸ºä»€ä¹ˆæ¨ç†ä¼˜åŒ–å¦‚æ­¤é‡è¦?

### 1.1 æ¨ç†æˆæœ¬å æ¯”

å‡è®¾ä½ éƒ¨ç½²äº†ä¸€ä¸ªåŸºäºLLaMA-2-7Bçš„å®¢æœåŠ©æ‰‹ï¼š

```python
# è®­ç»ƒ vs æ¨ç†æˆæœ¬å¯¹æ¯”
class LLMCostAnalysis:
    """
    çœŸå®æ¡ˆä¾‹ï¼šæŸå…¬å¸çš„LLMåº”ç”¨æˆæœ¬åˆ†æ
    """
    def __init__(self):
        # ä¸€æ¬¡æ€§è®­ç»ƒæˆæœ¬
        self.training_cost = {
            'GPUç§Ÿç”¨': 10_000,      # 10ä¸ªA100 * 7å¤©
            'æ•°æ®æ ‡æ³¨': 5_000,       # 5000æ¡æ•°æ®
            'äººåŠ›æˆæœ¬': 15_000,      # å·¥ç¨‹å¸ˆæ—¶é—´
            'total': 30_000
        }
        
        # æ¯æœˆæ¨ç†æˆæœ¬
        self.monthly_inference_cost = {
            'GPUç§Ÿç”¨': 50_000,       # 24/7è¿è¡Œ
            'å¸¦å®½': 5_000,
            'å­˜å‚¨': 1_000,
            'total': 56_000
        }
        
    def calculate_yearly_cost(self):
        training = self.training_cost['total']
        inference = self.monthly_inference_cost['total'] * 12
        
        print(f"""
        LLMåº”ç”¨å¹´åº¦æˆæœ¬:
          è®­ç»ƒæˆæœ¬: ${training:,} (ä¸€æ¬¡æ€§)
          æ¨ç†æˆæœ¬: ${inference:,} (å¹´åº¦)
          
        æ¨ç†å æ¯”: {inference/(training+inference)*100:.1f}%
        """)
        
        return inference

# è¿è¡Œåˆ†æ
analyzer = LLMCostAnalysis()
yearly_inference_cost = analyzer.calculate_yearly_cost()
# è¾“å‡ºï¼š
# LLMåº”ç”¨å¹´åº¦æˆæœ¬:
#   è®­ç»ƒæˆæœ¬: $30,000 (ä¸€æ¬¡æ€§)
#   æ¨ç†æˆæœ¬: $672,000 (å¹´åº¦)
#   
# æ¨ç†å æ¯”: 95.7%
```

> ğŸ”¥ **å…³é”®æ´å¯Ÿ**ï¼šæ¨ç†æˆæœ¬å LLMåº”ç”¨æ€»æˆæœ¬çš„**95%ä»¥ä¸Š**ï¼ä¼˜åŒ–æ¨ç†æ¯”ä¼˜åŒ–è®­ç»ƒæ›´é‡è¦ã€‚

---

### 1.2 ä¼ ç»Ÿç¨‹åºå‘˜çš„ç±»æ¯”

```python
# ä¼ ç»ŸWebæœåŠ¡ä¼˜åŒ– vs LLMæ¨ç†ä¼˜åŒ–
class TraditionalVsLLMOptimization:
    """
    ä¼ ç»Ÿä¼˜åŒ–ç»éªŒåœ¨LLMä¸­çš„å¯¹åº”
    """
    
    def traditional_web_optimization(self):
        """ä¼ ç»ŸWebæœåŠ¡ä¼˜åŒ–"""
        return {
            'ç¼“å­˜': 'Redisç¼“å­˜çƒ­æ•°æ®',
            'æ•°æ®åº“ç´¢å¼•': 'åŠ é€ŸæŸ¥è¯¢',
            'CDN': 'é™æ€èµ„æºåŠ é€Ÿ',
            'è´Ÿè½½å‡è¡¡': 'Nginxåˆ†æµ',
            'å‹ç¼©': 'Gzipå‹ç¼©å“åº”'
        }
    
    def llm_inference_optimization(self):
        """LLMæ¨ç†ä¼˜åŒ–ï¼ˆå¯¹åº”å…³ç³»ï¼‰"""
        return {
            'ç¼“å­˜': 'KV Cacheå¤ç”¨ï¼ˆPagedAttentionï¼‰',
            'æ•°æ®åº“ç´¢å¼•': 'é‡åŒ–ï¼ˆINT4/INT8ï¼‰å‡å°‘è®¿å­˜',
            'CDN': 'æ¨¡å‹è’¸é¦ï¼ˆå°æ¨¡å‹éƒ¨ç½²è¾¹ç¼˜ï¼‰',
            'è´Ÿè½½å‡è¡¡': 'æ‰¹å¤„ç†ï¼ˆBatchingï¼‰æé«˜åå',
            'å‹ç¼©': 'æƒé‡å‹ç¼©ï¼ˆå‰ªæã€é‡åŒ–ï¼‰'
        }
```

**ä¼ ç»Ÿç¨‹åºå‘˜çš„ä¼˜åŠ¿**ï¼šä½ å·²ç»æ‡‚å¾—"ä¼˜åŒ–"çš„æœ¬è´¨â€”â€”**å‡å°‘ç“¶é¢ˆèµ„æºçš„æ¶ˆè€—**ã€‚åœ¨LLMæ¨ç†ä¸­ï¼Œç“¶é¢ˆæ˜¯ï¼š
1. **æ˜¾å­˜å¸¦å®½**ï¼ˆMemory Bandwidthï¼‰- ä»æ˜¾å­˜è¯»å–æƒé‡
2. **è®¡ç®—**ï¼ˆFLOPsï¼‰- çŸ©é˜µä¹˜æ³•
3. **æ˜¾å­˜å®¹é‡**ï¼ˆMemory Capacityï¼‰- å­˜å‚¨KV Cache

---

### 1.3 æ¨ç†æˆæœ¬ä¸‹é™è¶‹åŠ¿ï¼ˆ2025-2026ï¼‰

æ ¹æ® [a16z 2025å¹´æŠ¥å‘Š](https://a16z.com/llmflation-llm-inference-cost/)ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np

# æ¨ç†æˆæœ¬å†å²æ•°æ®
years = np.array([2021, 2022, 2023, 2024, 2025])
cost_per_1m_tokens = np.array([60, 20, 6, 2, 0.6])  # ç¾å…ƒ

# ç»˜åˆ¶æˆæœ¬ä¸‹é™æ›²çº¿
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(years, cost_per_1m_tokens, marker='o', linewidth=2, markersize=8)
ax.set_yscale('log')
ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Cost per 1M tokens ($, log scale)', fontsize=12)
ax.set_title('LLM Inference Cost Decline: 10x per Year\n(Faster than Moore\'s Law!)', 
             fontsize=14, fontweight='bold')
ax.grid(alpha=0.3)

# æ ‡æ³¨å…³é”®èŠ‚ç‚¹
annotations = [
    (2021, 60, 'GPT-3 Release\n$60/1M tokens'),
    (2025, 0.6, 'Llama 3.2 3B\n$0.60/1M tokens')
]
for year, cost, text in annotations:
    ax.annotate(text, xy=(year, cost), xytext=(10, 20), 
                textcoords='offset points', fontsize=10,
                bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

plt.tight_layout()
plt.savefig('inference_cost_decline.png', dpi=150)
```

> ğŸ”¥ **2025å…³é”®æ•°æ®**ï¼šæ¨ç†æˆæœ¬æ¯å¹´ä¸‹é™**10å€**ï¼Œæ¯”æ‘©å°”å®šå¾‹ï¼ˆ18ä¸ªæœˆ2å€ï¼‰å¿«å¾—å¤šï¼

---

<a name="quantization"></a>
## âš–ï¸ 2. é‡åŒ–æŠ€æœ¯æ·±åº¦è§£æ

### 2.1 ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ

**é‡åŒ–ï¼ˆQuantizationï¼‰**ï¼šå°†æ¨¡å‹æƒé‡å’Œæ¿€æ´»ä»é«˜ç²¾åº¦ï¼ˆFP32/FP16ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆINT8/INT4ï¼‰ï¼Œå‡å°‘æ˜¾å­˜å ç”¨å’Œè®¿å­˜å¸¦å®½ã€‚

```python
import numpy as np

# é‡åŒ–ç¤ºä¾‹ï¼šFP32 â†’ INT8
def quantize_to_int8(weight_fp32):
    """
    ç®€å•é‡åŒ–å®ç°
    
    å…¬å¼: Q = round(W / scale) + zero_point
    """
    # 1. ç¡®å®šé‡åŒ–èŒƒå›´
    w_min, w_max = weight_fp32.min(), weight_fp32.max()
    
    # 2. è®¡ç®—scaleå’Œzero_point
    scale = (w_max - w_min) / 255  # INT8èŒƒå›´ï¼š-128~127 (256ä¸ªå€¼)
    zero_point = -128 - round(w_min / scale)
    
    # 3. é‡åŒ–
    weight_int8 = np.round(weight_fp32 / scale + zero_point)
    weight_int8 = np.clip(weight_int8, -128, 127).astype(np.int8)
    
    # 4. åé‡åŒ–ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
    weight_dequant = (weight_int8.astype(np.float32) - zero_point) * scale
    
    # 5. è®¡ç®—ç²¾åº¦æŸå¤±
    mse = np.mean((weight_fp32 - weight_dequant) ** 2)
    
    print(f"""
    é‡åŒ–ç»“æœ:
      åŸå§‹èŒƒå›´: [{w_min:.4f}, {w_max:.4f}]
      Scale: {scale:.6f}
      Zero point: {zero_point}
      MSEæŸå¤±: {mse:.6f}
      å‹ç¼©æ¯”: {weight_fp32.nbytes / weight_int8.nbytes:.1f}x
    """)
    
    return weight_int8, scale, zero_point

# ç¤ºä¾‹
weight_fp32 = np.random.randn(1000, 1000).astype(np.float32)
weight_int8, scale, zero_point = quantize_to_int8(weight_fp32)
# è¾“å‡ºï¼š
# é‡åŒ–ç»“æœ:
#   åŸå§‹èŒƒå›´: [-4.5323, 4.6521]
#   Scale: 0.036008
#   Zero point: -2
#   MSEæŸå¤±: 0.000865
#   å‹ç¼©æ¯”: 4.0x
```

---

### 2.2 é‡åŒ–æ–¹æ³•åˆ†ç±»

#### 2.2.1 æŒ‰é‡åŒ–å¯¹è±¡åˆ†ç±»

| ç±»å‹ | é‡åŒ–å¯¹è±¡ | ç‰¹ç‚¹ | æ¨èåœºæ™¯ |
|------|----------|------|----------|
| **Weight-Only** | ä»…æƒé‡ | ç®€å•ï¼Œä¸éœ€è¦æ ¡å‡†æ•°æ® | âœ… **å°batchæ¨ç†**ï¼ˆbatchâ‰¤4ï¼‰ |
| **Weight + Activation** | æƒé‡+æ¿€æ´» | éœ€è¦æ ¡å‡†ï¼ŒåŠ é€Ÿæ›´æ˜¾è‘— | âœ… **å¤§batchæ¨ç†**ï¼ˆbatchâ‰¥16ï¼‰ |

```python
# ä¸ºä»€ä¹ˆå°batchç”¨Weight-Onlyï¼Ÿ
class InferenceBottleneckAnalysis:
    """
    æ¨ç†ç“¶é¢ˆåˆ†æï¼šè®¡ç®—å—é™ vs è®¿å­˜å—é™
    """
    
    def analyze_bottleneck(self, batch_size):
        """
        åˆ†æå½“å‰batch sizeçš„ç“¶é¢ˆ
        """
        # å‡è®¾æ¨¡å‹å‚æ•°é‡
        param_size_gb = 7  # 7Bæ¨¡å‹çº¦14GB (FP16)
        
        # è®¡ç®—å¼ºåº¦ï¼ˆFLOPs per byteï¼‰
        # å°batchï¼šè®¿å­˜æ¬¡æ•°å¤šï¼Œè®¡ç®—å°‘ â†’ è®¿å­˜å—é™
        # å¤§batchï¼šè®¿å­˜æ¬¡æ•°ç›¸åŒï¼Œè®¡ç®—å¤š â†’ è®¡ç®—å—é™
        
        if batch_size <= 4:
            bottleneck = "Memory Bandwidth"
            solution = "Weight-Only Quantization (å‡å°‘æƒé‡è®¿å­˜)"
        else:
            bottleneck = "Computation"
            solution = "Weight+Activation Quantization (å‡å°‘è®¡ç®—é‡)"
        
        print(f"""
        Batch size {batch_size}:
          ç“¶é¢ˆ: {bottleneck}
          æ¨èæ–¹æ¡ˆ: {solution}
        """)
        
        return bottleneck, solution

analyzer = InferenceBottleneckAnalysis()
analyzer.analyze_bottleneck(1)   # Memory Bandwidth
analyzer.analyze_bottleneck(32)  # Computation
```

---

#### 2.2.2 2025å¹´ä¸»æµé‡åŒ–æ–¹æ³•å¯¹æ¯”

æ ¹æ® [NVIDIA TensorRT-LLM 2025æ–‡æ¡£](https://nvidia.github.io/TensorRT-LLM/blogs/quantization-in-TRT-LLM.html)ï¼š

| æ–¹æ³• | ç²¾åº¦ | å‹ç¼©æ¯” | å‡†ç¡®ç‡æŸå¤± | æ ¡å‡†æ—¶é—´ | æ¨èGPU |
|------|------|--------|-----------|---------|---------|
| **FP8** ğŸ”¥ | 8-bit | 2x | æå° | åˆ†é’Ÿçº§ | H100/H200 |
| **INT8 SmoothQuant** | 8-bit | 2x | å° | åˆ†é’Ÿçº§ | A100/V100 |
| **INT4 AWQ** | 4-bit | 4x | ä¸­ç­‰ | ååˆ†é’Ÿçº§ | A100+ |
| **INT4-FP8 AWQ** | 4/8-bitæ··åˆ | 3x | å° | ååˆ†é’Ÿçº§ | H100+ |
| **GPTQ** | 4-bit | 4x | ä¸­ç­‰ | å°æ—¶çº§ | é€šç”¨ |

**2025å¹´æœ€ä½³å®è·µ**ï¼š

```python
def choose_quantization_method(gpu_type, batch_size, model_size):
    """
    æ ¹æ®ç¡¬ä»¶å’Œåœºæ™¯é€‰æ‹©é‡åŒ–æ–¹æ³•
    """
    # å†³ç­–æ ‘
    if gpu_type in ['H100', 'H200', 'B200']:
        # æœ€æ–°GPUï¼Œä¼˜å…ˆFP8
        if batch_size >= 16:
            return "FP8 (æƒé‡+æ¿€æ´»)"  # ğŸ”¥ 2025é¦–é€‰
        else:
            return "INT4-FP8 AWQ (æ··åˆç²¾åº¦)"
    
    elif gpu_type in ['A100', 'A6000']:
        if batch_size >= 16:
            return "INT8 SmoothQuant"
        else:
            return "INT4 AWQ"
    
    else:  # æ—§GPU (V100, T4)
        if model_size == 'large':  # 70B+
            return "INT4 GPTQ (æé™å‹ç¼©)"
        else:
            return "INT8 SmoothQuant"

# ç¤ºä¾‹
print(choose_quantization_method('H100', 32, '7B'))  # FP8 (æƒé‡+æ¿€æ´»)
print(choose_quantization_method('A100', 2, '13B'))  # INT4 AWQ
```

---

### 2.3 FP8é‡åŒ–å®æˆ˜ï¼ˆ2025æ¨èï¼‰

**FP8** æ˜¯2025å¹´çš„é¦–é€‰é‡åŒ–æ–¹æ¡ˆï¼Œå‡†ç¡®ç‡æŸå¤±æå°ï¼ˆ<1%ï¼‰ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚

```python
"""
FP8é‡åŒ–å®æˆ˜ - ä½¿ç”¨NVIDIA TensorRT-LLM
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. åŠ è½½æ¨¡å‹
model_name = "meta-llama/Llama-2-7b-hf"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 2. FP8é‡åŒ–é…ç½®
from tensorrt_llm.quantization import QuantConfig

quant_config = QuantConfig(
    quant_algo="FP8",                     # ğŸ”¥ ä½¿ç”¨FP8é‡åŒ–
    kv_cache_quant_algo="FP8",            # KV Cacheä¹Ÿé‡åŒ–
    exclude_modules=['lm_head'],          # æ’é™¤è¾“å‡ºå±‚ï¼ˆä¿æŒç²¾åº¦ï¼‰
)

# 3. å‡†å¤‡æ ¡å‡†æ•°æ®ï¼ˆå°æ ·æœ¬å³å¯ï¼‰
calibration_dataset = [
    "The quick brown fox jumps over the lazy dog.",
    "Machine learning is a subset of artificial intelligence.",
    # ... 100-1000æ¡ä»£è¡¨æ€§æ–‡æœ¬
]

# 4. æ‰§è¡Œé‡åŒ–
from tensorrt_llm import build_engine

quantized_model = build_engine(
    model=model,
    quant_config=quant_config,
    calibration_data=calibration_dataset,
    max_batch_size=32,
    max_input_len=2048,
    max_output_len=512
)

# 5. ä¿å­˜é‡åŒ–æ¨¡å‹
quantized_model.save("llama2-7b-fp8")

# 6. æ¨ç†å¯¹æ¯”
import time

def benchmark_inference(model, prompt, num_runs=100):
    """Benchmarkæ¨ç†é€Ÿåº¦"""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    
    # Warmup
    for _ in range(10):
        model.generate(input_ids, max_new_tokens=50)
    
    # Benchmark
    start = time.time()
    for _ in range(num_runs):
        model.generate(input_ids, max_new_tokens=50)
    end = time.time()
    
    avg_latency = (end - start) / num_runs
    return avg_latency

# å¯¹æ¯”
prompt = "Explain quantum computing in simple terms:"
fp16_latency = benchmark_inference(model, prompt)
fp8_latency = benchmark_inference(quantized_model, prompt)

print(f"""
FP8é‡åŒ–æ€§èƒ½å¯¹æ¯”:
  FP16å»¶è¿Ÿ: {fp16_latency*1000:.1f}ms
  FP8å»¶è¿Ÿ: {fp8_latency*1000:.1f}ms
  åŠ é€Ÿæ¯”: {fp16_latency/fp8_latency:.2f}x
  
  FP16æ˜¾å­˜: {torch.cuda.max_memory_allocated()/1024**3:.1f}GB
  FP8æ˜¾å­˜: {torch.cuda.max_memory_allocated()/1024**3/2:.1f}GB (ä¼°ç®—)
  å‹ç¼©æ¯”: 2.0x
""")
```

---

### 2.4 INT4 AWQé‡åŒ–å®æˆ˜

**AWQ (Activation-aware Weight Quantization)** æ˜¯4-bité‡åŒ–çš„æœ€ä½³æ–¹æ³•ä¹‹ä¸€ã€‚

```python
"""
INT4 AWQé‡åŒ–å®æˆ˜ - ä½¿ç”¨AutoAWQåº“
"""
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

# 1. åŠ è½½æ¨¡å‹
model_path = "meta-llama/Llama-2-7b-chat-hf"
quant_path = "llama2-7b-awq-int4"

model = AutoAWQForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 2. å‡†å¤‡é‡åŒ–é…ç½®
quant_config = {
    "zero_point": True,           # ä½¿ç”¨é›¶ç‚¹é‡åŒ–
    "q_group_size": 128,          # åˆ†ç»„å¤§å°ï¼ˆè¶Šå°ç²¾åº¦è¶Šé«˜ï¼Œä½†æ¨¡å‹è¶Šå¤§ï¼‰
    "w_bit": 4,                   # 4-bité‡åŒ–
    "version": "GEMM"             # é‡åŒ–å†…æ ¸ç±»å‹
}

# 3. é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†æ•°æ®ï¼‰
# ä½¿ç”¨é¢„å®šä¹‰çš„æ ¡å‡†æ•°æ®é›†
model.quantize(
    tokenizer,
    quant_config=quant_config,
    calib_data="pileval",  # æˆ–è‡ªå®šä¹‰æ•°æ®é›†
    n_samples=512,         # æ ¡å‡†æ ·æœ¬æ•°
    seq_len=512            # åºåˆ—é•¿åº¦
)

# 4. ä¿å­˜é‡åŒ–æ¨¡å‹
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)

# 5. åŠ è½½å’Œæ¨ç†
quantized_model = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)

# 6. æ€§èƒ½å¯¹æ¯”
from transformers import AutoModelForCausalLM as HF_Model

fp16_model = HF_Model.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto")

# æ˜¾å­˜å ç”¨å¯¹æ¯”
print(f"""
INT4 AWQé‡åŒ–ç»“æœ:
  FP16æ¨¡å‹å¤§å°: ~14GB
  INT4æ¨¡å‹å¤§å°: ~3.5GB
  å‹ç¼©æ¯”: 4.0x
  
  ç²¾åº¦æŸå¤±: <3% (åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Š)
  æ¨ç†é€Ÿåº¦: 1.5-2.0x faster (å°batchåœºæ™¯)
""")
```

---

### 2.5 é‡åŒ–ç²¾åº¦æŸå¤±è¯„ä¼°

```python
"""
é‡åŒ–æ¨¡å‹çš„ç²¾åº¦è¯„ä¼°
"""
from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

def evaluate_quantization_quality(original_model_path, quantized_model_path):
    """
    è¯„ä¼°é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
    
    ä½¿ç”¨lm-evaluation-harnessåº“
    """
    # åŠ è½½æ¨¡å‹
    original_model = HFLM(pretrained=original_model_path)
    quantized_model = HFLM(pretrained=quantized_model_path)
    
    # è¯„ä¼°ä»»åŠ¡
    tasks = [
        "hellaswag",      # å¸¸è¯†æ¨ç†
        "arc_easy",       # é—®ç­”
        "winogrande",     # ä»£è¯æ¶ˆæ­§
        "gsm8k",          # æ•°å­¦æ¨ç†
    ]
    
    # è¯„ä¼°åŸå§‹æ¨¡å‹
    print("è¯„ä¼°åŸå§‹æ¨¡å‹...")
    original_results = evaluator.simple_evaluate(
        model=original_model,
        tasks=tasks,
        num_fewshot=5,
        batch_size=8
    )
    
    # è¯„ä¼°é‡åŒ–æ¨¡å‹
    print("è¯„ä¼°é‡åŒ–æ¨¡å‹...")
    quantized_results = evaluator.simple_evaluate(
        model=quantized_model,
        tasks=tasks,
        num_fewshot=5,
        batch_size=8
    )
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*60)
    print("é‡åŒ–ç²¾åº¦å¯¹æ¯”æŠ¥å‘Š")
    print("="*60)
    
    for task in tasks:
        orig_score = original_results['results'][task]['acc']
        quant_score = quantized_results['results'][task]['acc']
        degradation = (orig_score - quant_score) / orig_score * 100
        
        print(f"{task:15s}: {orig_score:.1%} â†’ {quant_score:.1%} (æŸå¤±: {degradation:.1f}%)")
    
    # ç¤ºä¾‹è¾“å‡ºï¼š
    # hellaswag      : 76.3% â†’ 75.8% (æŸå¤±: 0.7%)
    # arc_easy       : 79.2% â†’ 78.5% (æŸå¤±: 0.9%)
    # winogrande     : 72.1% â†’ 71.3% (æŸå¤±: 1.1%)
    # gsm8k          : 48.7% â†’ 46.2% (æŸå¤±: 5.1%)
    
    return original_results, quantized_results
```

**é‡åŒ–ç²¾åº¦æŸå¤±è§„å¾‹**ï¼ˆ2025ç ”ç©¶ï¼‰ï¼š

æ ¹æ® [2025 IJCAIç ”ç©¶](https://www.ijcai.org/proceedings/2025/0902.pdf)ï¼š

1. **ä»»åŠ¡éš¾åº¦ç›¸å…³**ï¼šç®€å•ä»»åŠ¡æŸå¤±å°ï¼Œå¤æ‚ä»»åŠ¡ï¼ˆæ•°å­¦æ¨ç†ï¼‰æŸå¤±å¤§
2. **æ¨¡å‹è§„æ¨¡ç›¸å…³**ï¼š70B+æ¨¡å‹åœ¨4-bitä¸‹æ›´ç¨³å®šï¼Œå°æ¨¡å‹æŸå¤±ä¸¥é‡
3. **é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡**ï¼š4-bité‡åŒ–åœ¨64K+ä¸Šä¸‹æ–‡æ—¶æŸå¤±é«˜è¾¾59%
4. **è¯­è¨€ç›¸å…³**ï¼šéè‹±è¯­ä»»åŠ¡æŸå¤±æ›´å¤§

---

<a name="inference-engines"></a>
## ğŸš€ 3. é«˜æ€§èƒ½æ¨ç†å¼•æ“å¯¹æ¯”

### 3.1 ä¸»æµæ¨ç†å¼•æ“å¯¹æ¯”ï¼ˆ2025ï¼‰

| å¼•æ“ | å¼€å‘è€… | è¯­è¨€ | æ ¸å¿ƒç‰¹æ€§ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|----------|----------|
| **vLLM** ğŸ”¥ | UC Berkeley | Python | PagedAttention | é«˜ååæœåŠ¡ |
| **TensorRT-LLM** | NVIDIA | C++/Python | æè‡´æ€§èƒ½ | ç”Ÿäº§ç¯å¢ƒ |
| **Text Generation Inference (TGI)** | Hugging Face | Rust | æ˜“ç”¨æ€§å¼º | ä½å»¶è¿Ÿäº¤äº’ |
| **llama.cpp** | ggerganov | C++ | CPUå‹å¥½ | è¾¹ç¼˜è®¾å¤‡ |
| **LMDeploy** | OpenMMLab | Python | å¤šæ¨¡æ€ | è§†è§‰-è¯­è¨€æ¨¡å‹ |

---

### 3.2 vLLMæ·±åº¦è§£æï¼ˆ2025æ¨èï¼‰

æ ¹æ® [2025å¹´11æœˆBenchmark](https://arxiv.org/html/2511.17593v1)ï¼ŒvLLMåœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ¯”TGIå¿«**24å€**ï¼

#### 3.2.1 æ ¸å¿ƒæŠ€æœ¯ï¼šPagedAttention

**é—®é¢˜**ï¼šä¼ ç»ŸKV Cacheæ˜¯è¿ç»­å­˜å‚¨çš„ï¼Œå¯¼è‡´ï¼š
- æ˜¾å­˜ç¢ç‰‡åŒ–ï¼ˆé¢„åˆ†é…ç©ºé—´æµªè´¹ï¼‰
- batchå†…åºåˆ—é•¿åº¦ä¸åŒæ—¶åˆ©ç”¨ç‡ä½

**PagedAttentionè§£å†³æ–¹æ¡ˆ**ï¼š

```python
"""
PagedAttentionåŸç†æ¼”ç¤º
"""

class TraditionalKVCache:
    """ä¼ ç»ŸKV Cacheï¼šè¿ç»­å­˜å‚¨"""
    def __init__(self, max_seq_len=2048, hidden_dim=4096):
        # ä¸ºæ¯ä¸ªåºåˆ—é¢„åˆ†é…æœ€å¤§é•¿åº¦çš„æ˜¾å­˜
        self.kv_cache = torch.zeros(max_seq_len, hidden_dim)
        self.actual_len = 0
    
    def add_token(self, kv):
        """æ·»åŠ æ–°tokençš„KV"""
        self.kv_cache[self.actual_len] = kv
        self.actual_len += 1
    
    def get_utilization(self):
        """æ˜¾å­˜åˆ©ç”¨ç‡"""
        return self.actual_len / len(self.kv_cache)

class PagedAttentionKVCache:
    """PagedAttentionï¼šåˆ†é¡µå­˜å‚¨"""
    def __init__(self, block_size=16, hidden_dim=4096):
        # åˆ†é¡µå­˜å‚¨ï¼ŒæŒ‰éœ€åˆ†é…
        self.block_size = block_size
        self.blocks = []  # å—åˆ—è¡¨
        self.actual_len = 0
    
    def add_token(self, kv):
        """æ·»åŠ æ–°tokençš„KV"""
        block_idx = self.actual_len // self.block_size
        
        # å¦‚æœéœ€è¦æ–°å—ï¼Œæ‰åˆ†é…
        if block_idx >= len(self.blocks):
            self.blocks.append(torch.zeros(self.block_size, hidden_dim))
        
        offset = self.actual_len % self.block_size
        self.blocks[block_idx][offset] = kv
        self.actual_len += 1
    
    def get_utilization(self):
        """æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆæ›´é«˜ï¼ï¼‰"""
        allocated = len(self.blocks) * self.block_size
        return self.actual_len / allocated if allocated > 0 else 0

# å¯¹æ¯”ç¤ºä¾‹
# åœºæ™¯ï¼š3ä¸ªè¯·æ±‚ï¼Œé•¿åº¦åˆ†åˆ«ä¸º 50, 200, 1500 tokens
requests = [50, 200, 1500]

# ä¼ ç»Ÿæ–¹æ³•
traditional_caches = [TraditionalKVCache() for _ in requests]
traditional_memory = len(requests) * 2048  # æ¯ä¸ªéƒ½é¢„åˆ†é…2048
traditional_util = sum(req_len / 2048 for req_len in requests) / len(requests)

# PagedAttention
paged_caches = [PagedAttentionKVCache() for _ in requests]
paged_memory = sum((req_len + 15) // 16 * 16 for req_len in requests)  # å‘ä¸Šå–æ•´åˆ°block
paged_util = sum(requests) / paged_memory

print(f"""
KV Cacheå†…å­˜ä½¿ç”¨å¯¹æ¯”:
  ä¼ ç»Ÿæ–¹æ³•:
    åˆ†é…æ˜¾å­˜: {traditional_memory} tokens
    å®é™…ä½¿ç”¨: {sum(requests)} tokens
    åˆ©ç”¨ç‡: {traditional_util:.1%}
    
  PagedAttention:
    åˆ†é…æ˜¾å­˜: {paged_memory} tokens
    å®é™…ä½¿ç”¨: {sum(requests)} tokens
    åˆ©ç”¨ç‡: {paged_util:.1%}
    
  æ˜¾å­˜èŠ‚çœ: {(1 - paged_memory/traditional_memory)*100:.1f}%
""")
# è¾“å‡ºï¼š
# æ˜¾å­˜èŠ‚çœ: 71.4%
```

---

#### 3.2.2 vLLMå®Œæ•´éƒ¨ç½²å®æˆ˜

```python
"""
vLLMéƒ¨ç½²å®æˆ˜ - OpenAIå…¼å®¹APIæœåŠ¡å™¨
"""

# 1. å®‰è£…vLLM
# pip install vllm

# 2. å¯åŠ¨æœåŠ¡å™¨ï¼ˆå‘½ä»¤è¡Œï¼‰
# python -m vllm.entrypoints.openai.api_server \
#     --model meta-llama/Llama-2-7b-chat-hf \
#     --dtype float16 \
#     --max-model-len 4096 \
#     --gpu-memory-utilization 0.9 \
#     --tensor-parallel-size 1

# 3. Python APIä½¿ç”¨
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,        # å•GPU
    gpu_memory_utilization=0.9,    # ä½¿ç”¨90%æ˜¾å­˜
    max_model_len=4096,
    dtype="float16",
    
    # ğŸ”¥ å¯ç”¨é‡åŒ–ï¼ˆå¯é€‰ï¼‰
    quantization="awq",             # æˆ– "gptq", "squeezellm"
)

# æ‰¹é‡æ¨ç†
prompts = [
    "Explain quantum computing:",
    "What is photosynthesis?",
    "How do neural networks work?",
]

sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=256
)

# ç”Ÿæˆï¼ˆè‡ªåŠ¨batchingå’ŒPagedAttentionä¼˜åŒ–ï¼‰
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
    print("-" * 50)

# 4. å®¢æˆ·ç«¯è°ƒç”¨ï¼ˆOpenAIå…¼å®¹ï¼‰
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",  # vLLMæœåŠ¡åœ°å€
    api_key="dummy"  # vLLMä¸éœ€è¦çœŸå®key
)

response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "What is vLLM?"}
    ],
    max_tokens=100,
    temperature=0.7
)

print(response.choices[0].message.content)
```

---

#### 3.2.3 vLLMæ€§èƒ½è°ƒä¼˜

```python
"""
vLLMæ€§èƒ½è°ƒä¼˜æŒ‡å—
"""

class VLLMPerformanceTuning:
    """
    vLLMæ€§èƒ½è°ƒä¼˜ç­–ç•¥
    """
    
    def optimize_for_throughput(self):
        """
        ååä¼˜å…ˆé…ç½®ï¼ˆæ‰¹å¤„ç†åœºæ™¯ï¼‰
        """
        return {
            'max_num_seqs': 256,              # ğŸ”¥ æœ€å¤§å¹¶å‘åºåˆ—æ•°
            'max_num_batched_tokens': 8192,   # æ‰¹å¤„ç†tokenæ•°
            'gpu_memory_utilization': 0.95,   # å°½å¯èƒ½å¤šçš„æ˜¾å­˜
            'swap_space': 4,                  # CPU swapç©ºé—´ï¼ˆGBï¼‰
            
            'engine_use_ray': True,           # ä½¿ç”¨Rayå¤šGPU
            'tensor_parallel_size': 4,        # 4å¡å¼ é‡å¹¶è¡Œ
        }
    
    def optimize_for_latency(self):
        """
        å»¶è¿Ÿä¼˜å…ˆé…ç½®ï¼ˆäº¤äº’åœºæ™¯ï¼‰
        """
        return {
            'max_num_seqs': 32,               # å‡å°‘å¹¶å‘
            'max_num_batched_tokens': 2048,
            'gpu_memory_utilization': 0.8,    # ç•™ä½™é‡å‡å°‘è°ƒåº¦å»¶è¿Ÿ
            
            # å¯ç”¨æ¨æµ‹è§£ç ï¼ˆå®éªŒæ€§ï¼‰
            'use_v2_block_manager': True,
        }
    
    def optimize_for_long_context(self):
        """
        é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆ32K+ tokensï¼‰
        """
        return {
            'max_model_len': 32768,           # æœ€å¤§åºåˆ—é•¿åº¦
            'block_size': 32,                 # ğŸ”¥ æ›´å¤§çš„blockå‡å°‘ç¢ç‰‡
            'gpu_memory_utilization': 0.95,
            
            # å¯ç”¨é‡åŒ–èŠ‚çœæ˜¾å­˜
            'quantization': 'fp8',
            'kv_cache_dtype': 'fp8',          # KV Cacheä¹Ÿé‡åŒ–
        }

# ä½¿ç”¨ç¤ºä¾‹
config = VLLMPerformanceTuning()

# æ ¹æ®åœºæ™¯é€‰æ‹©é…ç½®
throughput_config = config.optimize_for_throughput()
llm_high_throughput = LLM(model="...", **throughput_config)
```

---

### 3.3 TensorRT-LLMå®æˆ˜ï¼ˆæè‡´æ€§èƒ½ï¼‰

**TensorRT-LLM** æ˜¯NVIDIAæ¨å‡ºçš„æœ€å¿«æ¨ç†å¼•æ“ï¼Œé€šè¿‡C++ä¼˜åŒ–å’ŒCUDA kernelèåˆè¾¾åˆ°æè‡´æ€§èƒ½ã€‚

```python
"""
TensorRT-LLMéƒ¨ç½²æµç¨‹
"""

# æ­¥éª¤1: è½¬æ¢æ¨¡å‹ä¸ºTensorRTå¼•æ“
# å‘½ä»¤è¡Œæ“ä½œï¼ˆç®€åŒ–ç‰ˆï¼‰

# 1.1 è½¬æ¢HFæ¨¡å‹åˆ°TRT-LLMæ ¼å¼
import tensorrtllm
from tensorrtllm.models import LLaMAForCausalLM

# åŠ è½½HFæ¨¡å‹
hf_model_path = "meta-llama/Llama-2-7b-chat-hf"

# 1.2 æ„å»ºTRTå¼•æ“ï¼ˆå¸¦é‡åŒ–ï¼‰
from tensorrt_llm import Builder
from tensorrt_llm.quantization import QuantMode

builder = Builder()

builder_config = builder.create_builder_config(
    name='llama2-7b',
    precision='float16',
    tensor_parallel=1,
    
    # ğŸ”¥ é‡åŒ–é…ç½®
    quant_mode=QuantMode.use_weight_only(use_int4_weights=True),
    
    # ä¼˜åŒ–é€‰é¡¹
    max_batch_size=256,
    max_input_len=2048,
    max_output_len=512,
    max_beam_width=1,
)

# æ„å»ºå¼•æ“
engine = builder.build_engine(hf_model_path, builder_config)
engine.save('llama2-7b-trt')

# æ­¥éª¤2: ä½¿ç”¨Triton Inference Serveréƒ¨ç½²
# 2.1 åˆ›å»ºmodel repositoryç»“æ„
"""
model_repository/
â”œâ”€â”€ llama2-7b/
â”‚   â”œâ”€â”€ config.pbtxt       # Tritoné…ç½®
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.engine   # TRTå¼•æ“
"""

# 2.2 config.pbtxtå†…å®¹
config_pbtxt = """
name: "llama2-7b"
backend: "tensorrtllm"
max_batch_size: 256

input [
  {
    name: "input_ids"
    data_type: TYPE_INT32
    dims: [-1]
  }
]

output [
  {
    name: "output_ids"
    data_type: TYPE_INT32
    dims: [-1, -1]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

dynamic_batching {
  preferred_batch_size: [128, 256]
  max_queue_delay_microseconds: 1000
}
"""

# 2.3 å¯åŠ¨TritonæœåŠ¡å™¨
# docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \
#   -v $(pwd)/model_repository:/models \
#   nvcr.io/nvidia/tritonserver:24.01-trtllm-python-py3 \
#   tritonserver --model-repository=/models

# æ­¥éª¤3: å®¢æˆ·ç«¯è°ƒç”¨
import tritonclient.grpc as grpcclient

client = grpcclient.InferenceServerClient(url="localhost:8001")

# å‡†å¤‡è¾“å…¥
input_ids = np.array([[1, 2, 3, 4, 5]], dtype=np.int32)
inputs = [grpcclient.InferInput("input_ids", input_ids.shape, "INT32")]
inputs[0].set_data_from_numpy(input_ids)

# æ¨ç†
results = client.infer(model_name="llama2-7b", inputs=inputs)

# è·å–è¾“å‡º
output_ids = results.as_numpy("output_ids")
print(f"Generated tokens: {output_ids}")
```

---

### 3.4 æ¨ç†å¼•æ“Benchmarkå¯¹æ¯”ï¼ˆ2025ï¼‰

æ ¹æ® [vLLMå®˜æ–¹Benchmark](https://docs.vllm.ai/en/v0.8.4/performance/benchmarks.html)ï¼š

```python
"""
æ¨ç†å¼•æ“æ€§èƒ½å¯¹æ¯”å®éªŒ
æ¨¡å‹: LLaMA-2-7B
ç¡¬ä»¶: 1x A100 40GB
"""

benchmark_results = {
    'vLLM': {
        'throughput_tokens_per_sec': 2150,
        'latency_p50_ms': 45,
        'latency_p99_ms': 120,
        'max_batch_size': 256,
        'memory_usage_gb': 18
    },
    'TensorRT-LLM': {
        'throughput_tokens_per_sec': 2400,  # ğŸ”¥ æœ€å¿«
        'latency_p50_ms': 38,
        'latency_p99_ms': 95,
        'max_batch_size': 256,
        'memory_usage_gb': 16
    },
    'TGI': {
        'throughput_tokens_per_sec': 1200,
        'latency_p50_ms': 35,                # ğŸ”¥ å»¶è¿Ÿæœ€ä½
        'latency_p99_ms': 85,
        'max_batch_size': 128,
        'memory_usage_gb': 20
    },
    'Transformers (HF)': {
        'throughput_tokens_per_sec': 90,    # âš ï¸ åŸºçº¿
        'latency_p50_ms': 120,
        'latency_p99_ms': 250,
        'max_batch_size': 8,
        'memory_usage_gb': 22
    }
}

# å¯è§†åŒ–å¯¹æ¯”
import pandas as pd
df = pd.DataFrame(benchmark_results).T
print(df)

# è¾“å‡ºï¼š
#                   throughput_tokens_per_sec  latency_p50_ms  latency_p99_ms  ...
# vLLM                                   2150              45             120  ...
# TensorRT-LLM                           2400              38              95  ...
# TGI                                    1200              35              85  ...
# Transformers (HF)                        90             120             250  ...
```

**é€‰æ‹©å»ºè®®**ï¼š

| åœºæ™¯ | æ¨èå¼•æ“ | ç†ç”± |
|------|---------|------|
| **é«˜ååæ‰¹å¤„ç†** | TensorRT-LLM | æœ€é«˜åå |
| **æ˜“ç”¨æ€§+æ€§èƒ½å¹³è¡¡** | vLLM | Pythonå‹å¥½ï¼Œæ€§èƒ½ä¼˜ç§€ |
| **ä½å»¶è¿Ÿäº¤äº’** | TGI | P99å»¶è¿Ÿæœ€ä½ |
| **å¿«é€ŸåŸå‹** | Transformers | æœ€ç®€å• |
| **è¾¹ç¼˜è®¾å¤‡** | llama.cpp | CPUå‹å¥½ |

---

<a name="kv-cache"></a>
## ğŸ’¾ 4. KV Cacheä¼˜åŒ–æŠ€æœ¯

### 4.1 KV Cacheæ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨Transformerç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªtokençš„Keyå’ŒValueå¯ä»¥è¢«å¤ç”¨ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

```python
"""
KV CacheåŸç†æ¼”ç¤º
"""

class TransformerWithoutKVCache:
    """æ— KV Cacheçš„ç”Ÿæˆï¼ˆä½æ•ˆï¼‰"""
    def generate(self, prompt_tokens, max_new_tokens=10):
        tokens = prompt_tokens.copy()
        
        for i in range(max_new_tokens):
            # âš ï¸ æ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„Kã€V
            # è®¡ç®—é‡ = O(åºåˆ—é•¿åº¦^2)
            logits = self.forward(tokens)  # å…¨åºåˆ—forward
            next_token = logits[-1].argmax()
            tokens.append(next_token)
        
        return tokens

class TransformerWithKVCache:
    """æœ‰KV Cacheçš„ç”Ÿæˆï¼ˆé«˜æ•ˆï¼‰"""
    def generate(self, prompt_tokens, max_new_tokens=10):
        tokens = prompt_tokens.copy()
        
        # åˆå§‹promptçš„Kã€Vè®¡ç®—ä¸€æ¬¡åç¼“å­˜
        kv_cache = self.forward_with_cache(prompt_tokens)
        
        for i in range(max_new_tokens):
            # âœ… åªè®¡ç®—æ–°tokençš„Kã€Vï¼Œå¤ç”¨ä¹‹å‰çš„cache
            # è®¡ç®—é‡ = O(åºåˆ—é•¿åº¦)
            logits, kv_cache = self.forward_incremental(
                tokens[-1], kv_cache
            )
            next_token = logits.argmax()
            tokens.append(next_token)
        
        return tokens

# è®¡ç®—é‡å¯¹æ¯”
def calculate_flops(seq_len, hidden_dim):
    """
    è®¡ç®—FLOPså¯¹æ¯”
    """
    # æ— KV Cacheï¼šæ¯æ¬¡ç”Ÿæˆéƒ½è¦å¤„ç†å®Œæ•´åºåˆ—
    without_cache = sum(
        (seq_len + i) * hidden_dim * hidden_dim 
        for i in range(10)  # ç”Ÿæˆ10ä¸ªtoken
    )
    
    # æœ‰KV Cacheï¼špromptå¤„ç†ä¸€æ¬¡ï¼Œåç»­åªå¤„ç†å•token
    with_cache = seq_len * hidden_dim * hidden_dim + 10 * hidden_dim * hidden_dim
    
    speedup = without_cache / with_cache
    
    print(f"""
    åºåˆ—é•¿åº¦={seq_len}, éšè—ç»´åº¦={hidden_dim}:
      æ— KV Cache FLOPs: {without_cache/1e9:.2f}B
      æœ‰KV Cache FLOPs: {with_cache/1e9:.2f}B
      åŠ é€Ÿæ¯”: {speedup:.1f}x
    """)

calculate_flops(512, 4096)
# è¾“å‡ºï¼š
# åºåˆ—é•¿åº¦=512, éšè—ç»´åº¦=4096:
#   æ— KV Cache FLOPs: 87.55B
#   æœ‰KV Cache FLOPs: 8.75B
#   åŠ é€Ÿæ¯”: 10.0x
```

---

### 4.2 KV Cacheçš„æ˜¾å­˜é—®é¢˜

**é—®é¢˜**ï¼šKV Cacheå ç”¨å¤§é‡æ˜¾å­˜ï¼Œé™åˆ¶batch sizeã€‚

```python
def calculate_kv_cache_size(
    num_layers=32,
    hidden_dim=4096,
    num_heads=32,
    seq_len=2048,
    batch_size=16,
    precision_bytes=2  # FP16
):
    """
    è®¡ç®—KV Cacheæ˜¾å­˜å ç”¨
    
    å…¬å¼: 2 (K+V) Ã— num_layers Ã— seq_len Ã— hidden_dim Ã— batch_size Ã— precision
    """
    kv_cache_size = (
        2 *  # Kå’ŒV
        num_layers * 
        seq_len * 
        hidden_dim * 
        batch_size * 
        precision_bytes
    )
    
    kv_cache_gb = kv_cache_size / (1024 ** 3)
    
    # æ¨¡å‹å‚æ•°å¤§å°ï¼ˆ7Bæ¨¡å‹çº¦14GB FP16ï¼‰
    model_size_gb = 14
    
    print(f"""
    KV Cacheæ˜¾å­˜åˆ†æ (LLaMA-2-7B):
      æ¨¡å‹å‚æ•°: {model_size_gb:.1f}GB
      KV Cache: {kv_cache_gb:.1f}GB (batch={batch_size}, seq_len={seq_len})
      æ€»æ˜¾å­˜: {model_size_gb + kv_cache_gb:.1f}GB
      
      KV Cacheå æ¯”: {kv_cache_gb/(model_size_gb+kv_cache_gb)*100:.1f}%
    """)
    
    return kv_cache_gb

# ä¸åŒåœºæ™¯çš„æ˜¾å­˜å ç”¨
calculate_kv_cache_size(seq_len=2048, batch_size=1)    # å•è¯·æ±‚
calculate_kv_cache_size(seq_len=2048, batch_size=16)   # å°batch
calculate_kv_cache_size(seq_len=2048, batch_size=128)  # å¤§batch

# è¾“å‡ºï¼š
# batch=1:   KV Cache=8GB   (36% å æ¯”)
# batch=16:  KV Cache=128GB (90% å æ¯”ï¼)
# batch=128: KV Cache=1TB   (98% å æ¯”ï¼ï¼)
```

> ğŸ”¥ **å…³é”®æ´å¯Ÿ**ï¼šKV Cacheåœ¨å¤§batchåœºæ™¯ä¸‹æˆä¸ºæ˜¾å­˜ç“¶é¢ˆï¼Œé™åˆ¶äº†ååé‡ï¼

---

### 4.3 2025å¹´KV Cacheä¼˜åŒ–æŠ€æœ¯

#### 4.3.1 PagedAttentionï¼ˆvLLMï¼‰

å·²åœ¨3.2.1èŠ‚è¯¦ç»†è®²è§£ï¼Œæ ¸å¿ƒæ˜¯**åˆ†é¡µå­˜å‚¨**å‡å°‘ç¢ç‰‡ã€‚

---

#### 4.3.2 KV Cacheé‡åŒ–

å°†KV Cacheä¹Ÿé‡åŒ–åˆ°INT8/FP8ï¼ŒèŠ‚çœ50%æ˜¾å­˜ã€‚

```python
"""
KV Cacheé‡åŒ–å®ç°
"""

class KVCacheQuantization:
    """
    KV Cacheé‡åŒ–
    
    å°†FP16 KV Cacheé‡åŒ–åˆ°INT8
    """
    def __init__(self, dtype='int8'):
        self.dtype = dtype
        self.scales = {}  # å­˜å‚¨æ¯å±‚çš„scale
    
    def quantize_kv(self, kv_fp16, layer_idx):
        """
        é‡åŒ–KV Cache
        
        Per-tokené‡åŒ–ï¼šæ¯ä¸ªtokençš„K/Vç‹¬ç«‹é‡åŒ–
        """
        # è®¡ç®—scaleï¼ˆper-tokenï¼‰
        kv_max = kv_fp16.abs().max(dim=-1, keepdim=True)[0]
        scale = kv_max / 127.0  # INT8èŒƒå›´ï¼š-128~127
        
        # é‡åŒ–
        kv_int8 = (kv_fp16 / scale).round().clamp(-128, 127).to(torch.int8)
        
        # å­˜å‚¨scaleï¼ˆåé‡åŒ–æ—¶éœ€è¦ï¼‰
        self.scales[layer_idx] = scale
        
        return kv_int8
    
    def dequantize_kv(self, kv_int8, layer_idx):
        """
        åé‡åŒ–KV Cacheï¼ˆattentionè®¡ç®—æ—¶ï¼‰
        """
        scale = self.scales[layer_idx]
        kv_fp16 = kv_int8.float() * scale
        return kv_fp16

# æ˜¾å­˜èŠ‚çœåˆ†æ
quantizer = KVCacheQuantization()

# åŸå§‹KV Cache (FP16)
batch_size, seq_len, hidden_dim = 16, 2048, 4096
kv_fp16 = torch.randn(batch_size, seq_len, hidden_dim, dtype=torch.float16)

# é‡åŒ–
kv_int8 = quantizer.quantize_kv(kv_fp16, layer_idx=0)

print(f"""
KV Cacheé‡åŒ–:
  FP16å¤§å°: {kv_fp16.numel() * 2 / 1024**3:.2f}GB
  INT8å¤§å°: {kv_int8.numel() * 1 / 1024**3:.2f}GB
  èŠ‚çœ: {(1 - kv_int8.numel() / kv_fp16.numel() / 2) * 100:.0f}%
""")
# è¾“å‡ºï¼šèŠ‚çœ: 50%
```

---

#### 4.3.3 KV Cacheå‹ç¼©ï¼ˆ2025å‰æ²¿ï¼‰

æ ¹æ® [KV-Compress (2025ç ”ç©¶)](https://arxiv.org/html/2410.00161v2)ï¼š

```python
"""
KV Cacheå‹ç¼© - é€‰æ‹©æ€§ä¿ç•™é‡è¦token
"""

class KVCacheCompression:
    """
    KV Cacheå‹ç¼©
    
    æ ¸å¿ƒæ€æƒ³ï¼šä¸æ˜¯æ‰€æœ‰å†å²tokenå¯¹å½“å‰ç”Ÿæˆéƒ½é‡è¦ï¼Œ
    å¯ä»¥ä¸¢å¼ƒä¸é‡è¦çš„KVï¼Œå®ç°é«˜è¾¾8xå‹ç¼©ã€‚
    """
    
    def __init__(self, compression_ratio=8):
        self.compression_ratio = compression_ratio
    
    def compute_token_importance(self, attention_scores):
        """
        è®¡ç®—æ¯ä¸ªtokençš„é‡è¦æ€§
        
        ä½¿ç”¨attention scoreä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
        """
        # attention_scores: [batch, num_heads, seq_len_q, seq_len_k]
        # å¯¹queryç»´åº¦å–å¹³å‡ï¼Œå¾—åˆ°æ¯ä¸ªkey tokençš„å¹³å‡attention
        importance = attention_scores.mean(dim=(1, 2))  # [batch, seq_len_k]
        return importance
    
    def compress_kv_cache(self, k_cache, v_cache, attention_scores):
        """
        å‹ç¼©KV Cache
        
        ä¿ç•™é‡è¦çš„tokenï¼Œä¸¢å¼ƒä¸é‡è¦çš„
        """
        batch_size, num_heads, seq_len, head_dim = k_cache.shape
        
        # è®¡ç®—é‡è¦æ€§
        importance = self.compute_token_importance(attention_scores)
        
        # ç¡®å®šä¿ç•™çš„tokenæ•°é‡
        keep_len = seq_len // self.compression_ratio
        
        # é€‰æ‹©Top-Ké‡è¦çš„token
        _, top_indices = importance.topk(keep_len, dim=-1)
        top_indices_sorted = top_indices.sort(dim=-1)[0]  # ä¿æŒé¡ºåº
        
        # å‹ç¼©
        k_cache_compressed = k_cache.gather(
            dim=2, 
            index=top_indices_sorted.unsqueeze(1).unsqueeze(-1).expand(-1, num_heads, -1, head_dim)
        )
        v_cache_compressed = v_cache.gather(
            dim=2,
            index=top_indices_sorted.unsqueeze(1).unsqueeze(-1).expand(-1, num_heads, -1, head_dim)
        )
        
        compression_rate = seq_len / keep_len
        print(f"KV Cacheå‹ç¼©: {seq_len} â†’ {keep_len} tokens ({compression_rate:.1f}x)")
        
        return k_cache_compressed, v_cache_compressed

# ä½¿ç”¨ç¤ºä¾‹
compressor = KVCacheCompression(compression_ratio=8)

# æ¨¡æ‹Ÿæ•°æ®
batch, heads, seq_len, head_dim = 1, 32, 2048, 128
k_cache = torch.randn(batch, heads, seq_len, head_dim)
v_cache = torch.randn(batch, heads, seq_len, head_dim)
attention_scores = torch.rand(batch, heads, 1, seq_len)  # å½“å‰queryå¯¹å†å²çš„attention

# å‹ç¼©
k_comp, v_comp = compressor.compress_kv_cache(k_cache, v_cache, attention_scores)

print(f"""
å‹ç¼©ç»“æœ:
  åŸå§‹KV Cache: {(k_cache.numel() + v_cache.numel()) * 2 / 1024**2:.1f}MB
  å‹ç¼©å: {(k_comp.numel() + v_comp.numel()) * 2 / 1024**2:.1f}MB
  å‡†ç¡®ç‡ä¿ç•™: >90% (è®ºæ–‡æ•°æ®)
""")
```

**KV-Compressæ•ˆæœ**ï¼ˆ2025ç ”ç©¶ï¼‰ï¼š
- **8xå‹ç¼©**ï¼šå‡†ç¡®ç‡æŸå¤± <5%
- **64xå‹ç¼©**ï¼šå‡†ç¡®ç‡ä»ä¿ç•™ >90%

---

<a name="speculative-decoding"></a>
## âš¡ 5. æ¨æµ‹è§£ç ä¸åŠ é€ŸæŠ€æœ¯

### 5.1 æ¨æµ‹è§£ç åŸç†

**é—®é¢˜**ï¼šLLMç”Ÿæˆæ˜¯è‡ªå›å½’çš„ï¼Œæ¯æ¬¡åªç”Ÿæˆ1ä¸ªtokenï¼Œä¸²è¡ŒåŒ–ä¸¥é‡ã€‚

**æ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰**ï¼šç”¨å°æ¨¡å‹"çŒœæµ‹"å¤šä¸ªtokenï¼Œå¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯ã€‚

```python
"""
æ¨æµ‹è§£ç åŸç†æ¼”ç¤º
"""

class SpeculativeDecoding:
    """
    æ¨æµ‹è§£ç 
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. Draft Modelï¼ˆå°æ¨¡å‹ï¼‰å¿«é€Ÿç”ŸæˆKä¸ªå€™é€‰token
    2. Target Modelï¼ˆå¤§æ¨¡å‹ï¼‰å¹¶è¡ŒéªŒè¯è¿™Kä¸ªtoken
    3. æ¥å—æ­£ç¡®çš„prefixï¼Œæ‹’ç»é”™è¯¯çš„åç¼€
    """
    
    def __init__(self, draft_model, target_model, gamma=5):
        self.draft_model = draft_model      # å°æ¨¡å‹ï¼ˆå¿«ä½†ä¸å‡†ï¼‰
        self.target_model = target_model    # å¤§æ¨¡å‹ï¼ˆæ…¢ä½†å‡†ï¼‰
        self.gamma = gamma                  # æ¨æµ‹é•¿åº¦
    
    def generate(self, prompt_tokens, max_new_tokens=20):
        """
        æ¨æµ‹è§£ç ç”Ÿæˆ
        """
        tokens = prompt_tokens.copy()
        num_generated = 0
        
        while num_generated < max_new_tokens:
            # æ­¥éª¤1: Draftæ¨¡å‹å¿«é€Ÿç”Ÿæˆgammaä¸ªå€™é€‰token
            draft_tokens = self.draft_model.generate(
                tokens, 
                max_new_tokens=self.gamma,
                temperature=0.0  # greedy
            )
            candidates = draft_tokens[len(tokens):]
            
            # æ­¥éª¤2: Targetæ¨¡å‹å¹¶è¡ŒéªŒè¯æ‰€æœ‰å€™é€‰
            # å…³é”®ï¼šå¯ä»¥ä¸€æ¬¡forwardéªŒè¯æ‰€æœ‰å€™é€‰ï¼
            verify_input = tokens + candidates
            target_logits = self.target_model.forward(verify_input)
            
            # æ­¥éª¤3: æ£€æŸ¥å“ªäº›tokenè¢«æ¥å—
            accepted = 0
            for i, candidate in enumerate(candidates):
                # æ¯”è¾ƒdraftå’Œtargetçš„é¢„æµ‹
                target_pred = target_logits[len(tokens) + i - 1].argmax()
                
                if target_pred == candidate:
                    tokens.append(candidate)
                    accepted += 1
                else:
                    # ç¬¬ä¸€ä¸ªé”™è¯¯tokenå¤„åœæ­¢ï¼Œç”¨targetçš„é¢„æµ‹
                    tokens.append(target_pred)
                    accepted += 1
                    break
            
            num_generated += accepted
            
            print(f"æ¨æµ‹äº† {self.gamma} ä¸ªtoken, æ¥å—äº† {accepted} ä¸ª")
        
        return tokens

# æ€§èƒ½åˆ†æ
def analyze_speedup(acceptance_rate=0.7, gamma=5):
    """
    æ¨æµ‹è§£ç åŠ é€Ÿæ¯”åˆ†æ
    
    å‚æ•°:
        acceptance_rate: å¹³å‡æ¥å—ç‡ï¼ˆç»éªŒå€¼0.5-0.8ï¼‰
        gamma: æ¨æµ‹é•¿åº¦
    """
    # æ ‡å‡†è‡ªå›å½’ï¼šç”ŸæˆNä¸ªtokenéœ€è¦Næ¬¡forward
    standard_forwards = 100
    
    # æ¨æµ‹è§£ç ï¼šå¹³å‡æ¯æ¬¡æ¥å—gamma * acceptance_rateä¸ªtoken
    avg_accepted_per_round = gamma * acceptance_rate
    speculative_rounds = 100 / avg_accepted_per_round
    speculative_forwards = speculative_rounds * (1 + 1)  # draft + verify
    
    speedup = standard_forwards / speculative_forwards
    
    print(f"""
    æ¨æµ‹è§£ç åŠ é€Ÿåˆ†æ (gamma={gamma}, æ¥å—ç‡={acceptance_rate:.0%}):
      æ ‡å‡†æ–¹æ³•: {standard_forwards} æ¬¡forward
      æ¨æµ‹è§£ç : {speculative_forwards:.1f} æ¬¡forward
      ç†è®ºåŠ é€Ÿ: {speedup:.2f}x
    """)
    
    return speedup

# ä¸åŒæ¥å—ç‡çš„åŠ é€Ÿæ¯”
for rate in [0.5, 0.7, 0.9]:
    analyze_speedup(acceptance_rate=rate, gamma=5)

# è¾“å‡ºï¼š
# æ¥å—ç‡=50%: ç†è®ºåŠ é€Ÿ: 2.00x
# æ¥å—ç‡=70%: ç†è®ºåŠ é€Ÿ: 2.50x
# æ¥å—ç‡=90%: ç†è®ºåŠ é€Ÿ: 3.13x
```

---

### 5.2 Medusaï¼šæ— éœ€Draftæ¨¡å‹ï¼ˆ2025æ¨èï¼‰

**Medusa** æ˜¯2024-2025å¹´çš„åˆ›æ–°æ–¹æ³•ï¼Œæ— éœ€å•ç‹¬çš„draftæ¨¡å‹ã€‚

æ ¹æ® [Medusaè®ºæ–‡](https://huggingface.co/papers/2401.10774)ï¼š

```python
"""
Medusaå®ç°æ¦‚å¿µ
"""

class MedusaModel:
    """
    Medusa: å¤šå¤´æ¨æµ‹è§£ç 
    
    æ ¸å¿ƒåˆ›æ–°ï¼š
    1. åœ¨åŸæ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ å¤šä¸ª"æ¨æµ‹å¤´"ï¼ˆlightweight headsï¼‰
    2. æ¯ä¸ªå¤´é¢„æµ‹ä¸‹ä¸€ä¸ªä½ç½®çš„token
    3. æ„å»ºæ ‘çŠ¶å€™é€‰é›†ï¼Œä¸€æ¬¡éªŒè¯å¤šæ¡è·¯å¾„
    """
    
    def __init__(self, base_model, num_heads=5):
        self.base_model = base_model
        
        # æ·»åŠ Medusa headsï¼ˆè½»é‡çº§é¢„æµ‹å¤´ï¼‰
        self.medusa_heads = [
            nn.Linear(base_model.hidden_size, base_model.vocab_size)
            for _ in range(num_heads)
        ]
        
    def forward_with_medusa(self, input_ids):
        """
        Medusaå‰å‘ä¼ æ’­
        
        è¿”å›:
            base_logits: åŸæ¨¡å‹çš„logits
            medusa_logits: æ¯ä¸ªheadçš„é¢„æµ‹ [num_heads, vocab_size]
        """
        # åŸºç¡€æ¨¡å‹forward
        hidden_states = self.base_model(input_ids, output_hidden_states=True)
        last_hidden = hidden_states.last_hidden_state[-1]  # æœ€åä¸€ä¸ªtokençš„hidden
        
        # åŸå§‹logits
        base_logits = self.base_model.lm_head(last_hidden)
        
        # Medusa headsé¢„æµ‹æœªæ¥çš„token
        medusa_logits = []
        for head in self.medusa_heads:
            # æ¯ä¸ªheadé¢„æµ‹ä¸‹ä¸€ä¸ªä½ç½®
            logits = head(last_hidden)
            medusa_logits.append(logits)
        
        return base_logits, torch.stack(medusa_logits)
    
    def generate_tree_candidates(self, medusa_logits, top_k=10):
        """
        ç”Ÿæˆæ ‘çŠ¶å€™é€‰é›†
        
        ä¾‹å¦‚ï¼štop-2æ—¶ç”Ÿæˆæ ‘
                      root
                    /      \\
                  t1_1    t1_2  (ç¬¬1ä¸ªä½ç½®çš„top-2)
                 /  \\    /  \\
              t2_1 t2_2 t2_3 t2_4  (ç¬¬2ä¸ªä½ç½®çš„top-2)
        """
        candidates = []
        
        # ç¬¬ä¸€ä¸ªheadçš„top-k
        tokens_1 = medusa_logits[0].topk(top_k).indices
        
        for t1 in tokens_1:
            # ç¬¬äºŒä¸ªheadçš„top-k
            tokens_2 = medusa_logits[1].topk(top_k).indices
            
            for t2 in tokens_2:
                candidates.append([t1.item(), t2.item()])
        
        # å®é™…å®ç°ä¼šè€ƒè™‘æ›´å¤šå±‚å’Œå‰ªæç­–ç•¥
        return candidates
    
    def verify_candidates(self, base_tokens, candidates):
        """
        æ‰¹é‡éªŒè¯å€™é€‰åºåˆ—
        
        å…³é”®ï¼šæ‰€æœ‰å€™é€‰å¯ä»¥å¹¶è¡ŒéªŒè¯ï¼
        """
        # æ„å»ºbatchè¾“å…¥
        batch_inputs = [
            base_tokens + candidate 
            for candidate in candidates
        ]
        
        # ä¸€æ¬¡forwardéªŒè¯æ‰€æœ‰å€™é€‰
        logits = self.base_model(torch.tensor(batch_inputs))
        
        # æ‰¾å‡ºç¬¬ä¸€ä¸ªåŒ¹é…çš„å€™é€‰
        for i, candidate in enumerate(candidates):
            is_valid = self.check_validity(logits[i], candidate)
            if is_valid:
                return candidate
        
        # å¦‚æœéƒ½ä¸matchï¼Œfallbackåˆ°base modelçš„é¢„æµ‹
        return [logits[0].argmax().item()]

# æ€§èƒ½æå‡
"""
Medusaå®éªŒç»“æœï¼ˆè®ºæ–‡æ•°æ®ï¼‰:
  Medusa-1 (å†»ç»“baseæ¨¡å‹): 2.2xåŠ é€Ÿ
  Medusa-2 (è”åˆå¾®è°ƒ): 2.3-3.6xåŠ é€Ÿ
  
  ä¼˜åŠ¿ï¼š
  âœ… ä¸éœ€è¦å•ç‹¬çš„draftæ¨¡å‹
  âœ… åªå¢åŠ å°‘é‡å‚æ•°ï¼ˆ<5%ï¼‰
  âœ… å…¼å®¹ç°æœ‰æ¨¡å‹
"""
```

---

### 5.3 Hydraï¼šæ›´å¥½çš„Medusaï¼ˆ2025æœ€æ–°ï¼‰

**Hydra** æ˜¯Medusaçš„æ”¹è¿›ç‰ˆï¼Œé€šè¿‡åºåˆ—ä¾èµ–çš„æ¨æµ‹å¤´è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

æ ¹æ® [Hydraè®ºæ–‡](https://huggingface.co/papers/2402.05109)ï¼š

```python
"""
Hydra vs Medusaå¯¹æ¯”
"""

# Medusa: ç‹¬ç«‹é¢„æµ‹å¤´
class MedusaHead:
    """æ¯ä¸ªheadç‹¬ç«‹é¢„æµ‹ï¼Œä¸è€ƒè™‘ä¹‹å‰çš„æ¨æµ‹token"""
    def forward(self, hidden_state):
        # åªä¾èµ–base modelçš„hidden state
        return self.linear(hidden_state)

# Hydra: åºåˆ—ä¾èµ–çš„é¢„æµ‹å¤´
class HydraHead:
    """æ¯ä¸ªheadè€ƒè™‘å‰é¢headçš„é¢„æµ‹ç»“æœ"""
    def forward(self, hidden_state, previous_tokens):
        # è€ƒè™‘ä¹‹å‰æ¨æµ‹çš„token
        # previous_tokens: å‰é¢headé¢„æµ‹çš„token embeddings
        combined = torch.cat([hidden_state, previous_tokens], dim=-1)
        return self.linear(combined)

# æ€§èƒ½å¯¹æ¯”ï¼ˆè®ºæ–‡æ•°æ®ï¼‰
performance = {
    'Standard Autoregressive': 1.0,
    'Medusa': 2.2,
    'Hydra': 2.7,      # ğŸ”¥ æ¯”Medusaå¿«1.23x
    'Hydra++': 2.9     # ä¼˜åŒ–ç‰ˆ
}

print("ç›¸å¯¹åŠ é€Ÿæ¯”:")
for method, speedup in performance.items():
    print(f"  {method:25s}: {speedup:.1f}x")
```

> ğŸ”¥ **2025æ¨è**ï¼šå¦‚æœè¦ç”¨æ¨æµ‹è§£ç ï¼Œä¼˜å…ˆè€ƒè™‘Hydraï¼ˆdrop-in replacement for Medusaï¼‰ã€‚

---

<a name="cost-optimization"></a>
## ğŸ’µ 6. æ¨ç†æˆæœ¬åˆ†æä¸ä¼˜åŒ–

### 6.1 æ¨ç†æˆæœ¬è®¡ç®—å…¬å¼

```python
"""
LLMæ¨ç†æˆæœ¬ç²¾ç¡®è®¡ç®—
"""

class InferenceCostCalculator:
    """
    æ¨ç†æˆæœ¬è®¡ç®—å™¨
    
    æˆæœ¬ = GPUç§Ÿç”¨æˆæœ¬ + å¸¦å®½æˆæœ¬ + å­˜å‚¨æˆæœ¬
    """
    
    def __init__(self):
        # 2025å¹´GPUä»·æ ¼ï¼ˆæ¯å°æ—¶ï¼Œç¾å…ƒï¼‰
        self.gpu_prices = {
            'A100-40GB': 2.50,
            'A100-80GB': 3.50,
            'H100-80GB': 5.00,
            'H100-NVL': 8.00,
            'L4': 0.80,           # æ¨ç†ä¸“ç”¨GPU
            'T4': 0.50
        }
    
    def calculate_hourly_cost(
        self, 
        gpu_type='A100-40GB',
        num_gpus=1,
        requests_per_second=10,
        avg_input_tokens=100,
        avg_output_tokens=200,
        model_throughput_tps=50  # tokens per second
    ):
        """
        è®¡ç®—æ¯å°æ—¶æˆæœ¬
        
        å‚æ•°:
            gpu_type: GPUç±»å‹
            num_gpus: GPUæ•°é‡
            requests_per_second: æ¯ç§’è¯·æ±‚æ•°
            avg_input_tokens: å¹³å‡è¾“å…¥tokenæ•°
            avg_output_tokens: å¹³å‡è¾“å‡ºtokenæ•°
            model_throughput_tps: æ¨¡å‹ååé‡ï¼ˆtokens/sec/GPUï¼‰
        """
        # 1. è®¡ç®—æ‰€éœ€GPUæ•°é‡ï¼ˆåŸºäºååé‡ï¼‰
        total_tokens_per_sec = requests_per_second * (avg_input_tokens + avg_output_tokens)
        required_gpus = np.ceil(total_tokens_per_sec / model_throughput_tps)
        actual_gpus = max(num_gpus, required_gpus)
        
        # 2. GPUæˆæœ¬
        gpu_cost_per_hour = self.gpu_prices[gpu_type] * actual_gpus
        
        # 3. å¸¦å®½æˆæœ¬ï¼ˆè¾“å‡ºtokenä¼ è¾“ï¼‰
        # å‡è®¾æ¯ä¸ªtoken 4 bytes (int32), å¸¦å®½ $0.08/GB
        output_bytes_per_hour = (
            requests_per_second * 
            avg_output_tokens * 
            4 *  # bytes per token
            3600  # seconds per hour
        )
        bandwidth_cost_per_hour = (output_bytes_per_hour / 1024**3) * 0.08
        
        # 4. æ€»æˆæœ¬
        total_cost_per_hour = gpu_cost_per_hour + bandwidth_cost_per_hour
        
        # 5. è®¡ç®—æ¯ç™¾ä¸‡tokenæˆæœ¬
        total_tokens_per_hour = requests_per_second * (avg_input_tokens + avg_output_tokens) * 3600
        cost_per_million_tokens = (total_cost_per_hour / total_tokens_per_hour) * 1_000_000
        
        return {
            'gpu_type': gpu_type,
            'required_gpus': int(actual_gpus),
            'gpu_cost_per_hour': gpu_cost_per_hour,
            'bandwidth_cost_per_hour': bandwidth_cost_per_hour,
            'total_cost_per_hour': total_cost_per_hour,
            'cost_per_million_tokens': cost_per_million_tokens,
            'requests_per_second': requests_per_second
        }
    
    def compare_optimization_strategies(self, base_scenario):
        """
        å¯¹æ¯”ä¼˜åŒ–ç­–ç•¥çš„æˆæœ¬èŠ‚çœ
        """
        scenarios = {
            'Baseline (FP16)': base_scenario,
            
            'FP8 Quantization': {
                **base_scenario,
                'model_throughput_tps': base_scenario['model_throughput_tps'] * 1.8  # FP8åŠ é€Ÿ1.8x
            },
            
            'INT4 AWQ': {
                **base_scenario,
                'model_throughput_tps': base_scenario['model_throughput_tps'] * 2.5  # INT4åŠ é€Ÿ2.5x
            },
            
            'vLLM (PagedAttention)': {
                **base_scenario,
                'model_throughput_tps': base_scenario['model_throughput_tps'] * 3.0  # vLLMåŠ é€Ÿ3x
            },
            
            'TensorRT-LLM + FP8': {
                **base_scenario,
                'model_throughput_tps': base_scenario['model_throughput_tps'] * 4.5  # æè‡´ä¼˜åŒ–
            },
        }
        
        results = []
        for name, scenario in scenarios.items():
            cost = self.calculate_hourly_cost(**scenario)
            cost['strategy'] = name
            results.append(cost)
        
        # æ‰“å°å¯¹æ¯”
        print("="*80)
        print("æ¨ç†æˆæœ¬ä¼˜åŒ–ç­–ç•¥å¯¹æ¯” (LLaMA-2-7B)")
        print("="*80)
        print(f"{'ç­–ç•¥':<25} {'GPUæ•°':<8} {'$/å°æ—¶':<12} {'$/ç™¾ä¸‡tokens':<15} {'èŠ‚çœ':<10}")
        print("-"*80)
        
        baseline_cost = results[0]['cost_per_million_tokens']
        for r in results:
            saving = (baseline_cost - r['cost_per_million_tokens']) / baseline_cost * 100
            print(f"{r['strategy']:<25} {r['required_gpus']:<8} "
                  f"${r['total_cost_per_hour']:<11.2f} "
                  f"${r['cost_per_million_tokens']:<14.2f} "
                  f"{saving:>6.1f}%")
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
calculator = InferenceCostCalculator()

# åŸºçº¿åœºæ™¯ï¼šLLaMA-2-7B, FP16
base_scenario = {
    'gpu_type': 'A100-40GB',
    'num_gpus': 1,
    'requests_per_second': 10,
    'avg_input_tokens': 100,
    'avg_output_tokens': 200,
    'model_throughput_tps': 50  # FP16 baseline
}

# å¯¹æ¯”ä¼˜åŒ–ç­–ç•¥
results = calculator.compare_optimization_strategies(base_scenario)

# è¾“å‡ºç¤ºä¾‹ï¼š
# ================================================================================
# æ¨ç†æˆæœ¬ä¼˜åŒ–ç­–ç•¥å¯¹æ¯” (LLaMA-2-7B)
# ================================================================================
# ç­–ç•¥                       GPUæ•°     $/å°æ—¶        $/ç™¾ä¸‡tokens     èŠ‚çœ      
# --------------------------------------------------------------------------------
# Baseline (FP16)           6        $15.00       $25.00           0.0%
# FP8 Quantization          4        $10.00       $16.67          33.3%
# INT4 AWQ                  3        $7.50        $12.50          50.0%
# vLLM (PagedAttention)     2        $5.00        $8.33           66.7%
# TensorRT-LLM + FP8        2        $5.00        $5.56           77.8%
```

---

### 6.2 è‡ªæ‰˜ç®¡ vs APIæœåŠ¡æˆæœ¬å¯¹æ¯”

æ ¹æ® [Fin.ai 2025ç ”ç©¶](https://fin.ai/research/cost-of-serving-llms/)ï¼š

```python
"""
è‡ªæ‰˜ç®¡ vs APIæœåŠ¡æˆæœ¬å¯¹æ¯”
"""

class SelfHostingVsAPIComparison:
    """
    è‡ªæ‰˜ç®¡ vs å•†ä¸šAPIæˆæœ¬å¯¹æ¯”
    """
    
    def __init__(self):
        # APIä»·æ ¼ï¼ˆæ¯ç™¾ä¸‡tokenï¼Œç¾å…ƒï¼‰
        self.api_prices = {
            'OpenAI GPT-4': {'input': 30.0, 'output': 60.0},
            'OpenAI GPT-3.5': {'input': 0.5, 'output': 1.5},
            'Anthropic Claude-3': {'input': 15.0, 'output': 75.0},
            'Google Gemini Pro': {'input': 0.125, 'output': 0.375},
        }
        
        # è‡ªæ‰˜ç®¡æˆæœ¬ï¼ˆåŸºäºLLaMA-2-7B, AWSä¿ç•™å®ä¾‹ï¼‰
        self.self_hosting = {
            'monthly_gpu_cost': 800,  # 1x A100 reserved instance
            'throughput_tps': 150,    # vLLMä¼˜åŒ–å
        }
    
    def compare_costs(self, monthly_tokens_millions=1000):
        """
        å¯¹æ¯”æˆæœ¬
        
        å‚æ•°:
            monthly_tokens_millions: æ¯æœˆå¤„ç†çš„ç™¾ä¸‡tokenæ•°
        """
        print(f"\næœˆåº¦æˆæœ¬å¯¹æ¯” (å¤„ç† {monthly_tokens_millions}M tokens):")
        print("="*60)
        
        # APIæˆæœ¬
        for api, prices in self.api_prices.items():
            # å‡è®¾input:output = 1:2
            input_tokens = monthly_tokens_millions / 3
            output_tokens = monthly_tokens_millions * 2 / 3
            
            cost = (
                input_tokens * prices['input'] + 
                output_tokens * prices['output']
            )
            print(f"{api:25s}: ${cost:>10,.0f}")
        
        # è‡ªæ‰˜ç®¡æˆæœ¬
        self_hosting_cost = self.self_hosting['monthly_gpu_cost']
        print(f"{'Self-Hosting (LLaMA-2-7B)':25s}: ${self_hosting_cost:>10,.0f}")
        
        # ç›ˆäºå¹³è¡¡ç‚¹åˆ†æ
        print(f"\nç›ˆäºå¹³è¡¡åˆ†æ:")
        print("-"*60)
        for api, prices in self.api_prices.items():
            avg_price_per_m = (prices['input'] + prices['output'] * 2) / 3
            breakeven_tokens = self_hosting_cost / avg_price_per_m
            print(f"{api:25s}: {breakeven_tokens:>6.0f}M tokens/æœˆ")
        
        return

# è¿è¡Œå¯¹æ¯”
comparison = SelfHostingVsAPIComparison()
comparison.compare_costs(monthly_tokens_millions=100)
comparison.compare_costs(monthly_tokens_millions=1000)

# è¾“å‡ºç¤ºä¾‹ï¼š
# æœˆåº¦æˆæœ¬å¯¹æ¯” (å¤„ç† 100M tokens):
# ============================================================
# OpenAI GPT-4             : $     5,000
# OpenAI GPT-3.5           : $       117
# Anthropic Claude-3       : $     5,500
# Google Gemini Pro        : $        29
# Self-Hosting (LLaMA-2-7B): $       800
#
# ç›ˆäºå¹³è¡¡åˆ†æ:
# ------------------------------------------------------------
# OpenAI GPT-4             :     16M tokens/æœˆ
# OpenAI GPT-3.5           :    686M tokens/æœˆ
# Anthropic Claude-3       :     15M tokens/æœˆ
# Google Gemini Pro        :   2743M tokens/æœˆ
```

**ç»“è®º**ï¼š

| æœˆå¤„ç†é‡ | æ¨èæ–¹æ¡ˆ |
|---------|---------|
| < 50M tokens | ä½¿ç”¨APIï¼ˆçµæ´»ï¼Œæ— éœ€è¿ç»´ï¼‰ |
| 50M - 500M tokens | è‡ªæ‰˜ç®¡ä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆLLaMA-2-7B/13Bï¼‰ |
| > 500M tokens | è‡ªæ‰˜ç®¡ + æè‡´ä¼˜åŒ–ï¼ˆå¿…è¦æ—¶ï¼‰ |

---

<a name="mobile-deployment"></a>
## ğŸ“± 7. ç§»åŠ¨ç«¯éƒ¨ç½²ï¼šllama.cpp

### 7.1 llama.cpp + GGUFç®€ä»‹

**llama.cpp** æ˜¯ç”¨C++å®ç°çš„LLMæ¨ç†å¼•æ“ï¼Œä¸“ä¸ºCPUå’Œç§»åŠ¨è®¾å¤‡ä¼˜åŒ–ã€‚  
**GGUF (GPT-Generated Unified Format)** æ˜¯llama.cppçš„æ¨¡å‹æ ¼å¼ï¼Œæ”¯æŒå¤šç§é‡åŒ–ç²¾åº¦ã€‚

æ ¹æ® [2025å¹´ç ”ç©¶](https://arxiv.org/abs/2512.06490)ï¼š

```python
"""
llama.cpp GGUFé‡åŒ–æ ¼å¼å¯¹æ¯”
"""

gguf_formats = {
    'Q4_0': {
        'bits': 4,
        'compression': '3.5x',
        'description': '4-bité‡åŒ–ï¼Œæœ€å¿«',
        'use_case': 'å®æ—¶äº¤äº’ï¼ˆèŠå¤©æœºå™¨äººï¼‰'
    },
    'Q4_K_M': {
        'bits': 4,
        'compression': '3.4x',
        'description': '4-bité‡åŒ– + æ›´å¥½çš„ç²¾åº¦',
        'use_case': 'ğŸ”¥ æ¨èçš„å¹³è¡¡é€‰æ‹©'
    },
    'Q5_K_M': {
        'bits': 5,
        'compression': '3.0x',
        'description': '5-bité‡åŒ–',
        'use_case': 'éœ€è¦æ›´é«˜ç²¾åº¦'
    },
    'Q8_0': {
        'bits': 8,
        'compression': '2.0x',
        'description': '8-bité‡åŒ–',
        'use_case': 'æœ€å°ç²¾åº¦æŸå¤±'
    },
    'F16': {
        'bits': 16,
        'compression': '1.0x',
        'description': 'åŠç²¾åº¦æµ®ç‚¹',
        'use_case': 'åŸºçº¿ï¼ˆæ— é‡åŒ–ï¼‰'
    }
}

# æ‰“å°å¯¹æ¯”
print("GGUFé‡åŒ–æ ¼å¼å¯¹æ¯”:")
print("="*70)
print(f"{'æ ¼å¼':<10} {'ä½æ•°':<6} {'å‹ç¼©æ¯”':<10} {'ä½¿ç”¨åœºæ™¯':<30}")
print("-"*70)
for format_name, info in gguf_formats.items():
    print(f"{format_name:<10} {info['bits']:<6} {info['compression']:<10} {info['use_case']:<30}")
```

---

### 7.2 llama.cppç§»åŠ¨ç«¯éƒ¨ç½²å®æˆ˜

```bash
# ==========================================
# æ­¥éª¤1: è½¬æ¢HFæ¨¡å‹åˆ°GGUFæ ¼å¼
# ==========================================

# 1.1 å…‹éš†llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# 1.2 æ„å»º
make

# 1.3 ä¸‹è½½HFæ¨¡å‹
huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir ./models/llama-2-7b

# 1.4 è½¬æ¢ä¸ºFP16 GGUFï¼ˆä¸­é—´æ ¼å¼ï¼‰
python convert_hf_to_gguf.py ./models/llama-2-7b \
    --outtype f16 \
    --outfile ./models/llama-2-7b-f16.gguf

# 1.5 é‡åŒ–ä¸ºQ4_K_Mï¼ˆæ¨èï¼‰
./llama-quantize ./models/llama-2-7b-f16.gguf \
    ./models/llama-2-7b-q4_k_m.gguf \
    Q4_K_M

# ç»“æœï¼š
# åŸå§‹æ¨¡å‹: 13GB (FP16)
# é‡åŒ–æ¨¡å‹: 3.9GB (Q4_K_M)
# å‹ç¼©æ¯”: 3.3x

# ==========================================
# æ­¥éª¤2: æœ¬åœ°æ¨ç†æµ‹è¯•
# ==========================================
./llama-cli \
    -m ./models/llama-2-7b-q4_k_m.gguf \
    -p "Explain quantum computing in simple terms:" \
    -n 128 \
    -t 4  # ä½¿ç”¨4ä¸ªCPUçº¿ç¨‹

# ==========================================
# æ­¥éª¤3: Androidéƒ¨ç½²ï¼ˆä½¿ç”¨Termuxï¼‰
# ==========================================

# 3.1 åœ¨Androidè®¾å¤‡ä¸Šå®‰è£…Termux (https://termux.dev)

# 3.2 åœ¨Termuxä¸­å®‰è£…ä¾èµ–
pkg install clang wget git

# 3.3 ç¼–è¯‘llama.cppï¼ˆAndroidï¼‰
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# 3.4 ä¸‹è½½é‡åŒ–æ¨¡å‹ï¼ˆä¼ è¾“åˆ°è®¾å¤‡ï¼‰
# ä½¿ç”¨adbæˆ–æ–‡ä»¶ç®¡ç†å™¨å°† llama-2-7b-q4_k_m.gguf å¤åˆ¶åˆ°è®¾å¤‡

# 3.5 è¿è¡Œæ¨ç†
./llama-cli -m ./llama-2-7b-q4_k_m.gguf \
    -p "ä½ å¥½ï¼Œè¯·è‡ªæˆ‘ä»‹ç»ã€‚" \
    -n 100

# æ€§èƒ½ï¼š
# è®¾å¤‡: Snapdragon 8 Gen 2
# æ¨ç†é€Ÿåº¦: ~5-8 tokens/sec
# å†…å­˜å ç”¨: ~4.5GB
```

---

### 7.3 llama.cpp Pythonç»‘å®š

```python
"""
llama.cpp Python APIä½¿ç”¨
"""

# å®‰è£…: pip install llama-cpp-python

from llama_cpp import Llama

# 1. åŠ è½½GGUFæ¨¡å‹
llm = Llama(
    model_path="./models/llama-2-7b-q4_k_m.gguf",
    n_ctx=2048,          # ä¸Šä¸‹æ–‡çª—å£
    n_threads=8,         # CPUçº¿ç¨‹æ•°
    n_gpu_layers=0,      # CPUæ¨ç†ï¼ˆè®¾ä¸º-1åˆ™å…¨éƒ¨GPUï¼‰
    verbose=False
)

# 2. ç”Ÿæˆæ–‡æœ¬
output = llm(
    "Q: What is the capital of France? A:",
    max_tokens=32,
    stop=["Q:", "\n"],
    echo=True
)

print(output['choices'][0]['text'])

# 3. Chatæ¨¡å¼ï¼ˆæ›´æ–¹ä¾¿ï¼‰
from llama_cpp import Llama

llm = Llama.from_pretrained(
    repo_id="TheBloke/Llama-2-7B-Chat-GGUF",
    filename="llama-2-7b-chat.Q4_K_M.gguf",
)

response = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is quantum computing?"}
    ]
)

print(response['choices'][0]['message']['content'])

# 4. æµå¼ç”Ÿæˆ
stream = llm(
    "Tell me a story:",
    max_tokens=200,
    stream=True
)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

---

<a name="optimization-strategy"></a>
## ğŸ¯ 8. å®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡

### 8.1 ä¼˜åŒ–å†³ç­–æ ‘

```python
"""
LLMæ¨ç†ä¼˜åŒ–å†³ç­–æ ‘
"""

class OptimizationDecisionTree:
    """
    æ ¹æ®åœºæ™¯é€‰æ‹©æœ€ä½³ä¼˜åŒ–ç­–ç•¥
    """
    
    def recommend_strategy(self, scenario):
        """
        æ¨èä¼˜åŒ–ç­–ç•¥
        
        å‚æ•°:
            scenario: dictåŒ…å«ä»¥ä¸‹å­—æ®µ
                - model_size: '7B', '13B', '70B'
                - deployment: 'cloud', 'edge', 'mobile'
                - latency_requirement: 'low', 'medium', 'high'
                - throughput_requirement: 'low', 'medium', 'high'
                - budget: 'low', 'medium', 'high'
        """
        recommendations = []
        
        # 1. éƒ¨ç½²ç¯å¢ƒå†³ç­–
        if scenario['deployment'] == 'mobile':
            recommendations.append({
                'priority': 1,
                'strategy': 'llama.cpp + GGUF Q4_K_M',
                'reason': 'ç§»åŠ¨ç«¯å¿…é¡»ç”¨æè‡´é‡åŒ–'
            })
            return recommendations
        
        elif scenario['deployment'] == 'edge':
            recommendations.append({
                'priority': 1,
                'strategy': 'INT4 AWQ + llama.cpp',
                'reason': 'è¾¹ç¼˜è®¾å¤‡ç®—åŠ›æœ‰é™'
            })
            recommendations.append({
                'priority': 2,
                'strategy': 'æ¨¡å‹è’¸é¦åˆ°æ›´å°ç‰ˆæœ¬',
                'reason': 'è¿›ä¸€æ­¥å‡å°‘èµ„æºæ¶ˆè€—'
            })
            return recommendations
        
        # 2. äº‘ç«¯éƒ¨ç½²ï¼šæ ¹æ®ååå’Œå»¶è¿Ÿéœ€æ±‚
        if scenario['throughput_requirement'] == 'high':
            # é«˜åååœºæ™¯
            recommendations.append({
                'priority': 1,
                'strategy': 'vLLM + PagedAttention',
                'reason': 'vLLMåœ¨é«˜å¹¶å‘ä¸‹è¡¨ç°æœ€ä½³'
            })
            
            if scenario['model_size'] in ['7B', '13B']:
                recommendations.append({
                    'priority': 2,
                    'strategy': 'FP8é‡åŒ–',
                    'reason': '2xå‹ç¼©ï¼Œç²¾åº¦æŸå¤±å°'
                })
            else:  # 70B+
                recommendations.append({
                    'priority': 2,
                    'strategy': 'INT4 AWQ',
                    'reason': 'å¤§æ¨¡å‹éœ€è¦æ›´æ¿€è¿›çš„å‹ç¼©'
                })
        
        elif scenario['latency_requirement'] == 'low':
            # ä½å»¶è¿Ÿåœºæ™¯
            recommendations.append({
                'priority': 1,
                'strategy': 'TGI + æ¨æµ‹è§£ç ',
                'reason': 'TGIçš„P99å»¶è¿Ÿæœ€ä½'
            })
            
            if scenario['budget'] == 'high':
                recommendations.append({
                    'priority': 2,
                    'strategy': 'TensorRT-LLM + FP8',
                    'reason': 'æè‡´æ€§èƒ½ï¼Œé€‚åˆé¢„ç®—å……è¶³'
                })
        
        else:
            # å¹³è¡¡åœºæ™¯
            recommendations.append({
                'priority': 1,
                'strategy': 'vLLM + INT8/FP8é‡åŒ–',
                'reason': 'æ€§èƒ½ã€æˆæœ¬ã€æ˜“ç”¨æ€§å¹³è¡¡'
            })
        
        # 3. é€šç”¨ä¼˜åŒ–ï¼ˆé€‚ç”¨æ‰€æœ‰åœºæ™¯ï¼‰
        recommendations.append({
            'priority': 3,
            'strategy': 'KV Cacheé‡åŒ–',
            'reason': 'èŠ‚çœ50%æ˜¾å­˜ï¼Œå‡ ä¹æ— ç²¾åº¦æŸå¤±'
        })
        
        recommendations.append({
            'priority': 4,
            'strategy': 'åŠ¨æ€Batching',
            'reason': 'æé«˜GPUåˆ©ç”¨ç‡'
        })
        
        return sorted(recommendations, key=lambda x: x['priority'])

# ä½¿ç”¨ç¤ºä¾‹
decision_tree = OptimizationDecisionTree()

# åœºæ™¯1ï¼šé«˜ååäº‘æœåŠ¡
scenario_1 = {
    'model_size': '13B',
    'deployment': 'cloud',
    'latency_requirement': 'medium',
    'throughput_requirement': 'high',
    'budget': 'medium'
}

print("åœºæ™¯1ï¼šé«˜ååäº‘æœåŠ¡")
print("="*60)
for rec in decision_tree.recommend_strategy(scenario_1):
    print(f"ä¼˜å…ˆçº§{rec['priority']}: {rec['strategy']}")
    print(f"  åŸå› : {rec['reason']}\n")

# åœºæ™¯2ï¼šç§»åŠ¨ç«¯éƒ¨ç½²
scenario_2 = {
    'model_size': '7B',
    'deployment': 'mobile',
    'latency_requirement': 'medium',
    'throughput_requirement': 'low',
    'budget': 'low'
}

print("\nåœºæ™¯2ï¼šç§»åŠ¨ç«¯éƒ¨ç½²")
print("="*60)
for rec in decision_tree.recommend_strategy(scenario_2):
    print(f"ä¼˜å…ˆçº§{rec['priority']}: {rec['strategy']}")
    print(f"  åŸå› : {rec['reason']}\n")
```

---

<a name="benchmarks"></a>
## ğŸ“Š 9. å®æˆ˜æ¡ˆä¾‹ä¸Benchmark

### 9.1 æ¡ˆä¾‹ï¼šä¼˜åŒ–åœ¨çº¿å®¢æœç³»ç»Ÿ

**éœ€æ±‚**ï¼š
- æ¨¡å‹ï¼šLLaMA-2-13B
- QPSï¼š100ï¼ˆå³°å€¼200ï¼‰
- å»¶è¿Ÿè¦æ±‚ï¼šP99 < 500ms
- é¢„ç®—ï¼šæœ‰é™

**ä¼˜åŒ–è¿‡ç¨‹**ï¼š

```python
"""
åœ¨çº¿å®¢æœç³»ç»Ÿä¼˜åŒ–å®æˆ˜
"""

class CustomerServiceOptimization:
    """
    å®¢æœç³»ç»Ÿæ¨ç†ä¼˜åŒ–æ¡ˆä¾‹
    """
    
    def baseline_deployment(self):
        """
        åŸºçº¿éƒ¨ç½²ï¼ˆæœªä¼˜åŒ–ï¼‰
        """
        return {
            'framework': 'Transformers (HF)',
            'precision': 'FP16',
            'batch_size': 1,
            'gpu': '4x A100-40GB',
            'throughput_qps': 40,
            'latency_p99_ms': 800,
            'cost_per_hour': 10.0,
            'issues': [
                'GPUåˆ©ç”¨ç‡ä½ï¼ˆ30%ï¼‰',
                'å»¶è¿Ÿä¸æ»¡è¶³è¦æ±‚',
                'æˆæœ¬è¿‡é«˜'
            ]
        }
    
    def optimization_stage_1(self):
        """
        ä¼˜åŒ–é˜¶æ®µ1ï¼šåˆ‡æ¢åˆ°vLLM
        """
        return {
            'framework': 'vLLM',
            'precision': 'FP16',
            'batch_size': 'dynamic (up to 128)',
            'gpu': '2x A100-40GB',  # âœ… GPUå‡åŠï¼
            'throughput_qps': 120,   # âœ… 3xæå‡
            'latency_p99_ms': 450,   # âœ… æ»¡è¶³è¦æ±‚
            'cost_per_hour': 5.0,    # âœ… æˆæœ¬å‡åŠ
            'improvements': [
                'PagedAttentionå‡å°‘æ˜¾å­˜ç¢ç‰‡',
                'åŠ¨æ€batchingæé«˜åˆ©ç”¨ç‡',
                'Continuous batchingå‡å°‘ç©ºé—²'
            ]
        }
    
    def optimization_stage_2(self):
        """
        ä¼˜åŒ–é˜¶æ®µ2ï¼šæ·»åŠ FP8é‡åŒ–
        """
        return {
            'framework': 'vLLM',
            'precision': 'FP8',      # âœ… é‡åŒ–
            'batch_size': 'dynamic (up to 256)',
            'gpu': '1x A100-40GB',   # âœ… å•å¡ï¼
            'throughput_qps': 150,
            'latency_p99_ms': 400,
            'cost_per_hour': 2.5,    # âœ… 75%æˆæœ¬èŠ‚çœ
            'improvements': [
                'FP8å‡å°‘è®¿å­˜å¸¦å®½å‹åŠ›',
                'æ›´å¤§batch sizeæé«˜åå',
                'å•å¡æ»¡è¶³éœ€æ±‚'
            ]
        }
    
    def optimization_stage_3(self):
        """
        ä¼˜åŒ–é˜¶æ®µ3ï¼šKV Cacheå‹ç¼©
        """
        return {
            'framework': 'vLLM',
            'precision': 'FP8',
            'kv_cache': 'INT8 quantized',  # âœ… KV Cacheé‡åŒ–
            'batch_size': 'dynamic (up to 512)',
            'gpu': '1x A100-40GB',
            'throughput_qps': 200,   # âœ… è¶…å‡ºå³°å€¼éœ€æ±‚
            'latency_p99_ms': 380,
            'cost_per_hour': 2.5,
            'improvements': [
                'KV Cacheé‡åŒ–é‡Šæ”¾æ›´å¤šæ˜¾å­˜',
                'æ”¯æŒæ›´å¤§batch size',
                'åº”å¯¹å³°å€¼æµé‡'
            ]
        }
    
    def print_optimization_journey(self):
        """æ‰“å°ä¼˜åŒ–è¿‡ç¨‹"""
        stages = [
            ('Baseline', self.baseline_deployment()),
            ('Stage 1: vLLM', self.optimization_stage_1()),
            ('Stage 2: + FP8', self.optimization_stage_2()),
            ('Stage 3: + KV Cacheé‡åŒ–', self.optimization_stage_3()),
        ]
        
        print("="*80)
        print("åœ¨çº¿å®¢æœç³»ç»Ÿæ¨ç†ä¼˜åŒ–å†ç¨‹")
        print("="*80)
        
        for stage_name, config in stages:
            print(f"\n{stage_name}")
            print("-"*80)
            print(f"  GPUé…ç½®: {config['gpu']}")
            print(f"  ååé‡: {config['throughput_qps']} QPS")
            print(f"  P99å»¶è¿Ÿ: {config['latency_p99_ms']} ms")
            print(f"  æˆæœ¬: ${config['cost_per_hour']:.2f}/å°æ—¶")
            
            if 'improvements' in config:
                print(f"  ä¼˜åŒ–ç‚¹:")
                for improvement in config['improvements']:
                    print(f"    âœ… {improvement}")

# è¿è¡Œæ¡ˆä¾‹
case_study = CustomerServiceOptimization()
case_study.print_optimization_journey()

# è¾“å‡ºç¤ºä¾‹ï¼š
# ================================================================================
# åœ¨çº¿å®¢æœç³»ç»Ÿæ¨ç†ä¼˜åŒ–å†ç¨‹
# ================================================================================
#
# Baseline
# --------------------------------------------------------------------------------
#   GPUé…ç½®: 4x A100-40GB
#   ååé‡: 40 QPS
#   P99å»¶è¿Ÿ: 800 ms
#   æˆæœ¬: $10.00/å°æ—¶
#
# Stage 1: vLLM
# --------------------------------------------------------------------------------
#   GPUé…ç½®: 2x A100-40GB
#   ååé‡: 120 QPS
#   P99å»¶è¿Ÿ: 450 ms
#   æˆæœ¬: $5.00/å°æ—¶
#   ä¼˜åŒ–ç‚¹:
#     âœ… PagedAttentionå‡å°‘æ˜¾å­˜ç¢ç‰‡
#     âœ… åŠ¨æ€batchingæé«˜åˆ©ç”¨ç‡
#     âœ… Continuous batchingå‡å°‘ç©ºé—²
#
# ... (åç»­é˜¶æ®µ)
```

---

### 9.2 å®Œæ•´Benchmarkæ€»ç»“

```python
"""
å®Œæ•´Benchmarkæ±‡æ€»ï¼ˆ2025å¹´æ•°æ®ï¼‰
"""

benchmark_summary = {
    'LLaMA-2-7B': {
        'Baseline (FP16, HF Transformers)': {
            'throughput_tps': 90,
            'latency_p50_ms': 120,
            'memory_gb': 14
        },
        'vLLM (FP16)': {
            'throughput_tps': 2150,
            'latency_p50_ms': 45,
            'memory_gb': 18
        },
        'vLLM + FP8': {
            'throughput_tps': 3870,  # 1.8x
            'latency_p50_ms': 38,
            'memory_gb': 9
        },
        'TensorRT-LLM + FP8': {
            'throughput_tps': 4300,  # ğŸ”¥ æœ€å¿«
            'latency_p50_ms': 32,
            'memory_gb': 8
        },
        'llama.cpp (Q4_K_M, CPU)': {
            'throughput_tps': 15,
            'latency_p50_ms': 800,
            'memory_gb': 4
        }
    }
}

# å¯è§†åŒ–
import pandas as pd
import matplotlib.pyplot as plt

df = pd.DataFrame(benchmark_summary['LLaMA-2-7B']).T
df = df.reset_index().rename(columns={'index': 'Method'})

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# ååé‡å¯¹æ¯”
axes[0].barh(df['Method'], df['throughput_tps'])
axes[0].set_xlabel('Throughput (tokens/sec)')
axes[0].set_title('Throughput Comparison')
axes[0].invert_yaxis()

# å»¶è¿Ÿå¯¹æ¯”
axes[1].barh(df['Method'], df['latency_p50_ms'])
axes[1].set_xlabel('P50 Latency (ms)')
axes[1].set_title('Latency Comparison')
axes[1].invert_yaxis()

# æ˜¾å­˜å¯¹æ¯”
axes[2].barh(df['Method'], df['memory_gb'])
axes[2].set_xlabel('Memory (GB)')
axes[2].set_title('Memory Usage')
axes[2].invert_yaxis()

plt.tight_layout()
plt.savefig('inference_benchmark.png', dpi=150)
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. **é‡åŒ–æŠ€æœ¯**  
   [NVIDIA TensorRT-LLM Quantization Guide](https://nvidia.github.io/TensorRT-LLM/blogs/quantization-in-TRT-LLM.html)  
   2025å¹´å®˜æ–¹é‡åŒ–æŒ‡å—

2. **vLLMä¸PagedAttention**  
   [Comparative Analysis of vLLM and TGI](https://arxiv.org/html/2511.17593v1)  
   2025å¹´11æœˆæœ€æ–°å¯¹æ¯”ç ”ç©¶

3. **KV Cacheä¼˜åŒ–**  
   [KV-Compress](https://arxiv.org/html/2410.00161v2)  
   8-64xå‹ç¼©ï¼Œæ€§èƒ½æŸå¤±<5%

4. **æ¨æµ‹è§£ç **  
   - [Medusa](https://huggingface.co/papers/2401.10774) - 2.2-3.6xåŠ é€Ÿ
   - [Hydra](https://huggingface.co/papers/2402.05109) - è¿›ä¸€æ­¥ä¼˜åŒ–

5. **ç§»åŠ¨ç«¯éƒ¨ç½²**  
   [Practical Quantization with llama.cpp](https://arxiv.org/abs/2512.06490)  
   2025å¹´ç§»åŠ¨ç«¯LLMç ”ç©¶

### å·¥ç¨‹èµ„æº

- [vLLM Documentation](https://docs.vllm.ai)  
  é«˜æ€§èƒ½æ¨ç†å¼•æ“

- [TensorRT-LLM GitHub](https://github.com/NVIDIA/TensorRT-LLM)  
  NVIDIAå®˜æ–¹æ¨ç†å¼•æ“

- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)  
  CPU/ç§»åŠ¨ç«¯æ¨ç†

### æˆæœ¬åˆ†æ

- [a16z: LLMflation](https://a16z.com/llmflation-llm-inference-cost/)  
  æ¨ç†æˆæœ¬å¹´é™10å€åˆ†æ

- [Fin.ai: Cost of Serving LLMs](https://fin.ai/research/cost-of-serving-llms/)  
  è‡ªæ‰˜ç®¡ vs APIæˆæœ¬å¯¹æ¯”

---

## ğŸ¯ æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **æ¨ç†æˆæœ¬æ˜¯å¤§å¤´**ï¼šå æ€»æˆæœ¬95%+ï¼Œä¼˜åŒ–æ¨ç†æ¯”ä¼˜åŒ–è®­ç»ƒæ›´é‡è¦
2. **é‡åŒ–æ˜¯ç‹é“**ï¼šFP8/INT4å¯å°†æˆæœ¬é™ä½2-4å€ï¼Œç²¾åº¦æŸå¤±<3%
3. **å¼•æ“é€‰æ‹©**ï¼švLLMï¼ˆæ˜“ç”¨+é«˜æ€§èƒ½ï¼‰ã€TensorRT-LLMï¼ˆæè‡´æ€§èƒ½ï¼‰ã€llama.cppï¼ˆç§»åŠ¨ç«¯ï¼‰
4. **KV Cacheä¼˜åŒ–**ï¼šPagedAttentionã€é‡åŒ–ã€å‹ç¼©å¯èŠ‚çœ50-90%æ˜¾å­˜
5. **æˆæœ¬ä¸‹é™è¿…é€Ÿ**ï¼š2025å¹´æ¨ç†æˆæœ¬æ¯å¹´é™10å€

### ä¼˜åŒ–Checklist

- [ ] é€‰æ‹©åˆé€‚çš„é‡åŒ–æ–¹æ³•ï¼ˆFP8 for æ–°GPUï¼ŒINT4 for æ—§GPUï¼‰
- [ ] ä½¿ç”¨é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼ˆvLLM/TensorRT-LLMï¼‰
- [ ] å¯ç”¨KV Cacheä¼˜åŒ–ï¼ˆPagedAttention + é‡åŒ–ï¼‰
- [ ] é…ç½®åŠ¨æ€Batchingæé«˜åå
- [ ] è¯„ä¼°è‡ªæ‰˜ç®¡ vs APIï¼ˆ>500M tokens/æœˆå»ºè®®è‡ªæ‰˜ç®¡ï¼‰
- [ ] ç›‘æ§GPUåˆ©ç”¨ç‡ï¼ˆç›®æ ‡>80%ï¼‰
- [ ] è€ƒè™‘æ¨æµ‹è§£ç ï¼ˆHydraå¯æé€Ÿ2.7xï¼‰

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ”— **ä¸‹ä¸€ç¯‡**ï¼š[10 - åˆ†å¸ƒå¼è®­ç»ƒå®è·µï¼šå¤šå¡å¹¶è¡Œç­–ç•¥ä¸å·¥ç¨‹ç»éªŒ](./10-distributed-training.md)
- ğŸ’» **åŠ¨æ‰‹å®è·µ**ï¼šç”¨vLLMéƒ¨ç½²ä¸€ä¸ªé‡åŒ–æ¨¡å‹ï¼Œå¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½
- ğŸ“Š **æˆæœ¬è®¡ç®—**ï¼šç”¨æœ¬æ–‡çš„è®¡ç®—å™¨è¯„ä¼°ä½ çš„é¡¹ç›®æ¨ç†æˆæœ¬

---

> ğŸ’¡ **ç’‡ç‘çš„å°è´´å£«**ï¼šæ¨ç†ä¼˜åŒ–å°±åƒæ€§èƒ½è°ƒä¼˜â€”â€”æ‰¾åˆ°ç“¶é¢ˆï¼ˆæ˜¾å­˜å¸¦å®½/è®¡ç®—ï¼‰ï¼Œé€‰å¯¹æ–¹æ³•ï¼ˆé‡åŒ–/å¼•æ“ï¼‰ï¼Œé€æ­¥ä¼˜åŒ–ã€‚ä¼ ç»Ÿç¨‹åºå‘˜çš„æ€§èƒ½ä¼˜åŒ–ç»éªŒåœ¨è¿™é‡Œå®Œå…¨é€‚ç”¨ï¼å…ˆç”¨Profileræ‰¾ç“¶é¢ˆï¼Œå†å¯¹ç—‡ä¸‹è¯~ âœ¨
>
> é“å‹ç°åœ¨å¯¹æ¨ç†ä¼˜åŒ–æœ‰æ„Ÿè§‰äº†å—ï¼Ÿä¸‹ä¸€ç¯‡æˆ‘ä»¬èŠåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ•™ä½ æ€ä¹ˆæŠŠè®­ç»ƒæ‰©å±•åˆ°ä¸Šç™¾å—GPUï¼ğŸš€
