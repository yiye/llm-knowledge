# 10 - åˆ†å¸ƒå¼è®­ç»ƒå®è·µï¼šå¤šå¡å¹¶è¡Œç­–ç•¥ä¸å·¥ç¨‹ç»éªŒ

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šå•å¡è®­ç»ƒ7Bæ¨¡å‹éœ€è¦3å¤©ï¼Œ8å¡å¹¶è¡Œåªéœ€5å°æ—¶â€”â€”ä½†å‰ææ˜¯ä½ æ‡‚å¾—å¦‚ä½•æ­£ç¡®ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚æœ¬æ–‡æ·±å…¥è®²è§£DPã€DDPã€FSDPã€DeepSpeed ZeROã€Megatron-LMç­‰å¹¶è¡Œç­–ç•¥ï¼Œå‰–æNCCLé€šä¿¡åŸç†ï¼Œæä¾›å®Œæ•´çš„å¤šèŠ‚ç‚¹è®­ç»ƒä»£ç ï¼Œå¹¶åˆ†äº«æ•…éšœæ¢å¤ã€æ€§èƒ½è°ƒä¼˜çš„å®æˆ˜ç»éªŒã€‚

---

## ğŸ“‹ ç›®å½•

1. [ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ](#why-distributed)
2. [å¹¶è¡Œç­–ç•¥å…¨æ™¯å›¾](#parallelism-overview)
3. [æ•°æ®å¹¶è¡Œï¼šDP vs DDP](#data-parallel)
4. [FSDPï¼šZeROçš„PyTorchå®ç°](#fsdp)
5. [DeepSpeed ZeROæ·±åº¦è§£æ](#deepspeed-zero)
6. [Megatron-LMï¼šå¼ é‡ä¸æµæ°´çº¿å¹¶è¡Œ](#megatron)
7. [3Då¹¶è¡Œï¼šDP+TP+PPç»„åˆæ‹³](#3d-parallelism)
8. [é€šä¿¡ä¼˜åŒ–ï¼šNCCLä¸Ring-AllReduce](#communication)
9. [æ¢¯åº¦ç´¯ç§¯ä¸æ··åˆç²¾åº¦](#gradient-accumulation)
10. [å¤šèŠ‚ç‚¹è®­ç»ƒå®æˆ˜](#multi-node-training)
11. [Checkpointä¸æ•…éšœæ¢å¤](#fault-tolerance)
12. [æ€§èƒ½è°ƒä¼˜ä¸Profiling](#performance-tuning)
13. [å¸¸è§é—®é¢˜ä¸è°ƒè¯•](#debugging)

---

<a name="why-distributed"></a>
## ğŸ¤” 1. ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ

### 1.1 å•å¡çš„æé™

```python
"""
å•å¡è®­ç»ƒçš„ç“¶é¢ˆåˆ†æ
"""

class SingleGPUBottleneck:
    """
    å•å¡è®­ç»ƒç“¶é¢ˆè®¡ç®—
    """
    
    def calculate_memory_requirement(
        self,
        model_params_billion=7,
        precision='fp16',
        batch_size=8,
        seq_len=2048
    ):
        """
        è®¡ç®—è®­ç»ƒæ‰€éœ€æ˜¾å­˜
        
        å…¬å¼ï¼š
        æ€»æ˜¾å­˜ = æ¨¡å‹å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼
        """
        # 1. æ¨¡å‹å‚æ•°ï¼ˆFP16: 2 bytes per paramï¼‰
        bytes_per_param = 2 if precision == 'fp16' else 4
        model_memory_gb = (model_params_billion * 1e9 * bytes_per_param) / (1024 ** 3)
        
        # 2. æ¢¯åº¦ï¼ˆä¸æ¨¡å‹å‚æ•°ç›¸åŒå¤§å°ï¼‰
        gradient_memory_gb = model_memory_gb
        
        # 3. ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdam: 2å€æ¨¡å‹å‚æ•°ï¼ŒFP32å­˜å‚¨ï¼‰
        # moment1 + moment2 + FP32 master weights
        optimizer_memory_gb = model_params_billion * 1e9 * 4 * 3 / (1024 ** 3)
        
        # 4. æ¿€æ´»å€¼ï¼ˆå–å†³äºbatch sizeå’Œåºåˆ—é•¿åº¦ï¼‰
        # ç®€åŒ–ä¼°ç®—ï¼šæ¯ä¸ªtokençš„æ¿€æ´»çº¦å  batch_size * seq_len * hidden_dim * num_layers * 2
        hidden_dim = 4096  # 7Bæ¨¡å‹å…¸å‹å€¼
        num_layers = 32
        activation_memory_gb = (
            batch_size * seq_len * hidden_dim * num_layers * bytes_per_param * 10  # 10å€æ˜¯ç»éªŒç³»æ•°
        ) / (1024 ** 3)
        
        # æ€»æ˜¾å­˜
        total_memory_gb = (
            model_memory_gb + 
            gradient_memory_gb + 
            optimizer_memory_gb + 
            activation_memory_gb
        )
        
        print(f"""
        LLaMA-7B è®­ç»ƒæ˜¾å­˜éœ€æ±‚åˆ†æ (FP16, batch_size={batch_size}):
        
        æ¨¡å‹å‚æ•°:      {model_memory_gb:.1f} GB
        æ¢¯åº¦:          {gradient_memory_gb:.1f} GB
        ä¼˜åŒ–å™¨çŠ¶æ€:    {optimizer_memory_gb:.1f} GB
        æ¿€æ´»å€¼:        {activation_memory_gb:.1f} GB
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        æ€»è®¡:          {total_memory_gb:.1f} GB
        
        âš ï¸ A100-40GB æ— æ³•è®­ç»ƒï¼éœ€è¦åˆ†å¸ƒå¼æ–¹æ¡ˆã€‚
        """)
        
        return total_memory_gb

# è¿è¡Œåˆ†æ
analyzer = SingleGPUBottleneck()
analyzer.calculate_memory_requirement(model_params_billion=7, batch_size=8)

# è¾“å‡ºï¼š
# æ€»è®¡: 98.6 GB
# âš ï¸ A100-40GB æ— æ³•è®­ç»ƒï¼
```

> ğŸ”¥ **å…³é”®æ´å¯Ÿ**ï¼šè®­ç»ƒ7Bæ¨¡å‹éœ€è¦~100GBæ˜¾å­˜ï¼Œè¿œè¶…å•å¡å®¹é‡ã€‚å³ä½¿æ˜¯70Bæ¨¡å‹ï¼Œæ¨ç†éœ€è¦140GBï¼Œè®­ç»ƒéœ€è¦700GB+ï¼

---

### 1.2 åˆ†å¸ƒå¼è®­ç»ƒçš„æ”¶ç›Š

```python
"""
åˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿæ¯”è®¡ç®—
"""

import matplotlib.pyplot as plt
import numpy as np

def plot_scaling_efficiency():
    """
    å¯è§†åŒ–ä¸åŒå¹¶è¡Œç­–ç•¥çš„æ‰©å±•æ•ˆç‡
    """
    num_gpus = np.array([1, 2, 4, 8, 16, 32, 64, 128])
    
    # ç†æƒ³çº¿æ€§åŠ é€Ÿï¼ˆ100%æ•ˆç‡ï¼‰
    ideal = num_gpus
    
    # å®é™…åŠ é€Ÿæ¯”ï¼ˆä¸åŒç­–ç•¥ï¼‰
    dp_speedup = num_gpus * 0.95  # DataParallel: 95%æ•ˆç‡ï¼ˆé€šä¿¡å¼€é”€å°ï¼‰
    ddp_speedup = num_gpus * 0.92  # DDP: 92%æ•ˆç‡
    fsdp_speedup = num_gpus * 0.88  # FSDP: 88%æ•ˆç‡ï¼ˆZeRO-3é€šä¿¡å¤šï¼‰
    pipeline_speedup = num_gpus * 0.70  # Pipeline: 70%æ•ˆç‡ï¼ˆæ°”æ³¡ï¼‰
    
    plt.figure(figsize=(10, 6))
    plt.plot(num_gpus, ideal, 'k--', label='ç†æƒ³çº¿æ€§', linewidth=2)
    plt.plot(num_gpus, dp_speedup, 'b-o', label='DP (å•æœº)', linewidth=2)
    plt.plot(num_gpus, ddp_speedup, 'g-s', label='DDP', linewidth=2)
    plt.plot(num_gpus, fsdp_speedup, 'r-^', label='FSDP/ZeRO-3', linewidth=2)
    plt.plot(num_gpus, pipeline_speedup, 'm-d', label='Pipeline Parallel', linewidth=2)
    
    plt.xlabel('GPUæ•°é‡', fontsize=12)
    plt.ylabel('åŠ é€Ÿæ¯”', fontsize=12)
    plt.title('åˆ†å¸ƒå¼è®­ç»ƒæ‰©å±•æ•ˆç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3)
    plt.xscale('log', base=2)
    plt.yscale('log', base=2)
    plt.tight_layout()
    plt.savefig('distributed_scaling.png', dpi=150)

plot_scaling_efficiency()
```

**ä¼ ç»Ÿç¨‹åºå‘˜çš„ç±»æ¯”**ï¼š

| å•æœºç¨‹åº | åˆ†å¸ƒå¼è®­ç»ƒ |
|---------|-----------|
| å•çº¿ç¨‹å¤„ç† | å•GPUè®­ç»ƒ |
| å¤šçº¿ç¨‹ï¼ˆå…±äº«å†…å­˜ï¼‰ | DataParallel (DP) |
| å¤šè¿›ç¨‹ï¼ˆæ¶ˆæ¯ä¼ é€’ï¼‰ | DistributedDataParallel (DDP) |
| åˆ†å¸ƒå¼ç³»ç»Ÿï¼ˆMapReduceï¼‰ | Pipeline/Tensor Parallel |

---

<a name="parallelism-overview"></a>
## ğŸ—ºï¸ 2. å¹¶è¡Œç­–ç•¥å…¨æ™¯å›¾

### 2.1 å››å¤§å¹¶è¡ŒèŒƒå¼

```python
"""
å¹¶è¡Œç­–ç•¥åˆ†ç±»
"""

parallelism_taxonomy = {
    'æ•°æ®å¹¶è¡Œ (Data Parallelism)': {
        'principle': 'æ¯ä¸ªGPUæŒæœ‰å®Œæ•´æ¨¡å‹ï¼Œè®­ç»ƒä¸åŒæ•°æ®',
        'variants': ['DP', 'DDP', 'FSDP/ZeRO'],
        'communication': 'AllReduceæ¢¯åº¦',
        'memory_per_gpu': 'å®Œæ•´æ¨¡å‹ï¼ˆDP/DDPï¼‰æˆ–åˆ†ç‰‡æ¨¡å‹ï¼ˆFSDPï¼‰',
        'use_case': 'æ¨¡å‹èƒ½æ”¾å…¥å•å¡æ˜¾å­˜',
        'icon': 'ğŸ”€'
    },
    
    'å¼ é‡å¹¶è¡Œ (Tensor Parallelism)': {
        'principle': 'å°†å•ä¸ªå±‚çš„æƒé‡çŸ©é˜µåˆ‡åˆ†åˆ°å¤šGPU',
        'variants': ['Megatron-LM TP'],
        'communication': 'AllReduceæ¿€æ´»å€¼ï¼ˆé¢‘ç¹ï¼‰',
        'memory_per_gpu': '1/N çš„æ¯å±‚å‚æ•°',
        'use_case': 'è¶…å¤§å±‚ï¼ˆhidden_dim > 8192ï¼‰',
        'icon': 'âœ‚ï¸'
    },
    
    'æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)': {
        'principle': 'æŒ‰æ·±åº¦åˆ‡åˆ†æ¨¡å‹ï¼Œæ¯ä¸ªGPUè´Ÿè´£è‹¥å¹²å±‚',
        'variants': ['GPipe', 'PipeDream', 'Megatron-LM PP'],
        'communication': 'ç‚¹å¯¹ç‚¹ä¼ è¾“æ¿€æ´»å€¼',
        'memory_per_gpu': 'L/N å±‚çš„å‚æ•°ï¼ˆL=æ€»å±‚æ•°ï¼‰',
        'use_case': 'è¶…æ·±æ¨¡å‹ï¼ˆlayers > 50ï¼‰',
        'icon': 'ğŸ”„'
    },
    
    'ä¸“å®¶å¹¶è¡Œ (Expert Parallelism)': {
        'principle': 'MoEæ¨¡å‹çš„ä¸“å®¶åˆ†å¸ƒåˆ°ä¸åŒGPU',
        'variants': ['DeepSpeed MoE', 'FairScale MoE'],
        'communication': 'AllToAllè·¯ç”±token',
        'memory_per_gpu': 'E/N ä¸ªä¸“å®¶',
        'use_case': 'MoEæ¶æ„ï¼ˆå¦‚Mixtralï¼‰',
        'icon': 'ğŸ¯'
    }
}

# å¯è§†åŒ–å†³ç­–æ ‘
def choose_parallelism_strategy(model_size_b, num_layers, hidden_dim, num_gpus):
    """
    æ ¹æ®æ¨¡å‹ç‰¹å¾é€‰æ‹©å¹¶è¡Œç­–ç•¥
    """
    print(f"\næ¨¡å‹ç‰¹å¾:")
    print(f"  å‚æ•°é‡: {model_size_b}B")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  éšè—ç»´åº¦: {hidden_dim}")
    print(f"  å¯ç”¨GPU: {num_gpus}")
    print(f"\næ¨èç­–ç•¥:")
    
    # å†³ç­–é€»è¾‘
    strategies = []
    
    # 1. æ¨¡å‹èƒ½å¦æ”¾å…¥å•å¡ï¼Ÿ
    model_memory_gb = model_size_b * 2  # FP16
    if model_memory_gb < 40:  # A100-40GB
        strategies.append("âœ… æ•°æ®å¹¶è¡Œ (DDP) - æ¨¡å‹è¾ƒå°ï¼Œå•å¡å¯å®¹çº³")
    else:
        strategies.append("âŒ å•å¡æ— æ³•å®¹çº³ï¼Œéœ€è¦æ¨¡å‹å¹¶è¡Œ")
    
    # 2. æ˜¯å¦éœ€è¦å¼ é‡å¹¶è¡Œï¼Ÿ
    if hidden_dim > 8192:
        strategies.append("âœ… å¼ é‡å¹¶è¡Œ (TP) - éšè—ç»´åº¦è¶…å¤§")
    
    # 3. æ˜¯å¦éœ€è¦æµæ°´çº¿å¹¶è¡Œï¼Ÿ
    if num_layers > 50:
        strategies.append("âœ… æµæ°´çº¿å¹¶è¡Œ (PP) - æ¨¡å‹è¶…æ·±")
    
    # 4. æœ€ç»ˆæ¨è
    if model_size_b <= 7:
        strategies.append("ğŸ”¥ æœ€ä½³ï¼šDDP + FSDP")
    elif model_size_b <= 70:
        strategies.append("ğŸ”¥ æœ€ä½³ï¼šTP=4 + PP=4 + DP")
    else:
        strategies.append("ğŸ”¥ æœ€ä½³ï¼š3Då¹¶è¡Œ (TP + PP + DP)")
    
    for s in strategies:
        print(f"  {s}")

# ç¤ºä¾‹
choose_parallelism_strategy(model_size_b=7, num_layers=32, hidden_dim=4096, num_gpus=8)
choose_parallelism_strategy(model_size_b=70, num_layers=80, hidden_dim=8192, num_gpus=64)
```

---

<a name="data-parallel"></a>
## ğŸ”€ 3. æ•°æ®å¹¶è¡Œï¼šDP vs DDP

### 3.1 DataParallel (DP)ï¼šå•æœºå¤šå¡

```python
"""
DataParallel å®ç°ï¼ˆå·²è¿‡æ—¶ï¼Œä»…ä¾›ç†è§£ï¼‰
"""

import torch
import torch.nn as nn

# ç®€å•æ¨¡å‹
model = nn.Sequential(
    nn.Linear(1000, 2000),
    nn.ReLU(),
    nn.Linear(2000, 10)
)

# DPå°è£…ï¼ˆè‡ªåŠ¨åˆ†å‘åˆ°æ‰€æœ‰GPUï¼‰
model = nn.DataParallel(model)
model = model.cuda()

# è®­ç»ƒ
for batch in dataloader:
    inputs, labels = batch
    inputs = inputs.cuda()
    labels = labels.cuda()
    
    # Forward passï¼ˆè‡ªåŠ¨å¹¶è¡Œï¼‰
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    # Backward pass
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

**DPçš„é—®é¢˜**ï¼š

| é—®é¢˜ | åŸå›  | å½±å“ |
|------|------|------|
| **GPU 0è´Ÿè½½ä¸å‡** | æ‰€æœ‰æ¢¯åº¦èšåˆåˆ°GPU 0 | GPU 0æ˜¾å­˜/è®¡ç®—æ›´é«˜ |
| **GILé™åˆ¶** | Pythonå¤šçº¿ç¨‹å—GILå½±å“ | æ‰©å±•æ€§å·® |
| **å•è¿›ç¨‹** | æ— æ³•è·¨èŠ‚ç‚¹ | åªèƒ½å•æœº |
| **æ•ˆç‡ä½** | é¢‘ç¹çš„CPU-GPUä¼ è¾“ | ååé‡ä½ |

> âš ï¸ **ç»“è®º**ï¼šDataParallelå·²è¢«åºŸå¼ƒï¼Œ**æ°¸è¿œä½¿ç”¨DDP**ï¼

---

### 3.2 DistributedDataParallel (DDP)ï¼šå¤šè¿›ç¨‹å¹¶è¡Œ

**DDPåŸç†**ï¼š

```python
"""
DDPæ ¸å¿ƒåŸç†æ¼”ç¤º
"""

class DDPSimulation:
    """
    æ¨¡æ‹ŸDDPçš„å·¥ä½œæµç¨‹
    """
    
    def __init__(self, world_size=4):
        self.world_size = world_size  # æ€»GPUæ•°
        self.ranks = list(range(world_size))
    
    def simulate_training_step(self):
        """
        æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ­¥éª¤
        """
        print("="*60)
        print("DDPè®­ç»ƒæ­¥éª¤æ¨¡æ‹Ÿ")
        print("="*60)
        
        # æ­¥éª¤1: æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹forward
        print("\n1ï¸âƒ£ Forward Passï¼ˆå„è¿›ç¨‹ç‹¬ç«‹ï¼‰:")
        for rank in self.ranks:
            print(f"  Rank {rank}: Forward(batch_{rank}) â†’ loss_{rank}")
        
        # æ­¥éª¤2: æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹backward
        print("\n2ï¸âƒ£ Backward Passï¼ˆå„è¿›ç¨‹ç‹¬ç«‹ï¼‰:")
        for rank in self.ranks:
            print(f"  Rank {rank}: Backward() â†’ gradients_{rank}")
        
        # æ­¥éª¤3: AllReduceæ¢¯åº¦ï¼ˆå…³é”®ï¼ï¼‰
        print("\n3ï¸âƒ£ AllReduceæ¢¯åº¦ï¼ˆåŒæ­¥ï¼‰:")
        print(f"  æ‰€æœ‰è¿›ç¨‹çš„æ¢¯åº¦é€šè¿‡AllReduceåŒæ­¥å¹¶å¹³å‡")
        print(f"  gradient_avg = (grad_0 + grad_1 + grad_2 + grad_3) / 4")
        
        # æ­¥éª¤4: æ¯ä¸ªè¿›ç¨‹ç”¨ç›¸åŒçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹
        print("\n4ï¸âƒ£ å‚æ•°æ›´æ–°ï¼ˆå„è¿›ç¨‹ç›¸åŒï¼‰:")
        for rank in self.ranks:
            print(f"  Rank {rank}: params -= lr * gradient_avg")
        
        print("\nâœ… ç»“æœï¼šæ‰€æœ‰è¿›ç¨‹çš„æ¨¡å‹å‚æ•°ä¿æŒä¸€è‡´")

# è¿è¡Œæ¨¡æ‹Ÿ
sim = DDPSimulation(world_size=4)
sim.simulate_training_step()
```

---

### 3.3 DDPå®Œæ•´å®æˆ˜ä»£ç 

```python
"""
DDPå®Œæ•´è®­ç»ƒè„šæœ¬ - å•æœºå¤šå¡
"""

import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler

# ============================================
# 1. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
# ============================================
def setup_ddp(rank, world_size):
    """
    åˆå§‹åŒ–DDP
    
    å‚æ•°:
        rank: å½“å‰è¿›ç¨‹çš„rank (0 åˆ° world_size-1)
        world_size: æ€»è¿›ç¨‹æ•°ï¼ˆé€šå¸¸ç­‰äºGPUæ•°ï¼‰
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼ˆNCCLåç«¯ç”¨äºGPUï¼‰
    dist.init_process_group(
        backend='nccl',      # NCCLæ˜¯NVIDIA GPUçš„æœ€ä½³åç«¯
        init_method='env://',
        world_size=world_size,
        rank=rank
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    torch.cuda.set_device(rank)

def cleanup_ddp():
    """æ¸…ç†DDP"""
    dist.destroy_process_group()

# ============================================
# 2. å®šä¹‰æ¨¡å‹
# ============================================
class SimpleModel(nn.Module):
    """ç¤ºä¾‹æ¨¡å‹"""
    def __init__(self, input_dim=1000, hidden_dim=2000, output_dim=10):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.layers(x)

# ============================================
# 3. è®­ç»ƒå‡½æ•°
# ============================================
def train_ddp(rank, world_size):
    """
    DDPè®­ç»ƒä¸»å‡½æ•°
    
    æ¯ä¸ªGPUä¼šå¯åŠ¨ä¸€ä¸ªè¿›ç¨‹è¿è¡Œè¿™ä¸ªå‡½æ•°
    """
    print(f"ğŸš€ å¯åŠ¨è¿›ç¨‹ Rank {rank}/{world_size}")
    
    # åˆå§‹åŒ–DDP
    setup_ddp(rank, world_size)
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°GPU
    model = SimpleModel().cuda(rank)
    
    # ğŸ”¥ å…³é”®ï¼šç”¨DDPåŒ…è£…æ¨¡å‹
    model = DDP(
        model, 
        device_ids=[rank],
        output_device=rank,
        find_unused_parameters=False  # å¦‚æœæœ‰æœªä½¿ç”¨çš„å‚æ•°ï¼Œè®¾ä¸ºTrue
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆç¤ºä¾‹ï¼‰
    class DummyDataset(Dataset):
        def __len__(self):
            return 1000
        
        def __getitem__(self, idx):
            return torch.randn(1000), torch.randint(0, 10, (1,))
    
    dataset = DummyDataset()
    
    # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨DistributedSamplerç¡®ä¿æ¯ä¸ªè¿›ç¨‹çœ‹åˆ°ä¸åŒæ•°æ®
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        sampler=sampler,
        num_workers=2,
        pin_memory=True
    )
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(10):
        # ğŸ”¥ é‡è¦ï¼šæ¯ä¸ªepochè®¾ç½®samplerçš„epochï¼Œç¡®ä¿shuffleä¸åŒ
        sampler.set_epoch(epoch)
        
        epoch_loss = 0.0
        for batch_idx, (inputs, labels) in enumerate(dataloader):
            # æ•°æ®ç§»åˆ°GPU
            inputs = inputs.cuda(rank)
            labels = labels.cuda(rank).squeeze()
            
            # Forward pass
            outputs = model(inputs)
            loss = nn.functional.cross_entropy(outputs, labels)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()  # DDPè‡ªåŠ¨AllReduceæ¢¯åº¦ï¼
            optimizer.step()
            
            epoch_loss += loss.item()
        
        # åªåœ¨rank 0æ‰“å°ï¼ˆé¿å…é‡å¤ï¼‰
        if rank == 0:
            avg_loss = epoch_loss / len(dataloader)
            print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹ï¼ˆåªåœ¨rank 0ä¿å­˜ï¼‰
    if rank == 0:
        # DDPæ¨¡å‹éœ€è¦é€šè¿‡.moduleè®¿é—®åŸå§‹æ¨¡å‹
        torch.save(model.module.state_dict(), "ddp_model.pth")
        print("âœ… æ¨¡å‹å·²ä¿å­˜")
    
    # æ¸…ç†
    cleanup_ddp()

# ============================================
# 4. ä¸»å‡½æ•°ï¼šå¯åŠ¨å¤šè¿›ç¨‹
# ============================================
if __name__ == "__main__":
    world_size = torch.cuda.device_count()  # è‡ªåŠ¨æ£€æµ‹GPUæ•°
    
    print(f"ä½¿ç”¨ {world_size} å—GPUè¿›è¡ŒDDPè®­ç»ƒ")
    
    # ä½¿ç”¨torch.multiprocessingå¯åŠ¨å¤šä¸ªè¿›ç¨‹
    torch.multiprocessing.spawn(
        train_ddp,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
```

**è¿è¡Œæ–¹å¼**ï¼š

```bash
# æ–¹å¼1: ä½¿ç”¨torch.multiprocessingï¼ˆä¸Šé¢ä»£ç ï¼‰
python train_ddp.py

# æ–¹å¼2: ä½¿ç”¨torchrunï¼ˆæ¨èï¼‰
torchrun --nproc_per_node=4 train_ddp.py

# æ–¹å¼3: å¤šèŠ‚ç‚¹è®­ç»ƒ
# Node 0:
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr="192.168.1.100" --master_port=12355 train_ddp.py

# Node 1:
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
    --master_addr="192.168.1.100" --master_port=12355 train_ddp.py
```

---

### 3.4 DDPå…³é”®è¦ç‚¹

```python
"""
DDPæœ€ä½³å®è·µ Checklist
"""

ddp_best_practices = {
    'âœ… å¿…é¡»åš': [
        'æ¯ä¸ªè¿›ç¨‹è®¾ç½®ä¸åŒçš„random seedï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰',
        'ä½¿ç”¨DistributedSamplerç¡®ä¿æ•°æ®ä¸é‡å¤',
        'æ¯ä¸ªepochè°ƒç”¨sampler.set_epoch(epoch)',
        'åªåœ¨rank 0ä¿å­˜æ¨¡å‹/æ‰“å°æ—¥å¿—',
        'é€šè¿‡model.moduleè®¿é—®DDPåŒ…è£…çš„æ¨¡å‹',
        'ä½¿ç”¨NCCLåç«¯ï¼ˆGPUï¼‰',
        'è®¾ç½®find_unused_parameters=Falseï¼ˆæ€§èƒ½ï¼‰'
    ],
    
    'âŒ ä¸è¦åš': [
        'ä¸è¦åœ¨ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ç»“æ„',
        'ä¸è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜æ¨¡å‹ç»“æ„',
        'ä¸è¦å¿˜è®°è°ƒç”¨dist.barrier()è¿›è¡ŒåŒæ­¥ï¼ˆéœ€è¦æ—¶ï¼‰',
        'ä¸è¦åœ¨rank > 0çš„è¿›ç¨‹ä¸­ä¿å­˜æ¨¡å‹ï¼ˆä¼šé‡å¤ï¼‰',
        'ä¸è¦ä½¿ç”¨Pythonçš„printï¼ˆæ”¹ç”¨loggingï¼‰'
    ],
    
    'âš¡ æ€§èƒ½ä¼˜åŒ–': [
        'gradient_as_bucket_view=Trueï¼ˆå‡å°‘å†…å­˜æ‹·è´ï¼‰',
        'static_graph=Trueï¼ˆå¦‚æœæ¨¡å‹ç»“æ„å›ºå®šï¼‰',
        'broadcast_buffers=Falseï¼ˆå¦‚æœæ²¡æœ‰BNå±‚ï¼‰',
        'ä½¿ç”¨torch.compile()ï¼ˆPyTorch 2.0+ï¼‰',
        'å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ'
    ]
}

# æ‰“å°
for category, items in ddp_best_practices.items():
    print(f"\n{category}:")
    for item in items:
        print(f"  {item}")
```

---

<a name="fsdp"></a>
## ğŸ”¥ 4. FSDPï¼šZeROçš„PyTorchå®ç°

### 4.1 FSDPåŸç†

**Fully Sharded Data Parallel (FSDP)** æ˜¯PyTorchå¯¹DeepSpeed ZeRO-3çš„åŸç”Ÿå®ç°ã€‚

```python
"""
FSDPåŸç†æ¼”ç¤º
"""

class FSDPPrinciple:
    """
    FSDPåŸç†ï¼šåˆ†ç‰‡å­˜å‚¨ï¼ŒæŒ‰éœ€èšåˆ
    """
    
    def compare_memory_usage(self, model_params_b=7, world_size=8):
        """
        å¯¹æ¯”DDP vs FSDPçš„æ˜¾å­˜å ç”¨
        """
        # DDPï¼šæ¯ä¸ªGPUå®Œæ•´æ¨¡å‹
        ddp_memory_per_gpu = model_params_b * 2 * 3  # å‚æ•°+æ¢¯åº¦+ä¼˜åŒ–å™¨(3å€)
        
        # FSDPï¼šå‚æ•°åˆ†ç‰‡ï¼Œä½†éœ€è¦ä¸´æ—¶èšåˆ
        fsdp_static_memory = model_params_b * 2 * 3 / world_size  # åˆ†ç‰‡å­˜å‚¨
        fsdp_dynamic_memory = model_params_b * 2  # Forwardæ—¶ä¸´æ—¶èšåˆå½“å‰å±‚
        fsdp_total_memory = fsdp_static_memory + fsdp_dynamic_memory
        
        print(f"""
        æ˜¾å­˜å¯¹æ¯” ({model_params_b}Bæ¨¡å‹, {world_size}å¡):
        
        DDP (æ¯å¡):
          æ¨¡å‹+æ¢¯åº¦+ä¼˜åŒ–å™¨: {ddp_memory_per_gpu:.1f} GB
          æ€»æ˜¾å­˜å ç”¨:       {ddp_memory_per_gpu:.1f} GB
        
        FSDP (æ¯å¡):
          åˆ†ç‰‡å­˜å‚¨:         {fsdp_static_memory:.1f} GB
          ä¸´æ—¶èšåˆ:         {fsdp_dynamic_memory:.1f} GB
          æ€»æ˜¾å­˜å ç”¨:       {fsdp_total_memory:.1f} GB
        
        èŠ‚çœæ¯”ä¾‹: {(1 - fsdp_total_memory/ddp_memory_per_gpu)*100:.0f}%
        """)

principle = FSDPPrinciple()
principle.compare_memory_usage(model_params_b=7, world_size=8)

# è¾“å‡ºï¼š
# DDP (æ¯å¡): 42.0 GB
# FSDP (æ¯å¡): 19.3 GB
# èŠ‚çœæ¯”ä¾‹: 54%
```

**FSDPå·¥ä½œæµç¨‹**ï¼š

```python
"""
FSDPå‰å‘ä¼ æ’­æµç¨‹
"""

class FSDPForwardPass:
    """
    FSDP Forward Passè¯¦ç»†æµç¨‹
    """
    
    def simulate_forward(self, num_layers=4, world_size=4):
        """
        æ¨¡æ‹ŸFSDPçš„Forward Pass
        """
        print("FSDP Forward Passæµç¨‹:")
        print("="*60)
        
        for layer_idx in range(num_layers):
            print(f"\nğŸ“ Layer {layer_idx}:")
            
            # æ­¥éª¤1: AllGatherå‚æ•°
            print(f"  1ï¸âƒ£ AllGather: æ”¶é›†åˆ†ç‰‡å‚æ•°")
            for rank in range(world_size):
                print(f"     Rank {rank}: å¹¿æ’­è‡ªå·±çš„å‚æ•°åˆ†ç‰‡")
            print(f"     â†’ æ¯ä¸ªRankç°åœ¨éƒ½æœ‰å®Œæ•´çš„Layer {layer_idx}å‚æ•°")
            
            # æ­¥éª¤2: è®¡ç®—
            print(f"  2ï¸âƒ£ Compute: Forwardè®¡ç®—")
            print(f"     â†’ output_{layer_idx} = layer_{layer_idx}(input)")
            
            # æ­¥éª¤3: é‡Šæ”¾å‚æ•°
            print(f"  3ï¸âƒ£ Free: é‡Šæ”¾å®Œæ•´å‚æ•°ï¼Œä¿ç•™åˆ†ç‰‡")
            print(f"     â†’ æ˜¾å­˜é‡Šæ”¾ï¼Œåªä¿ç•™è‡ªå·±çš„åˆ†ç‰‡")
        
        print("\n" + "="*60)
        print("Forwardå®Œæˆï¼Œæ˜¾å­˜å ç”¨æœ€å°åŒ–ï¼")

sim = FSDPForwardPass()
sim.simulate_forward()
```

---

### 4.2 FSDPå®Œæ•´å®æˆ˜ä»£ç 

```python
"""
FSDPè®­ç»ƒå®Œæ•´ç¤ºä¾‹ - PyTorch 2.0+
"""

import torch
import torch.nn as nn
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    ShardingStrategy,
    BackwardPrefetch,
    StateDictType,
)
from torch.distributed.fsdp.wrap import (
    size_based_auto_wrap_policy,
    transformer_auto_wrap_policy,
)
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================================
# 1. FSDPé…ç½®
# ============================================
def get_fsdp_config():
    """
    FSDPé…ç½®ï¼ˆ2025æœ€ä½³å®è·µï¼‰
    """
    # æ··åˆç²¾åº¦é…ç½®
    mixed_precision_policy = MixedPrecision(
        param_dtype=torch.bfloat16,      # å‚æ•°ç”¨BF16
        reduce_dtype=torch.bfloat16,     # æ¢¯åº¦è§„çº¦ç”¨BF16
        buffer_dtype=torch.bfloat16,     # Bufferç”¨BF16
    )
    
    # Shardingç­–ç•¥
    sharding_strategy = ShardingStrategy.FULL_SHARD  # ZeRO-3ï¼ˆå®Œå…¨åˆ†ç‰‡ï¼‰
    # ShardingStrategy.SHARD_GRAD_OP  # ZeRO-2ï¼ˆæ¢¯åº¦+ä¼˜åŒ–å™¨åˆ†ç‰‡ï¼‰
    # ShardingStrategy.NO_SHARD       # DDPï¼ˆæ— åˆ†ç‰‡ï¼‰
    
    return {
        'mixed_precision': mixed_precision_policy,
        'sharding_strategy': sharding_strategy,
        'backward_prefetch': BackwardPrefetch.BACKWARD_PRE,  # é¢„å–ä¼˜åŒ–
        'forward_prefetch': True,  # Forwardé¢„å–
        'limit_all_gathers': True,  # é™åˆ¶AllGatheræ˜¾å­˜å³°å€¼
        'use_orig_params': True,    # ä½¿ç”¨åŸå§‹å‚æ•°ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
    }

# ============================================
# 2. è‡ªåŠ¨åŒ…è£…ç­–ç•¥
# ============================================
def get_transformer_wrap_policy(model):
    """
    Transformeræ¨¡å‹çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥
    
    å°†æ¯ä¸ªTransformerå—ä½œä¸ºä¸€ä¸ªFSDPå•å…ƒ
    """
    from transformers.models.llama.modeling_llama import LlamaDecoderLayer
    
    # è‡ªåŠ¨åŒ…è£…ç­–ç•¥ï¼šå¯¹LlamaDecoderLayeråº”ç”¨FSDP
    auto_wrap_policy = transformer_auto_wrap_policy(
        transformer_layer_cls={LlamaDecoderLayer},
    )
    
    return auto_wrap_policy

# ============================================
# 3. è®­ç»ƒå‡½æ•°
# ============================================
def train_fsdp(rank, world_size):
    """
    FSDPè®­ç»ƒä¸»å‡½æ•°
    """
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    setup_ddp(rank, world_size)
    
    # åŠ è½½æ¨¡å‹
    model_name = "meta-llama/Llama-2-7b-hf"
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        use_cache=False  # è®­ç»ƒæ—¶å…³é—­KV Cache
    )
    
    # è·å–FSDPé…ç½®
    fsdp_config = get_fsdp_config()
    auto_wrap_policy = get_transformer_wrap_policy(model)
    
    # ğŸ”¥ ç”¨FSDPåŒ…è£…æ¨¡å‹
    model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        **fsdp_config
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=2e-5,
        betas=(0.9, 0.95),
        weight_decay=0.1
    )
    
    # å‡†å¤‡æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    dataset = prepare_dataset(tokenizer)  # è‡ªå®šä¹‰å‡½æ•°
    sampler = DistributedSampler(dataset, rank=rank, num_replicas=world_size)
    dataloader = DataLoader(dataset, batch_size=4, sampler=sampler)
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(3):
        sampler.set_epoch(epoch)
        
        for batch_idx, batch in enumerate(dataloader):
            # æ•°æ®ç§»åˆ°GPU
            input_ids = batch['input_ids'].cuda(rank)
            attention_mask = batch['attention_mask'].cuda(rank)
            labels = batch['labels'].cuda(rank)
            
            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            
            # Backward pass
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé‡è¦ï¼ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            optimizer.zero_grad()
            
            if rank == 0 and batch_idx % 10 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
    
    # ä¿å­˜æ¨¡å‹ï¼ˆFSDPç‰¹æ®Šå¤„ç†ï¼‰
    if rank == 0:
        save_fsdp_checkpoint(model, optimizer, "fsdp_checkpoint")
    
    cleanup_ddp()

# ============================================
# 4. FSDP Checkpointä¿å­˜/åŠ è½½
# ============================================
def save_fsdp_checkpoint(model, optimizer, save_path):
    """
    ä¿å­˜FSDP checkpoint
    
    FSDPæœ‰ç‰¹æ®Šçš„checkpointæ ¼å¼
    """
    from torch.distributed.fsdp import FullStateDictConfig, StateDictType
    
    # é…ç½®ï¼šå°†åˆ†ç‰‡çš„çŠ¶æ€æ”¶é›†åˆ°rank 0
    save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
    
    with FSDP.state_dict_type(
        model, 
        StateDictType.FULL_STATE_DICT, 
        save_policy
    ):
        state_dict = model.state_dict()
        optimizer_state = FSDP.optim_state_dict(model, optimizer)
    
    # åªåœ¨rank 0ä¿å­˜
    if dist.get_rank() == 0:
        checkpoint = {
            'model': state_dict,
            'optimizer': optimizer_state,
        }
        torch.save(checkpoint, f"{save_path}/checkpoint.pth")
        print(f"âœ… FSDP checkpointå·²ä¿å­˜åˆ° {save_path}")

def load_fsdp_checkpoint(model, optimizer, load_path):
    """
    åŠ è½½FSDP checkpoint
    """
    from torch.distributed.fsdp import FullStateDictConfig, StateDictType
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(f"{load_path}/checkpoint.pth", map_location='cpu')
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    load_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
    with FSDP.state_dict_type(
        model, 
        StateDictType.FULL_STATE_DICT, 
        load_policy
    ):
        model.load_state_dict(checkpoint['model'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer_state = FSDP.optim_state_dict_to_load(
        model, optimizer, checkpoint['optimizer']
    )
    optimizer.load_state_dict(optimizer_state)
    
    print(f"âœ… FSDP checkpointå·²ä» {load_path} åŠ è½½")
```

---

### 4.3 FSDP vs DDPå¯¹æ¯”

```python
"""
FSDP vs DDPæ€§èƒ½å¯¹æ¯”
"""

performance_comparison = {
    'æŒ‡æ ‡': ['æ˜¾å­˜/GPU', 'é€šä¿¡é‡', 'è®­ç»ƒé€Ÿåº¦', 'æœ€å¤§æ¨¡å‹', 'æ˜“ç”¨æ€§'],
    
    'DDP': {
        'æ˜¾å­˜/GPU': 'å®Œæ•´æ¨¡å‹ï¼ˆ42GB for 7Bï¼‰',
        'é€šä¿¡é‡': 'ä½ï¼ˆåªåŒæ­¥æ¢¯åº¦ï¼‰',
        'è®­ç»ƒé€Ÿåº¦': 'å¿«ï¼ˆé€šä¿¡å°‘ï¼‰',
        'æœ€å¤§æ¨¡å‹': 'å—å•å¡æ˜¾å­˜é™åˆ¶',
        'æ˜“ç”¨æ€§': 'â­â­â­â­â­'
    },
    
    'FSDP (ZeRO-2)': {
        'æ˜¾å­˜/GPU': 'å‚æ•°å®Œæ•´+ä¼˜åŒ–å™¨åˆ†ç‰‡ï¼ˆ~30GBï¼‰',
        'é€šä¿¡é‡': 'ä¸­ï¼ˆAllReduceæ¢¯åº¦+ä¼˜åŒ–å™¨ï¼‰',
        'è®­ç»ƒé€Ÿåº¦': 'è¾ƒå¿«',
        'æœ€å¤§æ¨¡å‹': 'å¯è®­ç»ƒ13Bçº§åˆ«',
        'æ˜“ç”¨æ€§': 'â­â­â­â­'
    },
    
    'FSDP (ZeRO-3)': {
        'æ˜¾å­˜/GPU': 'å…¨éƒ¨åˆ†ç‰‡ï¼ˆ~19GB for 7Bï¼‰',
        'é€šä¿¡é‡': 'é«˜ï¼ˆAllGatherå‚æ•°ï¼‰',
        'è®­ç»ƒé€Ÿåº¦': 'ä¸­ï¼ˆé€šä¿¡å¤šï¼‰',
        'æœ€å¤§æ¨¡å‹': 'å¯è®­ç»ƒ70B+',
        'æ˜“ç”¨æ€§': 'â­â­â­â­'
    }
}

import pandas as pd
df = pd.DataFrame(performance_comparison).set_index('æŒ‡æ ‡')
print(df.to_string())
```

**é€‰æ‹©å»ºè®®**ï¼š

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ |
|------|---------|
| 7Bæ¨¡å‹ + 8å¡A100 | DDPï¼ˆå•å¡èƒ½æ”¾ä¸‹ï¼‰ |
| 13Bæ¨¡å‹ + 8å¡A100 | FSDP ZeRO-2 |
| 70Bæ¨¡å‹ + 64å¡A100 | FSDP ZeRO-3 + TP |
| å®éªŒ/è°ƒè¯• | DDPï¼ˆæ›´ç®€å•ï¼‰ |
| ç”Ÿäº§è®­ç»ƒ | FSDPï¼ˆæ›´çµæ´»ï¼‰ |

---

<a name="deepspeed-zero"></a>
## âš¡ 5. DeepSpeed ZeROæ·±åº¦è§£æ

### 5.1 ZeROä¸‰é˜¶æ®µåŸç†

æ ¹æ® [DeepSpeedå®˜æ–¹æ–‡æ¡£](https://www.deepspeed.ai/tutorials/zero/)ï¼š

```python
"""
ZeROä¸‰é˜¶æ®µåŸç†
"""

class ZeROExplainer:
    """
    ZeRO (Zero Redundancy Optimizer) åŸç†è§£æ
    """
    
    def explain_stages(self, model_params_b=7, world_size=8):
        """
        ZeROä¸‰é˜¶æ®µå¯¹æ¯”
        """
        # åŸºç¡€æ•°æ®
        model_size_gb = model_params_b * 2  # FP16
        gradient_size_gb = model_size_gb
        optimizer_size_gb = model_params_b * 4 * 2  # Adam: moment1 + moment2 (FP32)
        
        print("="*70)
        print(f"ZeROä¸‰é˜¶æ®µå¯¹æ¯” ({model_params_b}Bæ¨¡å‹, {world_size}å¡)")
        print("="*70)
        
        # Baseline: æ— ZeRO
        baseline_memory = model_size_gb + gradient_size_gb + optimizer_size_gb
        print(f"\nâŒ Baseline (æ— ä¼˜åŒ–):")
        print(f"   æ¯å¡æ˜¾å­˜: {baseline_memory:.1f} GB")
        print(f"   è¯¦ç»†: æ¨¡å‹({model_size_gb:.1f}) + æ¢¯åº¦({gradient_size_gb:.1f}) + ä¼˜åŒ–å™¨({optimizer_size_gb:.1f})")
        
        # ZeRO Stage 1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡
        stage1_memory = model_size_gb + gradient_size_gb + optimizer_size_gb / world_size
        print(f"\n1ï¸âƒ£ ZeRO Stage 1 (ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡):")
        print(f"   æ¯å¡æ˜¾å­˜: {stage1_memory:.1f} GB")
        print(f"   èŠ‚çœ: {(1 - stage1_memory/baseline_memory)*100:.0f}%")
        print(f"   è¯¦ç»†: æ¨¡å‹({model_size_gb:.1f}) + æ¢¯åº¦({gradient_size_gb:.1f}) + ä¼˜åŒ–å™¨åˆ†ç‰‡({optimizer_size_gb/world_size:.1f})")
        
        # ZeRO Stage 2: ä¼˜åŒ–å™¨+æ¢¯åº¦åˆ†ç‰‡
        stage2_memory = model_size_gb + gradient_size_gb / world_size + optimizer_size_gb / world_size
        print(f"\n2ï¸âƒ£ ZeRO Stage 2 (ä¼˜åŒ–å™¨+æ¢¯åº¦åˆ†ç‰‡):")
        print(f"   æ¯å¡æ˜¾å­˜: {stage2_memory:.1f} GB")
        print(f"   èŠ‚çœ: {(1 - stage2_memory/baseline_memory)*100:.0f}%")
        print(f"   è¯¦ç»†: æ¨¡å‹({model_size_gb:.1f}) + æ¢¯åº¦åˆ†ç‰‡({gradient_size_gb/world_size:.1f}) + ä¼˜åŒ–å™¨åˆ†ç‰‡({optimizer_size_gb/world_size:.1f})")
        
        # ZeRO Stage 3: å…¨éƒ¨åˆ†ç‰‡ï¼ˆä¸FSDPç›¸åŒï¼‰
        stage3_memory = (model_size_gb + gradient_size_gb + optimizer_size_gb) / world_size
        print(f"\n3ï¸âƒ£ ZeRO Stage 3 (å…¨éƒ¨åˆ†ç‰‡):")
        print(f"   æ¯å¡æ˜¾å­˜: {stage3_memory:.1f} GB")
        print(f"   èŠ‚çœ: {(1 - stage3_memory/baseline_memory)*100:.0f}%")
        print(f"   è¯¦ç»†: å…¨éƒ¨çŠ¶æ€åˆ†ç‰‡({(model_size_gb + gradient_size_gb + optimizer_size_gb)/world_size:.1f})")
        
        print("\n" + "="*70)

explainer = ZeROExplainer()
explainer.explain_stages(model_params_b=7, world_size=8)

# è¾“å‡ºï¼š
# âŒ Baseline: 70.0 GB
# 1ï¸âƒ£ Stage 1: 35.0 GB (èŠ‚çœ 50%)
# 2ï¸âƒ£ Stage 2: 19.8 GB (èŠ‚çœ 72%)
# 3ï¸âƒ£ Stage 3: 8.8 GB (èŠ‚çœ 87%)
```

---

### 5.2 DeepSpeedå®Œæ•´å®æˆ˜

```python
"""
DeepSpeed ZeROè®­ç»ƒå®Œæ•´ç¤ºä¾‹
"""

import torch
import deepspeed
from deepspeed.ops.adam import DeepSpeedCPUAdam
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================================
# 1. DeepSpeedé…ç½®æ–‡ä»¶ (ds_config.json)
# ============================================
ds_config = {
    "train_batch_size": 32,               # å…¨å±€batch size
    "train_micro_batch_size_per_gpu": 4,  # æ¯å¡batch size
    "gradient_accumulation_steps": 1,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    
    # ğŸ”¥ ZeROä¼˜åŒ–é…ç½®
    "zero_optimization": {
        "stage": 3,                        # ZeRO Stage 3
        
        # Stage 3ç‰¹å®šé…ç½®
        "offload_param": {
            "device": "cpu",               # å‚æ•°offloadåˆ°CPU
            "pin_memory": True
        },
        "offload_optimizer": {
            "device": "cpu",               # ä¼˜åŒ–å™¨offloadåˆ°CPU
            "pin_memory": True
        },
        
        # é€šä¿¡ä¼˜åŒ–
        "overlap_comm": True,              # é€šä¿¡ä¸è®¡ç®—é‡å 
        "contiguous_gradients": True,      # æ¢¯åº¦è¿ç»­å­˜å‚¨
        "sub_group_size": 1e9,             # AllGatheråˆ†ç»„å¤§å°
        "reduce_bucket_size": 5e8,         # Reduce bucketå¤§å°
        "stage3_prefetch_bucket_size": 5e8,  # é¢„å–bucket
        "stage3_param_persistence_threshold": 1e6,  # å‚æ•°æŒä¹…åŒ–é˜ˆå€¼
        
        # å†…å­˜ä¼˜åŒ–
        "stage3_max_live_parameters": 1e9,   # æœ€å¤§å­˜æ´»å‚æ•°
        "stage3_max_reuse_distance": 1e9,    # å‚æ•°å¤ç”¨è·ç¦»
        "stage3_gather_16bit_weights_on_model_save": True  # ä¿å­˜æ—¶æ”¶é›†FP16æƒé‡
    },
    
    # æ··åˆç²¾åº¦
    "fp16": {
        "enabled": True,
        "loss_scale": 0,                   # åŠ¨æ€loss scaling
        "loss_scale_window": 1000,
        "hysteresis": 2,
        "min_loss_scale": 1
    },
    
    # æˆ–ä½¿ç”¨BF16ï¼ˆH100æ¨èï¼‰
    # "bf16": {
    #     "enabled": True
    # },
    
    # ä¼˜åŒ–å™¨
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-5,
            "betas": [0.9, 0.95],
            "eps": 1e-8,
            "weight_decay": 0.1
        }
    },
    
    # å­¦ä¹ ç‡è°ƒåº¦
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-5,
            "warmup_num_steps": 100,
            "total_num_steps": 10000
        }
    },
    
    # Activation Checkpointingï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
    "activation_checkpointing": {
        "partition_activations": True,
        "cpu_checkpointing": False,
        "contiguous_memory_optimization": True,
        "number_checkpoints": None,
        "synchronize_checkpoint_boundary": False,
        "profile": False
    },
    
    # æ¢¯åº¦è£å‰ª
    "gradient_clipping": 1.0,
    
    # æ—¥å¿—
    "steps_per_print": 10,
    "wall_clock_breakdown": False
}

# ä¿å­˜é…ç½®æ–‡ä»¶
import json
with open('ds_config.json', 'w') as f:
    json.dump(ds_config, f, indent=2)

# ============================================
# 2. è®­ç»ƒè„šæœ¬
# ============================================
def train_deepspeed():
    """
    DeepSpeedè®­ç»ƒä¸»å‡½æ•°
    """
    # åŠ è½½æ¨¡å‹
    model_name = "meta-llama/Llama-2-7b-hf"
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        use_cache=False
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # å‡†å¤‡æ•°æ®
    train_dataset = prepare_dataset(tokenizer)
    
    # ğŸ”¥ åˆå§‹åŒ–DeepSpeed
    model_engine, optimizer, train_dataloader, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        training_data=train_dataset,
        config='ds_config.json'  # é…ç½®æ–‡ä»¶è·¯å¾„
    )
    
    # è®­ç»ƒå¾ªç¯
    model_engine.train()
    for epoch in range(3):
        for step, batch in enumerate(train_dataloader):
            # æ•°æ®ç§»åˆ°GPUï¼ˆDeepSpeedè‡ªåŠ¨å¤„ç†è®¾å¤‡ï¼‰
            input_ids = batch['input_ids'].to(model_engine.device)
            attention_mask = batch['attention_mask'].to(model_engine.device)
            labels = batch['labels'].to(model_engine.device)
            
            # Forward pass
            outputs = model_engine(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            
            # Backward passï¼ˆDeepSpeedè‡ªåŠ¨å¤„ç†ï¼‰
            model_engine.backward(loss)
            
            # Optimizer stepï¼ˆDeepSpeedè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯ï¼‰
            model_engine.step()
            
            if step % 10 == 0:
                print(f"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}")
    
    # ä¿å­˜checkpoint
    model_engine.save_checkpoint('deepspeed_checkpoint')
    print("âœ… DeepSpeed checkpointå·²ä¿å­˜")

# ============================================
# 3. è¿è¡Œæ–¹å¼
# ============================================
if __name__ == "__main__":
    # ä½¿ç”¨deepspeedå‘½ä»¤å¯åŠ¨
    # deepspeed --num_gpus=8 train_deepspeed.py
    train_deepspeed()
```

**è¿è¡Œå‘½ä»¤**ï¼š

```bash
# å•æœº8å¡
deepspeed --num_gpus=8 train_deepspeed.py

# å¤šèŠ‚ç‚¹ï¼ˆ2å°æœºå™¨ï¼Œæ¯å°8å¡ï¼‰
# Node 0:
deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 \
    --master_addr=192.168.1.100 --master_port=29500 train_deepspeed.py

# Node 1:
deepspeed --num_gpus=8 --num_nodes=2 --node_rank=1 \
    --master_addr=192.168.1.100 --master_port=29500 train_deepspeed.py

# ä½¿ç”¨hostfileï¼ˆæ¨èï¼‰
# hostfileå†…å®¹:
# node0_hostname slots=8
# node1_hostname slots=8

deepspeed --hostfile=hostfile train_deepspeed.py
```

---

### 5.3 ZeRO++ ä¼˜åŒ–ï¼ˆ2025æœ€æ–°ï¼‰

æ ¹æ® [DeepSpeed ZeRO++æ–‡æ¡£](https://www.deepspeed.ai/tutorials/zeropp/)ï¼š

```python
"""
ZeRO++ ä¼˜åŒ–é…ç½®ï¼ˆ2025ï¼‰
"""

zeropp_config = {
    "zero_optimization": {
        "stage": 3,
        
        # ğŸ”¥ ZeRO++ æ–°ç‰¹æ€§
        "zero_quantized_weights": True,         # qwZ: INT8é‡åŒ–æƒé‡
        "zero_hpz_partition_size": 16,          # hpZ: åˆ†å±‚åˆ†åŒº
        "zero_quantized_gradients": True,       # qgZ: é‡åŒ–æ¢¯åº¦
        
        # ZeRO++ å¯å‡å°‘4xé€šä¿¡é‡ï¼
        "reduce_scatter": True,
        "allgather_bucket_size": 5e8,
        
        # å…¶ä»–é…ç½®...
    }
}

# æ•ˆæœï¼š
# - å‚æ•°AllGatheré€šä¿¡é‡å‡åŠï¼ˆqwZï¼‰
# - åå‘ä¼ æ’­è·¨èŠ‚ç‚¹é€šä¿¡æ¶ˆé™¤ï¼ˆhpZï¼‰
# - æ¢¯åº¦AllReduceæ›¿æ¢ä¸ºé«˜æ•ˆAllToAllï¼ˆqgZï¼‰
```

> ğŸ”¥ **2025æ¨è**ï¼šä½¿ç”¨ZeRO++ Stage 3å¯å°†é€šä¿¡é‡å‡å°‘4å€ï¼Œè®­ç»ƒ175Bæ¨¡å‹é€Ÿåº¦æå‡2.16å€ï¼

---

<a name="megatron"></a>
## âœ‚ï¸ 6. Megatron-LMï¼šå¼ é‡ä¸æµæ°´çº¿å¹¶è¡Œ

### 6.1 å¼ é‡å¹¶è¡Œ (Tensor Parallelism, TP)

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†å•ä¸ªå±‚çš„æƒé‡çŸ©é˜µæŒ‰åˆ—/è¡Œåˆ‡åˆ†åˆ°å¤šGPUã€‚

```python
"""
å¼ é‡å¹¶è¡ŒåŸç†æ¼”ç¤º
"""

class TensorParallelismExplainer:
    """
    å¼ é‡å¹¶è¡ŒåŸç†
    """
    
    def explain_column_parallel(self):
        """
        åˆ—å¹¶è¡Œï¼šLinearå±‚çš„åˆ—åˆ‡åˆ†
        
        åŸå§‹ï¼šY = X @ W
        åˆ‡åˆ†ï¼šY = X @ [W1 | W2] = [X@W1 | X@W2]
        """
        print("åˆ—å¹¶è¡Œï¼ˆColumn Parallelï¼‰:")
        print("="*60)
        print("åŸå§‹æ“ä½œï¼š")
        print("  Y = X @ W")
        print("  å…¶ä¸­ W shape: [hidden_dim, hidden_dim]")
        print()
        print("åˆ‡åˆ†åˆ°2ä¸ªGPUï¼š")
        print("  GPU 0: Y1 = X @ W1  (W1æ˜¯Wçš„å‰åŠåˆ—)")
        print("  GPU 1: Y2 = X @ W2  (W2æ˜¯Wçš„ååŠåˆ—)")
        print()
        print("æ‹¼æ¥ï¼š")
        print("  Y = [Y1 | Y2]  (ç›´æ¥æ‹¼æ¥ï¼Œæ— éœ€é€šä¿¡ï¼)")
        print()
        
    def explain_row_parallel(self):
        """
        è¡Œå¹¶è¡Œï¼šLinearå±‚çš„è¡Œåˆ‡åˆ†
        """
        print("\nè¡Œå¹¶è¡Œï¼ˆRow Parallelï¼‰:")
        print("="*60)
        print("åŸå§‹æ“ä½œï¼š")
        print("  Y = X @ W")
        print()
        print("åˆ‡åˆ†åˆ°2ä¸ªGPUï¼š")
        print("  GPU 0: Y1 = X[:, :half] @ W1  (W1æ˜¯Wçš„å‰åŠè¡Œ)")
        print("  GPU 1: Y2 = X[:, half:] @ W2  (W2æ˜¯Wçš„ååŠè¡Œ)")
        print()
        print("å½’çº¦ï¼š")
        print("  Y = AllReduce(Y1 + Y2)  (éœ€è¦é€šä¿¡ï¼)")
        print()

explainer = TensorParallelismExplainer()
explainer.explain_column_parallel()
explainer.explain_row_parallel()
```

---

### 6.2 Transformerä¸­çš„TPå®ç°

```python
"""
Megatron-LMé£æ ¼çš„Transformer TP
"""

import torch
import torch.nn as nn
import torch.distributed as dist

class ColumnParallelLinear(nn.Module):
    """
    åˆ—å¹¶è¡ŒLinearå±‚
    
    å°†æƒé‡çŸ©é˜µæŒ‰åˆ—åˆ‡åˆ†åˆ°å¤šGPU
    """
    def __init__(self, input_size, output_size, tp_size):
        super().__init__()
        self.tp_size = tp_size
        self.rank = dist.get_rank()
        
        # æ¯ä¸ªGPUåªå­˜å‚¨ output_size / tp_size åˆ—
        self.output_size_per_partition = output_size // tp_size
        
        # æƒé‡çŸ©é˜µåˆ†ç‰‡
        self.weight = nn.Parameter(
            torch.randn(input_size, self.output_size_per_partition)
        )
    
    def forward(self, x):
        # Forwardï¼šæ¯ä¸ªGPUç‹¬ç«‹è®¡ç®—è‡ªå·±çš„åˆ†ç‰‡
        output = torch.matmul(x, self.weight)
        return output  # è¿”å›åˆ†ç‰‡ç»“æœï¼Œåç»­æ‹¼æ¥

class RowParallelLinear(nn.Module):
    """
    è¡Œå¹¶è¡ŒLinearå±‚
    
    å°†æƒé‡çŸ©é˜µæŒ‰è¡Œåˆ‡åˆ†åˆ°å¤šGPU
    """
    def __init__(self, input_size, output_size, tp_size):
        super().__init__()
        self.tp_size = tp_size
        self.rank = dist.get_rank()
        
        # æ¯ä¸ªGPUåªå­˜å‚¨ input_size / tp_size è¡Œ
        self.input_size_per_partition = input_size // tp_size
        
        # æƒé‡çŸ©é˜µåˆ†ç‰‡
        self.weight = nn.Parameter(
            torch.randn(self.input_size_per_partition, output_size)
        )
    
    def forward(self, x):
        # è¾“å…¥ä¹Ÿéœ€è¦åˆ†ç‰‡
        # x shape: [batch, seq_len, input_size]
        # æ¯ä¸ªGPUåªå¤„ç†input_sizeçš„ä¸€éƒ¨åˆ†
        
        # Forwardï¼šæ¯ä¸ªGPUç‹¬ç«‹è®¡ç®—
        output_parallel = torch.matmul(x, self.weight)
        
        # AllReduceï¼šå½’çº¦æ‰€æœ‰GPUçš„ç»“æœ
        dist.all_reduce(output_parallel)
        
        return output_parallel

class TransformerLayerTP(nn.Module):
    """
    å¸¦å¼ é‡å¹¶è¡Œçš„Transformerå±‚
    
    Megatron-LMçš„å®ç°æ–¹å¼
    """
    def __init__(self, hidden_size, ffn_hidden_size, tp_size):
        super().__init__()
        self.tp_size = tp_size
        
        # Attentionçš„Q/K/VæŠ•å½±ï¼šåˆ—å¹¶è¡Œ
        self.query_key_value = ColumnParallelLinear(
            hidden_size, 3 * hidden_size, tp_size
        )
        
        # Attentionè¾“å‡ºæŠ•å½±ï¼šè¡Œå¹¶è¡Œ
        self.dense = RowParallelLinear(
            hidden_size, hidden_size, tp_size
        )
        
        # FFNç¬¬ä¸€å±‚ï¼šåˆ—å¹¶è¡Œ
        self.mlp_h_to_4h = ColumnParallelLinear(
            hidden_size, ffn_hidden_size, tp_size
        )
        
        # FFNç¬¬äºŒå±‚ï¼šè¡Œå¹¶è¡Œ
        self.mlp_4h_to_h = RowParallelLinear(
            ffn_hidden_size, hidden_size, tp_size
        )
    
    def forward(self, hidden_states):
        # Attention
        qkv = self.query_key_value(hidden_states)
        # ... attentionè®¡ç®—ï¼ˆçœç•¥ï¼‰
        attention_output = self.dense(attention_output)
        
        # FFN
        ffn_hidden = self.mlp_h_to_4h(hidden_states)
        ffn_hidden = torch.nn.functional.gelu(ffn_hidden)
        ffn_output = self.mlp_4h_to_h(ffn_hidden)
        
        return ffn_output

# é€šä¿¡åˆ†æ
"""
æ¯ä¸ªTransformerå±‚çš„é€šä¿¡ï¼š
  Forward:
    - Attentionè¾“å‡ºï¼š1æ¬¡AllReduce (denseå±‚)
    - FFNè¾“å‡ºï¼š1æ¬¡AllReduce (mlp_4h_to_hå±‚)
    æ€»è®¡ï¼š2æ¬¡AllReduce
  
  Backward:
    - æ¢¯åº¦åå‘ä¼ æ’­ï¼š2æ¬¡AllReduce
    æ€»è®¡ï¼š2æ¬¡AllReduce

æ¯å±‚4æ¬¡AllReduceï¼Œé€šä¿¡é‡ = 4 * hidden_size * batch_size * seq_len
"""
```

---

### 6.3 æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism, PP)

```python
"""
æµæ°´çº¿å¹¶è¡ŒåŸç†
"""

class PipelineParallelismExplainer:
    """
    æµæ°´çº¿å¹¶è¡Œï¼šæŒ‰å±‚åˆ‡åˆ†æ¨¡å‹
    """
    
    def explain_gpipe_schedule(self, num_stages=4, num_microbatches=8):
        """
        GPipeè°ƒåº¦ï¼šç®€å•ä½†æœ‰æ°”æ³¡
        """
        print("GPipeè°ƒåº¦ï¼ˆæœ´ç´ æµæ°´çº¿ï¼‰:")
        print("="*60)
        
        # æ¨¡æ‹Ÿæ—¶é—´çº¿
        for stage in range(num_stages):
            print(f"\nStage {stage} (GPU {stage}):")
            timeline = [" "] * (num_stages + num_microbatches)
            
            # Forward pass
            for mb in range(num_microbatches):
                timeline[stage + mb] = "F"
            
            # Backward passï¼ˆreverse orderï¼‰
            for mb in range(num_microbatches):
                timeline[stage + num_microbatches + (num_microbatches - 1 - mb)] = "B"
            
            print("  " + "".join(timeline))
        
        bubble_time = num_stages - 1
        total_time = num_stages + num_microbatches * 2 - 1
        bubble_ratio = bubble_time / total_time
        
        print(f"\næ°”æ³¡æ—¶é—´: {bubble_time} / {total_time} = {bubble_ratio:.1%}")
    
    def explain_1f1b_schedule(self, num_stages=4, num_microbatches=8):
        """
        1F1Bè°ƒåº¦ï¼šMegatron-LMçš„ä¼˜åŒ–è°ƒåº¦
        
        æ ¸å¿ƒï¼šForwardå’ŒBackwardäº¤æ›¿ï¼Œå‡å°‘æ°”æ³¡
        """
        print("\n\n1F1Bè°ƒåº¦ï¼ˆMegatron-LMï¼‰:")
        print("="*60)
        print("ä¼˜åŒ–ï¼šæ¯å®Œæˆä¸€ä¸ªmicrobatchçš„Forwardï¼Œç«‹å³æ‰§è¡ŒBackward")
        print("æ•ˆæœï¼šæ°”æ³¡æ—¶é—´å‡å°‘åˆ° (num_stages - 1) ä¸ªmicrobatch")
        
        for stage in range(num_stages):
            print(f"\nStage {stage}:")
            timeline = [" "] * 20
            
            # Warmup: å‰num_stagesä¸ªForward
            for i in range(stage, num_stages):
                timeline[i] = "F"
            
            # Steady state: 1F1Bäº¤æ›¿
            pos = num_stages
            for i in range(num_microbatches - num_stages):
                timeline[pos] = "F"
                timeline[pos + 1] = "B"
                pos += 2
            
            # Cooldown: å‰©ä½™Backward
            for i in range(num_stages - stage - 1):
                timeline[pos] = "B"
                pos += 1
            
            print("  " + "".join(timeline))

explainer = PipelineParallelismExplainer()
explainer.explain_gpipe_schedule()
explainer.explain_1f1b_schedule()
```

---

### 6.4 Megatron-LMå®Œæ•´ç¤ºä¾‹

```python
"""
Megatron-LMé£æ ¼çš„è®­ç»ƒè„šæœ¬
"""

import torch
import torch.distributed as dist
from torch.distributed.pipeline.sync import Pipe

# ä½¿ç”¨Megatron-LMåº“
# pip install megatron-core

from megatron.core import parallel_state
from megatron.core.tensor_parallel import (
    ColumnParallelLinear,
    RowParallelLinear,
)
from megatron.core.pipeline_parallel import schedules

def setup_megatron_parallel(tp_size=4, pp_size=2):
    """
    åˆå§‹åŒ–Megatronå¹¶è¡Œ
    
    å‚æ•°:
        tp_size: å¼ é‡å¹¶è¡Œå¤§å°
        pp_size: æµæ°´çº¿å¹¶è¡Œå¤§å°
    """
    world_size = dist.get_world_size()
    assert world_size == tp_size * pp_size
    
    # åˆå§‹åŒ–å¹¶è¡Œç»„
    parallel_state.initialize_model_parallel(
        tensor_model_parallel_size=tp_size,
        pipeline_model_parallel_size=pp_size
    )
    
    # è·å–å½“å‰rankçš„å¹¶è¡Œä¿¡æ¯
    tp_rank = parallel_state.get_tensor_model_parallel_rank()
    pp_rank = parallel_state.get_pipeline_model_parallel_rank()
    
    print(f"Rank {dist.get_rank()}: TP rank={tp_rank}, PP rank={pp_rank}")

def train_megatron_model():
    """
    Megatronè®­ç»ƒç¤ºä¾‹
    """
    # åˆå§‹åŒ–
    dist.init_process_group(backend='nccl')
    setup_megatron_parallel(tp_size=4, pp_size=2)
    
    # æ„å»ºæ¨¡å‹ï¼ˆæ¯ä¸ªPP rankåªæ„å»ºè‡ªå·±çš„å±‚ï¼‰
    pp_rank = parallel_state.get_pipeline_model_parallel_rank()
    
    if pp_rank == 0:
        # å‰åŠéƒ¨åˆ†å±‚
        model = nn.ModuleList([
            TransformerLayerTP(...) for _ in range(16)
        ])
    else:
        # ååŠéƒ¨åˆ†å±‚
        model = nn.ModuleList([
            TransformerLayerTP(...) for _ in range(16)
        ])
    
    # è®­ç»ƒï¼ˆä½¿ç”¨1F1Bè°ƒåº¦ï¼‰
    optimizer = torch.optim.AdamW(model.parameters())
    
    for batch in dataloader:
        # ä½¿ç”¨Megatronçš„è°ƒåº¦å™¨
        loss = schedules.forward_backward_pipelining_without_interleaving(
            forward_step_func=forward_step,
            data_iterator=iter([batch]),
            model=model,
            num_microbatches=8,
            seq_length=2048,
            micro_batch_size=4,
        )
        
        optimizer.step()
        optimizer.zero_grad()

# è¿è¡Œæ–¹å¼ï¼š
# torchrun --nproc_per_node=8 train_megatron.py
# å…¶ä¸­8 = tp_size(4) * pp_size(2)
```

---

<a name="3d-parallelism"></a>
## ğŸ¯ 7. 3Då¹¶è¡Œï¼šDP+TP+PPç»„åˆæ‹³

### 7.1 3Då¹¶è¡ŒåŸç†

```python
"""
3Då¹¶è¡Œï¼šå¤§è§„æ¨¡è®­ç»ƒçš„æ ‡å‡†æ–¹æ¡ˆ
"""

class ThreeDParallelism:
    """
    3Då¹¶è¡Œç»„åˆ
    
    Data Parallel (DP) + Tensor Parallel (TP) + Pipeline Parallel (PP)
    """
    
    def calculate_parallelism_config(
        self, 
        model_params_b=175,
        total_gpus=512,
        gpu_memory_gb=40
    ):
        """
        ä¸ºå¤§æ¨¡å‹è®¾è®¡3Då¹¶è¡Œé…ç½®
        """
        print(f"è®¾è®¡3Då¹¶è¡Œæ–¹æ¡ˆï¼š{model_params_b}Bæ¨¡å‹ï¼Œ{total_gpus}å—GPU")
        print("="*70)
        
        # 1. ç¡®å®šTP sizeï¼ˆåŸºäºå±‚å¤§å°ï¼‰
        # ç»éªŒï¼šhidden_dim > 8192æ—¶æ‰éœ€è¦TP
        if model_params_b >= 70:
            tp_size = 8  # è¶…å¤§æ¨¡å‹
        elif model_params_b >= 13:
            tp_size = 4  # å¤§æ¨¡å‹
        else:
            tp_size = 1  # å°æ¨¡å‹ä¸éœ€è¦TP
        
        # 2. ç¡®å®šPP sizeï¼ˆåŸºäºæ¨¡å‹æ·±åº¦å’Œæ˜¾å­˜ï¼‰
        # æ¯ä¸ªstageçš„æ˜¾å­˜å ç”¨ = æ¨¡å‹å‚æ•° / pp_size
        model_memory_per_stage = (model_params_b * 2) / 1  # FP16, åˆå§‹å‡è®¾pp_size=1
        
        # æ‰¾åˆ°æœ€å°çš„pp_sizeä½¿å¾—æ¯stageèƒ½æ”¾å…¥æ˜¾å­˜
        pp_size = 1
        while model_memory_per_stage / pp_size > gpu_memory_gb * 0.5:  # ç•™50%ç»™æ¿€æ´»å€¼
            pp_size *= 2
        
        # 3. å‰©ä½™GPUç”¨äºDP
        dp_size = total_gpus // (tp_size * pp_size)
        
        print(f"\næ¨èé…ç½®:")
        print(f"  TP (Tensor Parallel):    {tp_size}  (å±‚å†…å¹¶è¡Œ)")
        print(f"  PP (Pipeline Parallel):  {pp_size}  (å±‚é—´å¹¶è¡Œ)")
        print(f"  DP (Data Parallel):      {dp_size}  (æ•°æ®å¹¶è¡Œ)")
        print(f"  æ€»GPU: {tp_size} Ã— {pp_size} Ã— {dp_size} = {tp_size * pp_size * dp_size}")
        
        print(f"\næ˜¾å­˜åˆ†æ:")
        memory_per_gpu = (model_params_b * 2) / (tp_size * pp_size) + 10  # æ¨¡å‹+æ¿€æ´»å€¼
        print(f"  æ¯å¡æ˜¾å­˜éœ€æ±‚: ~{memory_per_gpu:.1f} GB")
        print(f"  A100-40GB: {'âœ… å¯è¡Œ' if memory_per_gpu < 40 else 'âŒ ä¸å¯è¡Œ'}")
        
        print(f"\næœ‰æ•ˆbatch size:")
        micro_batch_size = 2
        global_batch_size = micro_batch_size * dp_size * pp_size  # PPä¼šåšgradient accumulation
        print(f"  Micro batch size: {micro_batch_size}")
        print(f"  Global batch size: {global_batch_size}")
        
        return {
            'tp_size': tp_size,
            'pp_size': pp_size,
            'dp_size': dp_size,
            'global_batch_size': global_batch_size
        }

designer = ThreeDParallelism()

# ç¤ºä¾‹1: GPT-3 175B, 512 GPUs
designer.calculate_parallelism_config(model_params_b=175, total_gpus=512)

# ç¤ºä¾‹2: LLaMA-70B, 64 GPUs
designer.calculate_parallelism_config(model_params_b=70, total_gpus=64)

# è¾“å‡ºç¤ºä¾‹ï¼š
# 175Bæ¨¡å‹ï¼Œ512å—GPU
# æ¨èé…ç½®:
#   TP: 8, PP: 16, DP: 4
#   æ€»GPU: 8 Ã— 16 Ã— 4 = 512
#   æ¯å¡æ˜¾å­˜éœ€æ±‚: ~32.8 GB
#   Global batch size: 128
```

---

### 7.2 3Då¹¶è¡Œå®Œæ•´å®ç°

```python
"""
3Då¹¶è¡Œè®­ç»ƒï¼ˆMegatron-LM + DeepSpeedï¼‰
"""

# DeepSpeedé…ç½®æ–‡ä»¶
ds_config_3d = {
    "train_batch_size": 128,
    "train_micro_batch_size_per_gpu": 2,
    "steps_per_print": 10,
    
    # ğŸ”¥ ZeRO Stage 1ï¼ˆç”¨äºDPç»´åº¦ï¼‰
    "zero_optimization": {
        "stage": 1,  # TP+PPæ—¶åªç”¨Stage 1
        "offload_optimizer": False,
    },
    
    # FP16
    "fp16": {
        "enabled": True,
        "loss_scale": 0,
        "loss_scale_window": 1000,
    },
    
    # Pipelineé…ç½®
    "pipeline": {
        "activation_checkpoint_interval": 1,
    },
    
    # å¼ é‡å¹¶è¡Œï¼ˆåœ¨Megatronå±‚é¢é…ç½®ï¼‰
    "tensor_parallel": {
        "tp_size": 8,
    },
}

def train_3d_parallel():
    """
    3Då¹¶è¡Œè®­ç»ƒä¸»å‡½æ•°
    """
    import deepspeed
    from megatron.core import parallel_state
    
    # 1. åˆå§‹åŒ–å¹¶è¡Œç»„
    world_size = dist.get_world_size()  # 512
    tp_size = 8
    pp_size = 16
    dp_size = world_size // (tp_size * pp_size)  # 4
    
    parallel_state.initialize_model_parallel(
        tensor_model_parallel_size=tp_size,
        pipeline_model_parallel_size=pp_size
    )
    
    # 2. æ„å»ºæ¨¡å‹ï¼ˆæŒ‰PP rankï¼‰
    pp_rank = parallel_state.get_pipeline_model_parallel_rank()
    layers_per_stage = total_layers // pp_size
    
    start_layer = pp_rank * layers_per_stage
    end_layer = (pp_rank + 1) * layers_per_stage
    
    model = build_model_layers(start_layer, end_layer, tp_size)
    
    # 3. DeepSpeedåˆå§‹åŒ–ï¼ˆå¤„ç†DPç»´åº¦ï¼‰
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        config=ds_config_3d,
    )
    
    # 4. è®­ç»ƒå¾ªç¯
    for step, batch in enumerate(dataloader):
        # ä½¿ç”¨1F1Bæµæ°´çº¿è°ƒåº¦
        loss = forward_backward_pipelining(
            model_engine,
            batch,
            num_microbatches=pp_size * dp_size
        )
        
        model_engine.step()
    
    print("âœ… 3Då¹¶è¡Œè®­ç»ƒå®Œæˆ")

# è¿è¡Œï¼š
# deepspeed --num_gpus=512 --num_nodes=64 train_3d.py
```

---

<a name="communication"></a>
## ğŸ“¡ 8. é€šä¿¡ä¼˜åŒ–ï¼šNCCLä¸Ring-AllReduce

### 8.1 NCCLé€šä¿¡åŸè¯­

```python
"""
NCCLæ ¸å¿ƒé€šä¿¡æ“ä½œ
"""

import torch
import torch.distributed as dist

class NCCLOperations:
    """
    NCCLé€šä¿¡åŸè¯­æ¼”ç¤º
    """
    
    def demo_all_reduce(self, rank, world_size):
        """
        AllReduce: æ‰€æœ‰GPUçš„æ•°æ®æ±‚å’Œå¹¶å¹¿æ’­
        
        ç”¨é€”ï¼šDDPæ¢¯åº¦åŒæ­¥
        """
        # åˆ›å»ºæ•°æ®
        tensor = torch.ones(10) * (rank + 1)  # GPU 0:[1,1,...], GPU 1:[2,2,...]
        tensor = tensor.cuda()
        
        print(f"Rank {rank} before AllReduce: {tensor[:3]}")
        
        # AllReduceï¼ˆæ±‚å’Œï¼‰
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        
        print(f"Rank {rank} after AllReduce: {tensor[:3]}")
        # æ‰€æœ‰GPUéƒ½æœ‰ç›¸åŒç»“æœ: [10, 10, ...] (1+2+3+4)
    
    def demo_all_gather(self, rank, world_size):
        """
        AllGather: æ”¶é›†æ‰€æœ‰GPUçš„æ•°æ®
        
        ç”¨é€”ï¼šFSDPå‚æ•°èšåˆ
        """
        # æ¯ä¸ªGPUçš„æ•°æ®
        tensor = torch.ones(10) * (rank + 1)
        tensor = tensor.cuda()
        
        # å‡†å¤‡æ¥æ”¶buffer
        tensor_list = [torch.zeros_like(tensor) for _ in range(world_size)]
        
        # AllGather
        dist.all_gather(tensor_list, tensor)
        
        print(f"Rank {rank} gathered: {[t[0].item() for t in tensor_list]}")
        # [1, 2, 3, 4] - æ”¶é›†äº†æ‰€æœ‰GPUçš„æ•°æ®
    
    def demo_reduce_scatter(self, rank, world_size):
        """
        ReduceScatter: AllReduce + Scatterçš„ç»„åˆ
        
        ç”¨é€”ï¼šä¼˜åŒ–çš„æ¢¯åº¦åˆ†ç‰‡
        """
        # è¾“å…¥ï¼šæ¯ä¸ªGPUæœ‰å®Œæ•´æ•°æ®
        tensor = torch.ones(world_size * 10) * (rank + 1)
        tensor = tensor.cuda()
        
        # è¾“å‡ºï¼šæ¯ä¸ªGPUæ¥æ”¶è‡ªå·±çš„åˆ†ç‰‡ï¼ˆå·²reduceï¼‰
        output = torch.zeros(10).cuda()
        
        # ReduceScatter
        dist.reduce_scatter(output, list(tensor.chunk(world_size)))
        
        print(f"Rank {rank} result: {output[:3]}")

# NCCLé€šä¿¡é‡åˆ†æ
"""
AllReduce: 2 * data_size * (world_size - 1) / world_size
  â‰ˆ 2 * data_size (world_sizeå¾ˆå¤§æ—¶)

AllGather: data_size * (world_size - 1)

ReduceScatter: data_size * (world_size - 1) / world_size
"""
```

---

### 8.2 Ring-AllReduceç®—æ³•

```python
"""
Ring-AllReduceåŸç†
"""

class RingAllReduceExplainer:
    """
    Ring-AllReduce: NCCLçš„æ ¸å¿ƒç®—æ³•
    """
    
    def explain_algorithm(self, world_size=4):
        """
        Ring-AllReduceä¸¤é˜¶æ®µï¼š
        1. ReduceScatter: æ¯ä¸ªGPUæ”¶åˆ°è‡ªå·±è´Ÿè´£chunkçš„sum
        2. AllGather: å¹¿æ’­å®Œæ•´ç»“æœ
        """
        print("Ring-AllReduceç®—æ³•æ¼”ç¤º")
        print("="*70)
        print(f"åœºæ™¯ï¼š{world_size}ä¸ªGPUï¼Œæ¯ä¸ªGPUæœ‰4ä¸ªchunkçš„æ¢¯åº¦")
        print()
        
        # åˆå§‹çŠ¶æ€
        print("åˆå§‹çŠ¶æ€:")
        for rank in range(world_size):
            print(f"  GPU {rank}: [A{rank}, B{rank}, C{rank}, D{rank}]")
        print()
        
        # é˜¶æ®µ1: ReduceScatter
        print("é˜¶æ®µ1ï¼šReduceScatter (world_size-1 æ­¥)")
        for step in range(world_size - 1):
            print(f"  Step {step+1}:")
            for rank in range(world_size):
                send_to = (rank + 1) % world_size
                recv_from = (rank - 1) % world_size
                print(f"    GPU {rank}: å‘é€ç»™GPU {send_to}ï¼Œæ¥æ”¶è‡ªGPU {recv_from}")
        
        print(f"  ç»“æœï¼šæ¯ä¸ªGPUæœ‰ä¸€ä¸ªchunkçš„å®Œæ•´sum")
        print(f"    GPU 0: [A_sum, ?, ?, ?]")
        print(f"    GPU 1: [?, B_sum, ?, ?]")
        print(f"    GPU 2: [?, ?, C_sum, ?]")
        print(f"    GPU 3: [?, ?, ?, D_sum]")
        print()
        
        # é˜¶æ®µ2: AllGather
        print("é˜¶æ®µ2ï¼šAllGather (world_size-1 æ­¥)")
        for step in range(world_size - 1):
            print(f"  Step {step+1}:")
            for rank in range(world_size):
                send_to = (rank + 1) % world_size
                recv_from = (rank - 1) % world_size
                print(f"    GPU {rank}: å‘é€å®Œæ•´chunkç»™GPU {send_to}")
        
        print(f"  æœ€ç»ˆï¼šæ‰€æœ‰GPUéƒ½æœ‰å®Œæ•´ç»“æœ")
        for rank in range(world_size):
            print(f"    GPU {rank}: [A_sum, B_sum, C_sum, D_sum]")
        
        # å¤æ‚åº¦åˆ†æ
        chunk_size = 1  # å•ä½
        total_data = world_size * chunk_size
        
        print(f"\né€šä¿¡å¤æ‚åº¦:")
        print(f"  ä¼ ç»ŸAllReduce: O(N * data_size)")
        print(f"  Ring-AllReduce: O(2 * (N-1) * data_size / N) â‰ˆ O(2 * data_size)")
        print(f"  ä¼˜åŠ¿ï¼šä¸GPUæ•°é‡æ— å…³ï¼")

explainer = RingAllReduceExplainer()
explainer.explain_algorithm()
```

---

### 8.3 é€šä¿¡ä¸è®¡ç®—é‡å 

```python
"""
é€šä¿¡ä¸è®¡ç®—é‡å ä¼˜åŒ–
"""

class CommunicationComputationOverlap:
    """
    é‡å é€šä¿¡ä¸è®¡ç®—ï¼Œæå‡æ•ˆç‡
    """
    
    def without_overlap(self, model, batch):
        """
        æ— é‡å ï¼šä¸²è¡Œæ‰§è¡Œ
        """
        # 1. è®¡ç®—æ¢¯åº¦ï¼ˆæ‰€æœ‰å±‚ï¼‰
        outputs = model(batch)
        loss = compute_loss(outputs)
        loss.backward()  # è®¡ç®—æ‰€æœ‰æ¢¯åº¦
        
        # 2. åŒæ­¥æ¢¯åº¦ï¼ˆAllReduceï¼‰
        for param in model.parameters():
            dist.all_reduce(param.grad)  # é˜»å¡ç­‰å¾…
        
        # 3. æ›´æ–°å‚æ•°
        optimizer.step()
        
        # é—®é¢˜ï¼šè®¡ç®—å’Œé€šä¿¡ä¸²è¡Œï¼ŒGPUç©ºé—²æ—¶é—´é•¿
    
    def with_overlap_ddp(self, model, batch):
        """
        DDPè‡ªåŠ¨é‡å 
        
        åŸç†ï¼šBackwardæ—¶è¾¹è®¡ç®—è¾¹é€šä¿¡
        """
        # DDPä¼šå°†æ¨¡å‹å‚æ•°åˆ†æˆå¤šä¸ªbucket
        # æ¯ä¸ªbucketçš„æ¢¯åº¦è®¡ç®—å®Œæˆåç«‹å³å¯åŠ¨AllReduce
        
        outputs = model(batch)  # modelæ˜¯DDPåŒ…è£…çš„
        loss = compute_loss(outputs)
        loss.backward()  # ğŸ”¥ DDPåœ¨backwardè¿‡ç¨‹ä¸­è‡ªåŠ¨é‡å é€šä¿¡ï¼
        
        # Backwardç»“æŸæ—¶ï¼Œæ¢¯åº¦å·²ç»åŒæ­¥å®Œæˆ
        optimizer.step()
        
        # æ•ˆæœï¼šé€šä¿¡æ—¶é—´è¢«è®¡ç®—æ—¶é—´æ©ç›–
    
    def visualize_overlap(self):
        """
        å¯è§†åŒ–é‡å æ•ˆæœ
        """
        print("é€šä¿¡ä¸è®¡ç®—é‡å :")
        print("="*70)
        
        print("\nâŒ æ— é‡å ï¼ˆä¸²è¡Œï¼‰:")
        print("  GPU: [è®¡ç®—Layer1] [è®¡ç®—Layer2] [è®¡ç®—Layer3]          [ç©ºé—²...]")
        print("  ç½‘ç»œ:                                     [AllReduce...]")
        print("  æ€»æ—¶é—´: è®¡ç®—æ—¶é—´ + é€šä¿¡æ—¶é—´")
        
        print("\nâœ… æœ‰é‡å ï¼ˆDDPï¼‰:")
        print("  GPU: [è®¡ç®—Layer1] [è®¡ç®—Layer2] [è®¡ç®—Layer3]")
        print("  ç½‘ç»œ:         [AllReduce1][AllReduce2][AllReduce3]")
        print("  æ€»æ—¶é—´: max(è®¡ç®—æ—¶é—´, é€šä¿¡æ—¶é—´)")
        print()
        print("  æ•ˆæœï¼šé€šä¿¡è¢«è®¡ç®—æ©ç›–ï¼Œå‡ ä¹æ— é¢å¤–å¼€é”€ï¼")

overlap_demo = CommunicationComputationOverlap()
overlap_demo.visualize_overlap()
```

---

<a name="gradient-accumulation"></a>
## ğŸ”‹ 9. æ¢¯åº¦ç´¯ç§¯ä¸æ··åˆç²¾åº¦

### 9.1 æ¢¯åº¦ç´¯ç§¯åŸç†

```python
"""
æ¢¯åº¦ç´¯ç§¯ï¼šæ¨¡æ‹Ÿå¤§batchè®­ç»ƒ
"""

class GradientAccumulation:
    """
    æ¢¯åº¦ç´¯ç§¯
    
    ç”¨é€”ï¼šæ˜¾å­˜ä¸è¶³æ—¶æ¨¡æ‹Ÿå¤§batch
    """
    
    def train_without_accumulation(self, model, dataloader, batch_size=32):
        """
        æ— æ¢¯åº¦ç´¯ç§¯ï¼šæ ‡å‡†è®­ç»ƒ
        """
        for batch in dataloader:
            outputs = model(batch)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        
        # æ¯ä¸ªbatchæ›´æ–°ä¸€æ¬¡å‚æ•°
        # æœ‰æ•ˆbatch_size = 32
    
    def train_with_accumulation(
        self, 
        model, 
        dataloader, 
        micro_batch_size=4, 
        accumulation_steps=8
    ):
        """
        æ¢¯åº¦ç´¯ç§¯ï¼šç´¯ç§¯å¤šä¸ªmicrobatchçš„æ¢¯åº¦
        
        æ•ˆæœï¼šæœ‰æ•ˆbatch_size = 4 * 8 = 32
        """
        optimizer.zero_grad()
        
        for step, batch in enumerate(dataloader):
            # Forward & Backward
            outputs = model(batch)
            loss = criterion(outputs, labels)
            
            # ğŸ”¥ é‡è¦ï¼šlosséœ€è¦é™¤ä»¥accumulation_steps
            loss = loss / accumulation_steps
            loss.backward()  # æ¢¯åº¦ç´¯ç§¯ï¼Œä¸æ¸…é›¶
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
            if (step + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
        
        # æ˜¾å­˜å ç”¨ï¼šåªéœ€å­˜å‚¨micro_batch_sizeçš„æ¿€æ´»å€¼
        # æœ‰æ•ˆbatch_sizeï¼šmicro_batch_size * accumulation_steps
    
    def compare_memory(self):
        """
        æ˜¾å­˜å¯¹æ¯”
        """
        print("æ¢¯åº¦ç´¯ç§¯æ˜¾å­˜å¯¹æ¯”:")
        print("="*70)
        
        batch_size = 32
        micro_batch = 4
        accum_steps = 8
        
        # æ— ç´¯ç§¯
        memory_without = batch_size * seq_len * hidden_dim * num_layers * 2
        
        # æœ‰ç´¯ç§¯
        memory_with = micro_batch * seq_len * hidden_dim * num_layers * 2
        
        print(f"ç›®æ ‡æœ‰æ•ˆbatch size: {batch_size}")
        print(f"\næ–¹æ¡ˆ1ï¼šç›´æ¥è®­ç»ƒbatch_size={batch_size}")
        print(f"  æ¿€æ´»å€¼æ˜¾å­˜: ~{memory_without / 1024**3:.1f} GB")
        
        print(f"\næ–¹æ¡ˆ2ï¼šæ¢¯åº¦ç´¯ç§¯ (micro_batch={micro_batch}, steps={accum_steps})")
        print(f"  æ¿€æ´»å€¼æ˜¾å­˜: ~{memory_with / 1024**3:.1f} GB")
        print(f"  èŠ‚çœ: {(1 - memory_with/memory_without)*100:.0f}%")
        
        print(f"\nâš ï¸ æ³¨æ„ï¼šæ¢¯åº¦ç´¯ç§¯ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ï¼ˆ{accum_steps}x forward/backwardï¼‰")

accum = GradientAccumulation()
accum.compare_memory()
```

---

### 9.2 æ··åˆç²¾åº¦è®­ç»ƒ

```python
"""
æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰
"""

import torch
from torch.cuda.amp import autocast, GradScaler

class MixedPrecisionTraining:
    """
    æ··åˆç²¾åº¦è®­ç»ƒ
    
    FP16: èŠ‚çœæ˜¾å­˜ï¼ŒåŠ é€Ÿè®¡ç®—
    BF16: æ›´ç¨³å®šï¼ˆH100æ¨èï¼‰
    """
    
    def train_fp32_baseline(self, model, dataloader):
        """
        FP32è®­ç»ƒï¼ˆåŸºçº¿ï¼‰
        """
        model = model.cuda()
        optimizer = torch.optim.AdamW(model.parameters())
        
        for batch in dataloader:
            inputs = batch['inputs'].cuda()
            labels = batch['labels'].cuda()
            
            # Forward (FP32)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Backward (FP32)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
    
    def train_fp16_automatic(self, model, dataloader):
        """
        FP16è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆPyTorch AMPï¼‰
        """
        model = model.cuda()
        optimizer = torch.optim.AdamW(model.parameters())
        
        # ğŸ”¥ GradScalerï¼šé˜²æ­¢FP16ä¸‹æº¢
        scaler = GradScaler()
        
        for batch in dataloader:
            inputs = batch['inputs'].cuda()
            labels = batch['labels'].cuda()
            
            # ğŸ”¥ autocastï¼šè‡ªåŠ¨FP16
            with autocast(dtype=torch.float16):
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            # Backwardï¼ˆå¸¦scaleï¼‰
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆunscaleåï¼‰
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # æ›´æ–°å‚æ•°
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
    
    def train_bf16(self, model, dataloader):
        """
        BF16æ··åˆç²¾åº¦ï¼ˆH100/A100æ¨èï¼‰
        
        ä¼˜åŠ¿ï¼šä¸éœ€è¦GradScalerï¼Œæ›´ç¨³å®š
        """
        model = model.cuda()
        optimizer = torch.optim.AdamW(model.parameters())
        
        for batch in dataloader:
            inputs = batch['inputs'].cuda()
            labels = batch['labels'].cuda()
            
            # ğŸ”¥ BF16ï¼šä¸éœ€è¦loss scaling
            with autocast(dtype=torch.bfloat16):
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            
            # Backwardï¼ˆæ— éœ€scalerï¼‰
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            optimizer.zero_grad()
    
    def compare_precision(self):
        """
        ç²¾åº¦å¯¹æ¯”
        """
        comparison = {
            'ç²¾åº¦': ['æ˜¾å­˜å ç”¨', 'è®¡ç®—é€Ÿåº¦', 'æ•°å€¼ç¨³å®šæ€§', 'é€‚ç”¨GPU'],
            
            'FP32': ['100%', '1.0x', 'æœ€å¥½', 'æ‰€æœ‰'],
            'FP16': ['50%', '2-3x', 'ä¸€èˆ¬ï¼ˆéœ€loss scalingï¼‰', 'V100+'],
            'BF16': ['50%', '2-3x', 'å¥½', 'A100/H100']
        }
        
        import pandas as pd
        df = pd.DataFrame(comparison).set_index('ç²¾åº¦')
        print(df.to_string())

mixed_precision = MixedPrecisionTraining()
mixed_precision.compare_precision()
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. **Megatron-LM**  
   [Megatron-LM: Training Multi-Billion Parameter Language Models](https://arxiv.org/abs/1909.08053)  
   NVIDIA, 2019

2. **DeepSpeed ZeRO**  
   [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)  
   Microsoft, 2020

3. **PyTorch FSDP**  
   [PyTorch FSDP Documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)  
   2025æœ€æ–°

4. **3D Parallelism**  
   [TorchTitan: PyTorch Native Solution for LLM Pre-training](https://arxiv.org/html/2410.06511v3)  
   2025

5. **Pipeline Parallelism**  
   [GPipe: Efficient Training of Giant Neural Networks](https://arxiv.org/abs/1811.06965)  
   Google, 2019

### å·¥ç¨‹å®è·µ

- [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)  
  å®˜æ–¹åˆ†å¸ƒå¼è®­ç»ƒæ•™ç¨‹

- [DeepSpeed Documentation](https://www.deepspeed.ai/)  
  ZeROä¼˜åŒ–å®Œæ•´æ–‡æ¡£

- [NVIDIA Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM)  
  Megatron-LMæºç 

- [NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/)  
  NCCLé€šä¿¡åº“

---

## ğŸ¯ æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç­–ç•¥**ï¼š
   - 7Bæ¨¡å‹ï¼šDDP
   - 13B-70Bï¼šFSDP ZeRO-3
   - 70B+ï¼š3Då¹¶è¡Œ (TP+PP+DP)

2. **é€šä¿¡ä¼˜åŒ–æ˜¯å…³é”®**ï¼š
   - DDPï¼šé€šä¿¡ä¸è®¡ç®—é‡å 
   - FSDPï¼šé¢„å–å‚æ•°
   - Megatronï¼šä¼˜åŒ–TP/PPé€šä¿¡

3. **æ˜¾å­˜ä¼˜åŒ–ç»„åˆæ‹³**ï¼š
   - æ¢¯åº¦ç´¯ç§¯
   - æ··åˆç²¾åº¦ï¼ˆBF16æ¨èï¼‰
   - Activation Checkpointing
   - ZeRO Stage 3

4. **æ•…éšœæ¢å¤å¿…ä¸å¯å°‘**ï¼š
   - å®šæœŸCheckpoint
   - Fault-tolerant training
   - çŠ¶æ€æŒä¹…åŒ–

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ”— **ä¸‹ä¸€ç¯‡**ï¼š[11 - MLOpsæœ€ä½³å®è·µï¼šå®éªŒè¿½è¸ªã€ç‰ˆæœ¬æ§åˆ¶ã€CI/CD](./11-mlops-practices.md)
- ğŸ’» **åŠ¨æ‰‹å®è·µ**ï¼šåœ¨8å¡æœºå™¨ä¸Šå¯¹æ¯”DDP vs FSDP
- ğŸ“Š **æ€§èƒ½åˆ†æ**ï¼šç”¨PyTorch Profileråˆ†æé€šä¿¡ç“¶é¢ˆ

---

> ğŸ’¡ **ç’‡ç‘çš„å°è´´å£«**ï¼šåˆ†å¸ƒå¼è®­ç»ƒå°±åƒå›¢é˜Ÿåä½œâ€”â€”æ²Ÿé€šï¼ˆé€šä¿¡ï¼‰æˆæœ¬æ˜¯å…³é”®ï¼å¥½çš„å¹¶è¡Œç­–ç•¥èƒ½è®©GPUä»¬é«˜æ•ˆé…åˆï¼Œå°±åƒæ•æ·å›¢é˜Ÿçš„ç«™ä¼šä¸€æ ·ç®€çŸ­æœ‰æ•ˆ~ âœ¨
>
> é“å‹ç°åœ¨å¯¹åˆ†å¸ƒå¼è®­ç»ƒæœ‰æ„Ÿè§‰äº†å—ï¼Ÿä¸‹ä¸€ç¯‡æˆ‘ä»¬èŠMLOpsï¼Œæ•™ä½ å¦‚ä½•ç®¡ç†è®­ç»ƒå®éªŒã€ç‰ˆæœ¬æ§åˆ¶å’ŒCI/CDï¼ğŸš€
