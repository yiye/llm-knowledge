# 13 - ç”Ÿäº§åŒ–éƒ¨ç½²æŒ‡å—ï¼šä»è®­ç»ƒåˆ°æœåŠ¡çš„å®Œæ•´æµç¨‹

> ğŸ¯ **æ ¸å¿ƒè§‚ç‚¹**ï¼šè®­ç»ƒå‡ºå¥½æ¨¡å‹åªæ˜¯ç¬¬ä¸€æ­¥ï¼Œç”Ÿäº§åŒ–éƒ¨ç½²æ‰æ˜¯çœŸæ­£çš„è€ƒéªŒã€‚æœ¬æ–‡æ·±å…¥è®²è§£FastAPI/Triton/TorchServeç­‰æœåŠ¡åŒ–æ¡†æ¶ï¼ŒKubernetes HPAè‡ªåŠ¨æ‰©å±•ï¼ŒåŠ¨æ€batchingä¼˜åŒ–ï¼Œæ¨¡å‹çƒ­æ›´æ–°ï¼ŒAPIå®‰å…¨è®¤è¯ï¼Œé™æµç†”æ–­æœºåˆ¶ï¼Œåˆ†å¸ƒå¼è¿½è¸ªï¼Œä»¥åŠé«˜å¯ç”¨æ¶æ„è®¾è®¡ï¼Œæä¾›å®Œæ•´çš„ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç”Ÿäº§åŒ–éƒ¨ç½²å…¨æ™¯](#deployment-overview)
2. [æ¨¡å‹æœåŠ¡åŒ–æ¡†æ¶é€‰æ‹©](#serving-frameworks)
3. [FastAPIè½»é‡çº§éƒ¨ç½²](#fastapi-deployment)
4. [Triton Inference Serverç”Ÿäº§çº§éƒ¨ç½²](#triton-deployment)
5. [è´Ÿè½½å‡è¡¡ä¸è‡ªåŠ¨æ‰©å±•](#load-balancing-autoscaling)
6. [æ¨ç†ä¼˜åŒ–ï¼šåŠ¨æ€Batching](#dynamic-batching)
7. [æ¨¡å‹çƒ­æ›´æ–°ä¸ç‰ˆæœ¬ç®¡ç†](#model-updates)
8. [APIå®‰å…¨ä¸è®¤è¯](#security-auth)
9. [é™æµä¸ç†”æ–­æœºåˆ¶](#rate-limiting-circuit-breaker)
10. [æ—¥å¿—ä¸åˆ†å¸ƒå¼è¿½è¸ª](#logging-tracing)
11. [ç¾å¤‡ä¸é«˜å¯ç”¨](#disaster-recovery)
12. [æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–](#cost-monitoring)
13. [å®Œæ•´éƒ¨ç½²æ¡ˆä¾‹](#complete-deployment)

---

<a name="deployment-overview"></a>
## ğŸ—ï¸ 1. ç”Ÿäº§åŒ–éƒ¨ç½²å…¨æ™¯

### 1.1 ä»è®­ç»ƒåˆ°æœåŠ¡çš„å…¨æµç¨‹

```python
"""
LLMç”Ÿäº§åŒ–éƒ¨ç½²æµç¨‹å›¾
"""

class ProductionDeploymentFlow:
    """
    å®Œæ•´éƒ¨ç½²æµç¨‹
    """
    
    def print_flow(self):
        """
        æ‰“å°éƒ¨ç½²æµç¨‹
        """
        print("""
LLMç”Ÿäº§åŒ–éƒ¨ç½²æµç¨‹:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ¨¡å‹è®­ç»ƒ                                                  â”‚
â”‚    â””â”€ åˆ†å¸ƒå¼è®­ç»ƒ â†’ Checkpoint â†’ MLflow Registry              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. æ¨¡å‹ä¼˜åŒ–                                                  â”‚
â”‚    â”œâ”€ é‡åŒ– (FP8/INT4)                                        â”‚
â”‚    â”œâ”€ æ¨¡å‹å‰ªæ                                               â”‚
â”‚    â””â”€ å¯¼å‡ºä¸ºONNX/TensorRT                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æœåŠ¡åŒ–å°è£…                                                â”‚
â”‚    â”œâ”€ é€‰æ‹©æ¡†æ¶ (FastAPI/Triton/TorchServe)                  â”‚
â”‚    â”œâ”€ æ·»åŠ é¢„å¤„ç†/åå¤„ç†                                      â”‚
â”‚    â””â”€ å®šä¹‰APIæ¥å£                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å®¹å™¨åŒ–                                                    â”‚
â”‚    â””â”€ Dockeré•œåƒ â†’ å®¹å™¨æ³¨å†Œè¡¨ (ECR/GCR)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. ç¼–æ’éƒ¨ç½² (Kubernetes)                                     â”‚
â”‚    â”œâ”€ Deployment (å¤šå‰¯æœ¬)                                    â”‚
â”‚    â”œâ”€ Service (è´Ÿè½½å‡è¡¡)                                     â”‚
â”‚    â”œâ”€ HPA (è‡ªåŠ¨æ‰©å±•)                                         â”‚
â”‚    â””â”€ Ingress (å¤–éƒ¨è®¿é—®)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. å¯è§‚æµ‹æ€§                                                  â”‚
â”‚    â”œâ”€ æ—¥å¿— (ELK/Loki)                                        â”‚
â”‚    â”œâ”€ æŒ‡æ ‡ (Prometheus)                                      â”‚
â”‚    â”œâ”€ è¿½è¸ª (Jaeger)                                          â”‚
â”‚    â””â”€ å‘Šè­¦ (AlertManager)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. å®‰å…¨ä¸æ²»ç†                                                â”‚
â”‚    â”œâ”€ APIè®¤è¯ (OAuth/JWT)                                    â”‚
â”‚    â”œâ”€ é™æµ (Rate Limiting)                                   â”‚
â”‚    â”œâ”€ ç†”æ–­ (Circuit Breaker)                                 â”‚
â”‚    â””â”€ å®¡è®¡æ—¥å¿—                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. æŒç»­ä¼˜åŒ–                                                  â”‚
â”‚    â”œâ”€ A/Bæµ‹è¯•                                                â”‚
â”‚    â”œâ”€ æ€§èƒ½ç›‘æ§                                               â”‚
â”‚    â”œâ”€ æˆæœ¬åˆ†æ                                               â”‚
â”‚    â””â”€ æ¨¡å‹è¿­ä»£                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)

flow = ProductionDeploymentFlow()
flow.print_flow()
```

---

<a name="serving-frameworks"></a>
## ğŸ¯ 2. æ¨¡å‹æœåŠ¡åŒ–æ¡†æ¶é€‰æ‹©

### 2.1 æ¡†æ¶å¯¹æ¯”ï¼ˆ2025ï¼‰

æ ¹æ® [2025å¹´å¯¹æ¯”åˆ†æ](https://medium.com/@hemanthodarwinr/nvidia-triton-vs-fastapi-choosing-the-right-ml-serving-solution-in-2024-3e6c771f3cf6)ï¼š

| æ¡†æ¶ | å¼€å‘è€… | è¯­è¨€ | æ€§èƒ½ | æ˜“ç”¨æ€§ | å¤šæ¡†æ¶æ”¯æŒ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|------|--------|-----------|----------|
| **FastAPI** | SebastiÃ¡n RamÃ­rez | Python | â­â­â­ | â­â­â­â­â­ | éœ€è‡ªå·±å®ç° | åŸå‹ã€è½»é‡æœåŠ¡ |
| **Triton** ğŸ”¥ | NVIDIA | C++/Python | â­â­â­â­â­ | â­â­â­ | âœ… PyTorch/TF/ONNX | ç”Ÿäº§ç¯å¢ƒ |
| **TorchServe** | AWS/Meta | Java/Python | â­â­â­â­ | â­â­â­â­ | PyTorchä¸“ç”¨ | PyTorchç”Ÿäº§ |
| **Ray Serve** | Anyscale | Python | â­â­â­â­ | â­â­â­â­ | é€šç”¨ | å¤æ‚workflow |

**æ€§èƒ½æ•°æ®**ï¼ˆNVIDIA Tritonï¼Œ2025ï¼‰ï¼š
- **ResNet-50**: ~2mså»¶è¿Ÿ @ 1000 QPS (V100)
- **BERT**: ~15msç«¯åˆ°ç«¯å»¶è¿Ÿ
- **LLaMA-2-7B**: ~45ms TTFT @ 32 batch (H100)

---

### 2.2 é€‰æ‹©å†³ç­–æ ‘

```python
"""
æœåŠ¡æ¡†æ¶é€‰æ‹©å†³ç­–æ ‘
"""

class ServingFrameworkSelector:
    """
    æ ¹æ®éœ€æ±‚é€‰æ‹©æœåŠ¡æ¡†æ¶
    """
    
    def recommend_framework(self, requirements):
        """
        æ¨èæœåŠ¡æ¡†æ¶
        
        å‚æ•°:
            requirements: dictåŒ…å«ä»¥ä¸‹å­—æ®µ
                - qps: ç›®æ ‡QPS
                - latency_requirement: 'low', 'medium', 'high'
                - model_framework: 'pytorch', 'tensorflow', 'mixed'
                - team_expertise: 'python', 'java', 'mixed'
                - budget: 'low', 'medium', 'high'
        """
        print("æœåŠ¡æ¡†æ¶æ¨è:")
        print("="*70)
        print(f"éœ€æ±‚åˆ†æ:")
        for key, value in requirements.items():
            print(f"  {key}: {value}")
        print()
        
        # å†³ç­–é€»è¾‘
        if requirements['qps'] < 10 and requirements['budget'] == 'low':
            recommendation = "FastAPI"
            reason = "ä½QPSï¼Œå¿«é€ŸåŸå‹ï¼ŒPythonå‹å¥½"
        
        elif requirements['qps'] > 1000 or requirements['latency_requirement'] == 'low':
            recommendation = "NVIDIA Triton"
            reason = "é«˜QPS/ä½å»¶è¿Ÿéœ€æ±‚ï¼Œç”Ÿäº§çº§æ€§èƒ½"
        
        elif requirements['model_framework'] == 'pytorch':
            recommendation = "TorchServe"
            reason = "PyTorchç”Ÿæ€ï¼ŒAWSå®˜æ–¹æ”¯æŒ"
        
        elif requirements['team_expertise'] == 'python':
            recommendation = "FastAPI æˆ– Ray Serve"
            reason = "Pythonå›¢é˜Ÿï¼Œå¼€å‘æ•ˆç‡é«˜"
        
        else:
            recommendation = "NVIDIA Triton"
            reason = "ç”Ÿäº§ç¯å¢ƒç»¼åˆæœ€ä¼˜"
        
        print(f"æ¨è: {recommendation}")
        print(f"ç†ç”±: {reason}")
        
        return recommendation

# ä½¿ç”¨ç¤ºä¾‹
selector = ServingFrameworkSelector()

# åœºæ™¯1ï¼šåˆ›ä¸šå…¬å¸åŸå‹
selector.recommend_framework({
    'qps': 5,
    'latency_requirement': 'medium',
    'model_framework': 'pytorch',
    'team_expertise': 'python',
    'budget': 'low'
})

# åœºæ™¯2ï¼šå¤§å…¬å¸ç”Ÿäº§ç¯å¢ƒ
selector.recommend_framework({
    'qps': 5000,
    'latency_requirement': 'low',
    'model_framework': 'mixed',
    'team_expertise': 'mixed',
    'budget': 'high'
})
```

---

<a name="fastapi-deployment"></a>
## ğŸš€ 3. FastAPIè½»é‡çº§éƒ¨ç½²

### 3.1 FastAPIå®Œæ•´æœåŠ¡

```python
"""
FastAPI + vLLMæ¨¡å‹æœåŠ¡
"""

from fastapi import FastAPI, HTTPException, Header
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List
import torch
import time
import asyncio
from vllm import LLM, SamplingParams

# ============================================
# 1. åˆå§‹åŒ–FastAPIåº”ç”¨
# ============================================

app = FastAPI(
    title="LLM Inference API",
    description="Production-ready LLM inference service",
    version="1.0.0"
)

# åŠ è½½æ¨¡å‹ï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼‰
print("Loading model...")
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    dtype="float16"
)
print("âœ… Model loaded")

# ============================================
# 2. è¯·æ±‚/å“åº”æ¨¡å‹
# ============================================

class GenerationRequest(BaseModel):
    """ç”Ÿæˆè¯·æ±‚"""
    prompt: str = Field(..., description="è¾“å…¥æç¤ºè¯")
    max_tokens: int = Field(256, ge=1, le=2048, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    temperature: float = Field(0.7, ge=0.0, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    top_p: float = Field(0.9, ge=0.0, le=1.0, description="Nucleus sampling")
    stream: bool = Field(False, description="æ˜¯å¦æµå¼è¿”å›")

class GenerationResponse(BaseModel):
    """ç”Ÿæˆå“åº”"""
    generated_text: str
    prompt: str
    finish_reason: str  # 'stop', 'length', 'error'
    tokens_generated: int
    latency_ms: float

# ============================================
# 3. APIç«¯ç‚¹
# ============================================

@app.post("/v1/generate", response_model=GenerationResponse)
async def generate(
    request: GenerationRequest,
    api_key: Optional[str] = Header(None, alias="X-API-Key")
):
    """
    æ–‡æœ¬ç”Ÿæˆç«¯ç‚¹
    """
    # API KeyéªŒè¯
    if not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    try:
        start_time = time.time()
        
        # é…ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
        )
        
        # ç”Ÿæˆ
        outputs = llm.generate([request.prompt], sampling_params)
        
        # æå–ç»“æœ
        generated_text = outputs[0].outputs[0].text
        finish_reason = outputs[0].outputs[0].finish_reason
        tokens_generated = len(outputs[0].outputs[0].token_ids)
        
        latency_ms = (time.time() - start_time) * 1000
        
        return GenerationResponse(
            generated_text=generated_text,
            prompt=request.prompt,
            finish_reason=finish_reason,
            tokens_generated=tokens_generated,
            latency_ms=latency_ms
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/generate/stream")
async def generate_stream(
    request: GenerationRequest,
    api_key: Optional[str] = Header(None, alias="X-API-Key")
):
    """
    æµå¼ç”Ÿæˆç«¯ç‚¹
    """
    if not verify_api_key(api_key):
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    async def generate_tokens():
        """æµå¼ç”Ÿæˆtoken"""
        sampling_params = SamplingParams(
            temperature=request.temperature,
            top_p=request.top_p,
            max_tokens=request.max_tokens,
        )
        
        # vLLMæµå¼ç”Ÿæˆï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        # ç®€åŒ–ç¤ºä¾‹ï¼šé€tokenè¿”å›
        outputs = llm.generate([request.prompt], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        # æ¨¡æ‹Ÿæµå¼è¿”å›
        for token in generated_text.split():
            yield f"data: {token}\n\n"
            await asyncio.sleep(0.01)
        
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(generate_tokens(), media_type="text/event-stream")

# ============================================
# 4. å¥åº·æ£€æŸ¥
# ============================================

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        # æµ‹è¯•æ¨¡å‹æ¨ç†
        test_output = llm.generate(["test"], SamplingParams(max_tokens=1))
        
        return {
            "status": "healthy",
            "model": "llama-2-7b-chat",
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count()
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")

@app.get("/metrics")
async def metrics():
    """Prometheus metrics"""
    from prometheus_client import generate_latest
    return generate_latest()

# ============================================
# 5. API KeyéªŒè¯
# ============================================

API_KEYS = {
    "sk-test-key-1": {"user": "alice", "rate_limit": 1000},
    "sk-test-key-2": {"user": "bob", "rate_limit": 100}
}

def verify_api_key(api_key: str) -> bool:
    """éªŒè¯API Key"""
    return api_key in API_KEYS

# ============================================
# 6. å¯åŠ¨æœåŠ¡
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1,  # FastAPI + vLLMåªç”¨1ä¸ªworker
        log_level="info"
    )
```

---

### 3.2 DockeråŒ–éƒ¨ç½²

```dockerfile
# Dockerfile for FastAPI + vLLM

FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# å®‰è£…Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# requirements.txtå†…å®¹ï¼š
# fastapi==0.109.0
# uvicorn[standard]==0.27.0
# vllm==0.3.0
# prometheus-client==0.19.0
# pydantic==2.5.0

# å¤åˆ¶ä»£ç 
WORKDIR /app
COPY app.py .
COPY models/ ./models/

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "app.py"]
```

```bash
# æ„å»ºä¸è¿è¡Œ

# 1. æ„å»ºDockeré•œåƒ
docker build -t llm-service:v1.0.0 .

# 2. è¿è¡Œå®¹å™¨
docker run -d \
    --name llm-service \
    --gpus all \
    -p 8000:8000 \
    -e MODEL_NAME=llama-2-7b-chat \
    -v $(pwd)/models:/app/models \
    llm-service:v1.0.0

# 3. æµ‹è¯•
curl -X POST http://localhost:8000/v1/generate \
    -H "Content-Type: application/json" \
    -H "X-API-Key: sk-test-key-1" \
    -d '{
        "prompt": "Explain quantum computing:",
        "max_tokens": 100,
        "temperature": 0.7
    }'

# 4. æŸ¥çœ‹æ—¥å¿—
docker logs -f llm-service

# 5. æŸ¥çœ‹GPUä½¿ç”¨
docker exec llm-service nvidia-smi
```

---

<a name="triton-deployment"></a>
## ğŸ­ 4. Triton Inference Serverç”Ÿäº§çº§éƒ¨ç½²

### 4.1 Triton Model Repository

```python
"""
Tritonæ¨¡å‹ä»“åº“ç»“æ„
"""

# ç›®å½•ç»“æ„
"""
model_repository/
â”œâ”€â”€ llama2-7b-chat/
â”‚   â”œâ”€â”€ config.pbtxt          # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ 1/                     # ç‰ˆæœ¬1
â”‚       â”œâ”€â”€ model.plan         # TensorRTå¼•æ“
â”‚       â””â”€â”€ config.json        # æ¨¡å‹å…ƒæ•°æ®
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ config.pbtxt
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.py           # Pythoné¢„å¤„ç†é€»è¾‘
â””â”€â”€ ensemble_model/
    â””â”€â”€ config.pbtxt           # ç»„åˆæ¨¡å‹é…ç½®
"""

# config.pbtxt for LLaMA model
llama_config = """
name: "llama2-7b-chat"
backend: "tensorrtllm"
max_batch_size: 256

# è¾“å…¥
input [
  {
    name: "input_ids"
    data_type: TYPE_INT32
    dims: [-1]
  },
  {
    name: "input_lengths"
    data_type: TYPE_INT32
    dims: [1]
    reshape: { shape: [] }
  }
]

# è¾“å‡º
output [
  {
    name: "output_ids"
    data_type: TYPE_INT32
    dims: [-1, -1]
  },
  {
    name: "sequence_length"
    data_type: TYPE_INT32
    dims: [-1]
  }
]

# å®ä¾‹ç»„ï¼ˆå¤šGPUï¼‰
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  }
]

# ğŸ”¥ åŠ¨æ€Batchingé…ç½®
dynamic_batching {
  preferred_batch_size: [16, 32, 64, 128]
  max_queue_delay_microseconds: 5000  # æœ€é•¿ç­‰å¾…5ms
  
  # ä¼˜å…ˆçº§é˜Ÿåˆ—
  priority_levels: 2
  default_priority_level: 1
  
  # Batchingç­–ç•¥
  preserve_ordering: false  # ä¸ä¿è¯é¡ºåºï¼ˆæé«˜ååï¼‰
}

# æ¨¡å‹çƒ­èº«
model_warmup [
  {
    name: "warmup_sample_1"
    batch_size: 1
    inputs: {
      key: "input_ids"
      value: {
        data_type: TYPE_INT32
        dims: [10]
        zero_data: true
      }
    }
  }
]

# ä¼˜åŒ–é…ç½®
optimization {
  cuda {
    graphs: true              # å¯ç”¨CUDA Graph
    busy_wait_events: true
  }
  
  graph_optimization_level: 1
}
"""

with open('model_repository/llama2-7b-chat/config.pbtxt', 'w') as f:
    f.write(llama_config)

print("âœ… Tritonæ¨¡å‹é…ç½®å·²ç”Ÿæˆ")
```

---

### 4.2 Triton Serveréƒ¨ç½²

```bash
# ============================================
# Triton Inference Serveréƒ¨ç½²
# ============================================

# 1. æ‹‰å–Triton Dockeré•œåƒ
docker pull nvcr.io/nvidia/tritonserver:25.06-trtllm-python-py3

# 2. å¯åŠ¨TritonæœåŠ¡å™¨
docker run -d \
    --gpus all \
    --shm-size=8g \  # å…±äº«å†…å­˜ï¼ˆé‡è¦ï¼ï¼‰
    -p 8000:8000 \   # HTTP
    -p 8001:8001 \   # gRPC
    -p 8002:8002 \   # Metrics
    -v $(pwd)/model_repository:/models \
    nvcr.io/nvidia/tritonserver:25.06-trtllm-python-py3 \
    tritonserver \
        --model-repository=/models \
        --strict-model-config=false \
        --log-verbose=1

# 3. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
curl http://localhost:8000/v2/health/ready

# è¾“å‡ºï¼š
# {
#   "ready": true
# }

# 4. æŸ¥çœ‹å·²åŠ è½½æ¨¡å‹
curl http://localhost:8000/v2/models

# 5. è·å–æ¨¡å‹é…ç½®
curl http://localhost:8000/v2/models/llama2-7b-chat/config

# ============================================
# Tritonå®¢æˆ·ç«¯è°ƒç”¨
# ============================================

# Pythonå®¢æˆ·ç«¯
import tritonclient.http as httpclient
import numpy as np

# åˆ›å»ºå®¢æˆ·ç«¯
client = httpclient.InferenceServerClient(url="localhost:8000")

# å‡†å¤‡è¾“å…¥
prompt = "Explain quantum computing:"
input_ids = tokenizer.encode(prompt)

inputs = [
    httpclient.InferInput("input_ids", [1, len(input_ids)], "INT32"),
    httpclient.InferInput("input_lengths", [1], "INT32")
]

inputs[0].set_data_from_numpy(np.array([input_ids], dtype=np.int32))
inputs[1].set_data_from_numpy(np.array([len(input_ids)], dtype=np.int32))

# æ¨ç†
outputs = [
    httpclient.InferRequestedOutput("output_ids"),
    httpclient.InferRequestedOutput("sequence_length")
]

response = client.infer(
    model_name="llama2-7b-chat",
    inputs=inputs,
    outputs=outputs
)

# è§£æè¾“å‡º
output_ids = response.as_numpy("output_ids")
generated_text = tokenizer.decode(output_ids[0])

print(f"Generated: {generated_text}")
```

---

### 4.3 Triton Ensemble Model

```python
"""
Triton Ensemble: ç»„åˆé¢„å¤„ç†+æ¨ç†+åå¤„ç†
"""

# ensemble_config.pbtxt
ensemble_config = """
name: "ensemble_llama2"
platform: "ensemble"
max_batch_size: 256

input [
  {
    name: "text_input"
    data_type: TYPE_STRING
    dims: [1]
  }
]

output [
  {
    name: "text_output"
    data_type: TYPE_STRING
    dims: [1]
  }
]

# ğŸ”¥ Ensembleæ­¥éª¤
ensemble_scheduling {
  step [
    {
      model_name: "preprocessing"
      model_version: -1
      input_map {
        key: "text_input"
        value: "text_input"
      }
      output_map {
        key: "input_ids"
        value: "tokenized_input"
      }
    },
    {
      model_name: "llama2-7b-chat"
      model_version: -1
      input_map {
        key: "input_ids"
        value: "tokenized_input"
      }
      output_map {
        key: "output_ids"
        value: "model_output"
      }
    },
    {
      model_name: "postprocessing"
      model_version: -1
      input_map {
        key: "output_ids"
        value: "model_output"
      }
      output_map {
        key: "text_output"
        value: "text_output"
      }
    }
  ]
}
"""

# ä¼˜åŠ¿ï¼š
# âœ… ç”¨æˆ·åªéœ€å‘é€åŸå§‹æ–‡æœ¬
# âœ… Tritonå†…éƒ¨å¤„ç†é¢„å¤„ç†+æ¨ç†+åå¤„ç†
# âœ… å‡å°‘ç½‘ç»œå¾€è¿”
# âœ… ç»Ÿä¸€çš„æ€§èƒ½ä¼˜åŒ–
```

---

<a name="load-balancing-autoscaling"></a>
## âš–ï¸ 5. è´Ÿè½½å‡è¡¡ä¸è‡ªåŠ¨æ‰©å±•

### 5.1 Kubernetes Deployment + HPA

```yaml
# ============================================
# llm-deployment.yaml
# Kuberneteséƒ¨ç½²é…ç½®
# ============================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference
  labels:
    app: llm-service
    version: v1.0.0
spec:
  replicas: 3  # åˆå§‹å‰¯æœ¬æ•°
  
  selector:
    matchLabels:
      app: llm-service
  
  template:
    metadata:
      labels:
        app: llm-service
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    
    spec:
      # èŠ‚ç‚¹é€‰æ‹©
      nodeSelector:
        gpu-type: "a100"
      
      # å®¹å¿
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      
      containers:
        - name: llm-container
          image: gcr.io/my-project/llm-service:v1.0.0
          imagePullPolicy: IfNotPresent
          
          # èµ„æºè¯·æ±‚
          resources:
            requests:
              nvidia.com/gpu: 1
              cpu: "8"
              memory: "32Gi"
            limits:
              nvidia.com/gpu: 1
              cpu: "16"
              memory: "48Gi"
          
          # ç¯å¢ƒå˜é‡
          env:
            - name: MODEL_NAME
              value: "llama-2-7b-chat"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            - name: NCCL_DEBUG
              value: "WARN"
          
          # ç«¯å£
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          
          # æ¢é’ˆ
          # å­˜æ´»æ¢é’ˆï¼ˆLiveness Probeï¼‰
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          
          # å°±ç»ªæ¢é’ˆï¼ˆReadiness Probeï¼‰
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          # å¯åŠ¨æ¢é’ˆï¼ˆStartup Probeï¼‰
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30  # æœ€å¤šç­‰å¾…300ç§’å¯åŠ¨
          
          # å·æŒ‚è½½
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
      
      # å·
      volumes:
        - name: model-cache
          emptyDir: {}
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "8Gi"

---
# ============================================
# Serviceé…ç½®
# ============================================

apiVersion: v1
kind: Service
metadata:
  name: llm-service
  labels:
    app: llm-service
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8000
      protocol: TCP
      name: http
  selector:
    app: llm-service
  
  # ğŸ”¥ ä¼šè¯äº²å’Œæ€§ï¼ˆåŒä¸€å®¢æˆ·ç«¯è·¯ç”±åˆ°åŒä¸€Podï¼‰
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600

---
# ============================================
# HPA (Horizontal Pod Autoscaler)é…ç½®
# ============================================

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference
  
  minReplicas: 2   # æœ€å°å‰¯æœ¬
  maxReplicas: 10  # æœ€å¤§å‰¯æœ¬
  
  # ğŸ”¥ åŸºäºå¤šä¸ªæŒ‡æ ‡æ‰©å±•
  metrics:
    # 1. åŸºäºè¯·æ±‚é˜Ÿåˆ—é•¿åº¦ï¼ˆæ¨èï¼‰
    - type: Pods
      pods:
        metric:
          name: inference_queue_length
        target:
          type: AverageValue
          averageValue: "10"  # é˜Ÿåˆ—é•¿åº¦>10æ—¶æ‰©å±•
    
    # 2. åŸºäºGPUåˆ©ç”¨ç‡
    - type: Pods
      pods:
        metric:
          name: gpu_utilization_percent
        target:
          type: AverageValue
          averageValue: "80"  # GPUåˆ©ç”¨ç‡>80%æ—¶æ‰©å±•
    
    # 3. åŸºäºè¯·æ±‚å»¶è¿Ÿ
    - type: Pods
      pods:
        metric:
          name: inference_latency_p95_seconds
        target:
          type: AverageValue
          averageValue: "2"  # P95å»¶è¿Ÿ>2sæ—¶æ‰©å±•
  
  # æ‰©å±•è¡Œä¸º
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¼©å®¹ç¨³å®šæœŸ5åˆ†é’Ÿ
      policies:
        - type: Percent
          value: 50  # æ¯æ¬¡æœ€å¤šç¼©å®¹50%
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # ç«‹å³æ‰©å®¹
      policies:
        - type: Percent
          value: 100  # æ¯æ¬¡æœ€å¤šæ‰©å®¹100%ï¼ˆç¿»å€ï¼‰
          periodSeconds: 30
        - type: Pods
          value: 2  # æ¯æ¬¡æœ€å°‘æ‰©å®¹2ä¸ªPod
          periodSeconds: 30
      selectPolicy: Max  # ä½¿ç”¨æœ€æ¿€è¿›çš„ç­–ç•¥
```

---

### 5.2 è‡ªå®šä¹‰Metrics Server

```python
"""
è‡ªå®šä¹‰HPA Metricsï¼ˆé˜Ÿåˆ—é•¿åº¦ï¼‰
"""

from prometheus_client import Gauge
import queue

# å®šä¹‰metric
inference_queue_length = Gauge(
    'inference_queue_length',
    'Number of requests in the inference queue'
)

class InferenceQueue:
    """
    æ¨ç†è¯·æ±‚é˜Ÿåˆ—ï¼ˆç”¨äºbatchingå’ŒHPAï¼‰
    """
    
    def __init__(self, max_size=1000):
        self.queue = queue.Queue(maxsize=max_size)
    
    def add_request(self, request):
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        self.queue.put(request)
        
        # æ›´æ–°metric
        inference_queue_length.set(self.queue.qsize())
    
    def get_batch(self, max_batch_size=32, timeout=0.05):
        """
        ä»é˜Ÿåˆ—è·å–batch
        
        ç­–ç•¥ï¼š
        - ç­‰å¾…timeoutç§’æ”¶é›†è¯·æ±‚
        - æˆ–è¾¾åˆ°max_batch_sizeç«‹å³è¿”å›
        """
        batch = []
        deadline = time.time() + timeout
        
        while len(batch) < max_batch_size and time.time() < deadline:
            try:
                request = self.queue.get(timeout=0.01)
                batch.append(request)
            except queue.Empty:
                if batch:  # æœ‰éƒ¨åˆ†è¯·æ±‚ï¼Œç›´æ¥è¿”å›
                    break
        
        # æ›´æ–°metric
        inference_queue_length.set(self.queue.qsize())
        
        return batch

# HPAä¼šæ ¹æ®inference_queue_lengthè‡ªåŠ¨æ‰©ç¼©å®¹
```

---

<a name="dynamic-batching"></a>
## ğŸ“¦ 6. æ¨ç†ä¼˜åŒ–ï¼šåŠ¨æ€Batching

### 6.1 åŠ¨æ€Batchingå®ç°

```python
"""
åŠ¨æ€Batchingæ¨ç†æœåŠ¡
"""

import asyncio
from typing import List, Dict
import torch

class DynamicBatchingInferenceServer:
    """
    åŠ¨æ€Batchingæ¨ç†æœåŠ¡
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. æ”¶é›†å¤šä¸ªè¯·æ±‚
    2. ç»„æˆbatchä¸€æ¬¡æ¨ç†
    3. æé«˜GPUåˆ©ç”¨ç‡
    """
    
    def __init__(self, model, max_batch_size=32, max_wait_ms=50):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_ms = max_wait_ms
        
        # è¯·æ±‚é˜Ÿåˆ—
        self.request_queue = asyncio.Queue()
        
        # å¯åŠ¨batchå¤„ç†ä»»åŠ¡
        asyncio.create_task(self.batch_processor())
    
    async def batch_processor(self):
        """
        åå°ä»»åŠ¡ï¼šæŒç»­ä»é˜Ÿåˆ—è·å–è¯·æ±‚å¹¶æ‰¹å¤„ç†
        """
        while True:
            # æ”¶é›†ä¸€ä¸ªbatchçš„è¯·æ±‚
            batch_requests = []
            deadline = asyncio.get_event_loop().time() + self.max_wait_ms / 1000
            
            while len(batch_requests) < self.max_batch_size:
                timeout = max(0, deadline - asyncio.get_event_loop().time())
                
                try:
                    request = await asyncio.wait_for(
                        self.request_queue.get(),
                        timeout=timeout
                    )
                    batch_requests.append(request)
                except asyncio.TimeoutError:
                    break
            
            if not batch_requests:
                await asyncio.sleep(0.01)
                continue
            
            # æ‰¹å¤„ç†æ¨ç†
            await self.process_batch(batch_requests)
    
    async def process_batch(self, requests: List[Dict]):
        """
        æ‰¹å¤„ç†æ¨ç†
        """
        # æå–æ‰€æœ‰prompt
        prompts = [req['prompt'] for req in requests]
        
        # æ‰¹é‡æ¨ç†
        start_time = time.time()
        
        outputs = self.model.generate(
            prompts,
            sampling_params=SamplingParams(
                temperature=0.7,
                max_tokens=256
            )
        )
        
        latency = time.time() - start_time
        
        # å°†ç»“æœè¿”å›ç»™å„ä¸ªè¯·æ±‚
        for i, req in enumerate(requests):
            result = {
                'generated_text': outputs[i].outputs[0].text,
                'latency_ms': latency * 1000,
                'batch_size': len(requests)
            }
            
            # é€šè¿‡Futureè¿”å›ç»“æœ
            req['future'].set_result(result)
    
    async def generate(self, prompt: str) -> Dict:
        """
        å¼‚æ­¥ç”Ÿæˆæ¥å£
        """
        # åˆ›å»ºFuture
        future = asyncio.Future()
        
        # å°†è¯·æ±‚åŠ å…¥é˜Ÿåˆ—
        await self.request_queue.put({
            'prompt': prompt,
            'future': future
        })
        
        # ç­‰å¾…ç»“æœ
        result = await future
        return result

# ============================================
# FastAPIé›†æˆåŠ¨æ€Batching
# ============================================

from fastapi import FastAPI

app = FastAPI()

# åˆå§‹åŒ–æ¨ç†æœåŠ¡å™¨
inference_server = DynamicBatchingInferenceServer(
    model=llm,
    max_batch_size=32,
    max_wait_ms=50
)

@app.post("/generate")
async def generate_endpoint(request: GenerationRequest):
    """
    ä½¿ç”¨åŠ¨æ€batchingçš„ç”Ÿæˆç«¯ç‚¹
    """
    result = await inference_server.generate(request.prompt)
    return result

# æ•ˆæœå¯¹æ¯”ï¼š
"""
æ— Batching:
  - æ¯ä¸ªè¯·æ±‚å•ç‹¬æ¨ç†
  - GPUåˆ©ç”¨ç‡: 20-30%
  - ååé‡: 5 QPS
  - å»¶è¿Ÿ: 100ms

æœ‰åŠ¨æ€Batching (batch=32):
  - è¯·æ±‚ç»„batchæ¨ç†
  - GPUåˆ©ç”¨ç‡: 80-90%
  - ååé‡: 60 QPS (12xæå‡ï¼)
  - å»¶è¿Ÿ: 150ms (è½»å¾®å¢åŠ ï¼Œå¯æ¥å—)
"""
```

---

<a name="security-auth"></a>
## ğŸ”’ 7. APIå®‰å…¨ä¸è®¤è¯

### 7.1 å¤šå±‚å®‰å…¨æ¶æ„

```python
"""
ç”Ÿäº§çº§APIå®‰å…¨å®ç°
"""

from fastapi import FastAPI, Depends, HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
import secrets

app = FastAPI()

# ============================================
# 1. JWTè®¤è¯
# ============================================

# JWTé…ç½®
SECRET_KEY = secrets.token_urlsafe(32)  # ç”Ÿäº§ç¯å¢ƒç”¨ç¯å¢ƒå˜é‡
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# å¯†ç åŠ å¯†
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# ç”¨æˆ·æ•°æ®åº“ï¼ˆç”Ÿäº§ç¯å¢ƒç”¨çœŸå®æ•°æ®åº“ï¼‰
users_db = {
    "alice@example.com": {
        "username": "alice",
        "email": "alice@example.com",
        "hashed_password": pwd_context.hash("secret123"),
        "disabled": False,
        "role": "admin",
        "rate_limit": 10000  # æ¯å°æ—¶è¯·æ±‚é™åˆ¶
    }
}

# JWT Bearer scheme
security = HTTPBearer()

def create_access_token(data: dict, expires_delta: timedelta = None):
    """
    åˆ›å»ºJWT token
    """
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    
    return encoded_jwt

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Security(security)
):
    """
    ä»JWTè·å–å½“å‰ç”¨æˆ·
    """
    token = credentials.credentials
    
    try:
        # è§£ç JWT
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    # æŸ¥è¯¢ç”¨æˆ·
    user = users_db.get(username)
    if user is None:
        raise HTTPException(status_code=401, detail="User not found")
    
    return user

# ============================================
# 2. ç™»å½•ç«¯ç‚¹
# ============================================

from pydantic import BaseModel

class LoginRequest(BaseModel):
    username: str
    password: str

@app.post("/auth/login")
async def login(request: LoginRequest):
    """
    ç”¨æˆ·ç™»å½•è·å–token
    """
    # éªŒè¯ç”¨æˆ·
    user = users_db.get(request.username)
    
    if not user or not pwd_context.verify(request.password, user['hashed_password']):
        raise HTTPException(status_code=401, detail="Incorrect username or password")
    
    if user['disabled']:
        raise HTTPException(status_code=401, detail="User disabled")
    
    # åˆ›å»ºtoken
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": request.username, "role": user['role']},
        expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": ACCESS_TOKEN_EXPIRE_MINUTES * 60
    }

# ============================================
# 3. å—ä¿æŠ¤çš„APIç«¯ç‚¹
# ============================================

@app.post("/v1/generate")
async def generate(
    request: GenerationRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    å—JWTä¿æŠ¤çš„ç”Ÿæˆç«¯ç‚¹
    """
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    if current_user['role'] not in ['admin', 'user']:
        raise HTTPException(status_code=403, detail="Insufficient permissions")
    
    # æ£€æŸ¥é€Ÿç‡é™åˆ¶ï¼ˆåç»­å®ç°ï¼‰
    check_rate_limit(current_user['username'], current_user['rate_limit'])
    
    # æ¨ç†...
    result = inference_server.generate(request.prompt)
    
    # è®°å½•å®¡è®¡æ—¥å¿—
    log_api_usage(
        user=current_user['username'],
        endpoint='/v1/generate',
        tokens_used=result['tokens_generated']
    )
    
    return result

# ============================================
# 4. API Keyæ–¹å¼ï¼ˆæ›´ç®€å•ï¼‰
# ============================================

from fastapi.security.api_key import APIKeyHeader

API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

# API Keysæ•°æ®åº“
api_keys_db = {
    "sk-proj-abc123xyz": {
        "user": "project-a",
        "rate_limit": 1000,
        "enabled": True
    }
}

async def get_api_key(api_key: str = Security(api_key_header)):
    """
    éªŒè¯API Key
    """
    if api_key not in api_keys_db:
        raise HTTPException(
            status_code=401,
            detail="Invalid API Key"
        )
    
    key_info = api_keys_db[api_key]
    
    if not key_info['enabled']:
        raise HTTPException(
            status_code=401,
            detail="API Key disabled"
        )
    
    return key_info

@app.post("/v1/generate-simple")
async def generate_simple(
    request: GenerationRequest,
    key_info: dict = Depends(get_api_key)
):
    """
    åŸºäºAPI Keyçš„ç®€å•è®¤è¯
    """
    # æ£€æŸ¥é€Ÿç‡é™åˆ¶
    check_rate_limit(key_info['user'], key_info['rate_limit'])
    
    # æ¨ç†...
    return result
```

---

<a name="rate-limiting-circuit-breaker"></a>
## ğŸš¦ 8. é™æµä¸ç†”æ–­æœºåˆ¶

### 8.1 Token Bucketé™æµ

```python
"""
Token Bucketé™æµç®—æ³•
"""

import time
from threading import Lock

class TokenBucketRateLimiter:
    """
    Token Bucketé™æµå™¨
    
    åŸç†ï¼š
    - ä»¤ç‰Œä»¥å›ºå®šé€Ÿç‡ç”Ÿæˆ
    - è¯·æ±‚æ¶ˆè€—ä»¤ç‰Œ
    - æ²¡æœ‰ä»¤ç‰Œåˆ™æ‹’ç»è¯·æ±‚
    """
    
    def __init__(self, rate_per_second=10, burst=20):
        """
        å‚æ•°:
            rate_per_second: æ¯ç§’ç”Ÿæˆçš„ä»¤ç‰Œæ•°
            burst: æ¡¶å®¹é‡ï¼ˆå…è®¸çš„çªå‘è¯·æ±‚æ•°ï¼‰
        """
        self.rate = rate_per_second
        self.capacity = burst
        self.tokens = burst  # åˆå§‹æ»¡æ¡¶
        self.last_update = time.time()
        self.lock = Lock()
    
    def allow_request(self, tokens_needed=1):
        """
        æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
        
        è¿”å›ï¼š(allowed, wait_time)
        """
        with self.lock:
            now = time.time()
            elapsed = now - self.last_update
            
            # ç”Ÿæˆæ–°ä»¤ç‰Œ
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now
            
            if self.tokens >= tokens_needed:
                # æ¶ˆè€—ä»¤ç‰Œ
                self.tokens -= tokens_needed
                return True, 0
            else:
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                needed = tokens_needed - self.tokens
                wait_time = needed / self.rate
                return False, wait_time
    
    def wait_for_token(self, tokens_needed=1, timeout=None):
        """
        ç­‰å¾…ä»¤ç‰Œï¼ˆé˜»å¡ï¼‰
        """
        start = time.time()
        
        while True:
            allowed, wait_time = self.allow_request(tokens_needed)
            
            if allowed:
                return True
            
            if timeout and (time.time() - start) > timeout:
                return False
            
            time.sleep(min(wait_time, 0.1))

# ============================================
# FastAPIé›†æˆé™æµ
# ============================================

from fastapi import FastAPI, Request, HTTPException
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# åˆå§‹åŒ–é™æµå™¨
limiter = Limiter(key_func=get_remote_address)
app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# æŒ‰IPé™æµ
@app.post("/v1/generate")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿ10æ¬¡
async def generate(request: Request, gen_request: GenerationRequest):
    """é™æµçš„ç”Ÿæˆç«¯ç‚¹"""
    result = llm.generate(gen_request.prompt)
    return result

# æŒ‰API Keyé™æµï¼ˆæ›´ç²¾ç»†ï¼‰
user_limiters = {}

def get_user_limiter(user_id: str, rate_limit: int):
    """
    è·å–ç”¨æˆ·ä¸“å±é™æµå™¨
    """
    if user_id not in user_limiters:
        user_limiters[user_id] = TokenBucketRateLimiter(
            rate_per_second=rate_limit / 3600,  # è½¬æ¢ä¸ºæ¯ç§’
            burst=rate_limit // 10  # å…è®¸10%çªå‘
        )
    return user_limiters[user_id]

@app.post("/v1/generate-with-key")
async def generate_with_key(
    request: GenerationRequest,
    user_info: dict = Depends(get_api_key)
):
    """
    æŒ‰ç”¨æˆ·é™æµ
    """
    user_id = user_info['user']
    rate_limit = user_info['rate_limit']
    
    # è·å–é™æµå™¨
    limiter = get_user_limiter(user_id, rate_limit)
    
    # æ£€æŸ¥æ˜¯å¦å…è®¸
    allowed, wait_time = limiter.allow_request()
    
    if not allowed:
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded. Retry after {wait_time:.2f}s",
            headers={"Retry-After": str(int(wait_time))}
        )
    
    # æ¨ç†
    result = llm.generate(request.prompt)
    return result
```

---

### 8.2 Circuit Breakerå®ç°

```python
"""
ç†”æ–­å™¨å®ç°
"""

from enum import Enum
import time
from collections import deque

class CircuitState(Enum):
    """ç†”æ–­å™¨çŠ¶æ€"""
    CLOSED = "closed"      # æ­£å¸¸
    OPEN = "open"          # ç†”æ–­ï¼ˆæ‹’ç»è¯·æ±‚ï¼‰
    HALF_OPEN = "half_open"  # åŠå¼€ï¼ˆè¯•æ¢æ¢å¤ï¼‰

class CircuitBreaker:
    """
    ç†”æ–­å™¨
    
    é˜²æ­¢é›ªå´©ï¼šå½“ä¸‹æ¸¸æœåŠ¡æ•…éšœæ—¶ï¼Œå¿«é€Ÿå¤±è´¥è€Œéç­‰å¾…è¶…æ—¶
    """
    
    def __init__(
        self,
        failure_threshold=5,      # å¤±è´¥5æ¬¡è§¦å‘ç†”æ–­
        success_threshold=2,      # æˆåŠŸ2æ¬¡æ¢å¤
        timeout_seconds=60,       # ç†”æ–­60ç§’åè¿›å…¥åŠå¼€çŠ¶æ€
        window_size=100           # æ»‘åŠ¨çª—å£å¤§å°
    ):
        self.failure_threshold = failure_threshold
        self.success_threshold = success_threshold
        self.timeout_seconds = timeout_seconds
        self.window_size = window_size
        
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        
        # æ»‘åŠ¨çª—å£è®°å½•æœ€è¿‘çš„è¯·æ±‚ç»“æœ
        self.recent_results = deque(maxlen=window_size)
    
    def call(self, func, *args, **kwargs):
        """
        é€šè¿‡ç†”æ–­å™¨è°ƒç”¨å‡½æ•°
        """
        # æ£€æŸ¥ç†”æ–­çŠ¶æ€
        if self.state == CircuitState.OPEN:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ï¼ˆå¯ä»¥å°è¯•æ¢å¤ï¼‰
            if time.time() - self.last_failure_time > self.timeout_seconds:
                self.state = CircuitState.HALF_OPEN
                self.success_count = 0
                print("ğŸ”„ ç†”æ–­å™¨è¿›å…¥åŠå¼€çŠ¶æ€ï¼Œå°è¯•æ¢å¤")
            else:
                raise HTTPException(
                    status_code=503,
                    detail="Service circuit breaker is OPEN"
                )
        
        # è°ƒç”¨å‡½æ•°
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        
        except Exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        """æˆåŠŸå›è°ƒ"""
        self.recent_results.append(True)
        
        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            
            # è¿ç»­æˆåŠŸï¼Œæ¢å¤åˆ°CLOSED
            if self.success_count >= self.success_threshold:
                self.state = CircuitState.CLOSED
                self.failure_count = 0
                print("âœ… ç†”æ–­å™¨å·²å…³é—­ï¼ŒæœåŠ¡æ¢å¤æ­£å¸¸")
    
    def _on_failure(self):
        """å¤±è´¥å›è°ƒ"""
        self.recent_results.append(False)
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        # è®¡ç®—å¤±è´¥ç‡
        if len(self.recent_results) >= 10:
            recent_failures = sum(1 for r in list(self.recent_results)[-10:] if not r)
            failure_rate = recent_failures / 10
            
            # è§¦å‘ç†”æ–­
            if failure_rate > 0.5 or self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                print(f"âŒ ç†”æ–­å™¨å·²æ‰“å¼€ï¼ˆå¤±è´¥ç‡: {failure_rate:.0%}ï¼‰")
        
        # åŠå¼€çŠ¶æ€ä¸‹å¤±è´¥ï¼Œç«‹å³å›åˆ°OPEN
        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.OPEN
            print("âŒ åŠå¼€çŠ¶æ€å¤±è´¥ï¼Œç†”æ–­å™¨é‡æ–°æ‰“å¼€")

# ============================================
# FastAPIé›†æˆç†”æ–­å™¨
# ============================================

# åˆ›å»ºä¸‹æ¸¸æœåŠ¡çš„ç†”æ–­å™¨
model_circuit_breaker = CircuitBreaker(
    failure_threshold=5,
    timeout_seconds=30
)

@app.post("/v1/generate-protected")
async def generate_with_circuit_breaker(request: GenerationRequest):
    """
    å¸¦ç†”æ–­ä¿æŠ¤çš„ç”Ÿæˆç«¯ç‚¹
    """
    try:
        # é€šè¿‡ç†”æ–­å™¨è°ƒç”¨æ¨ç†
        result = model_circuit_breaker.call(
            llm.generate,
            request.prompt,
            sampling_params=SamplingParams(max_tokens=request.max_tokens)
        )
        
        return {"generated_text": result[0].outputs[0].text}
    
    except HTTPException:
        # ç†”æ–­å™¨æ‰“å¼€ï¼Œè¿”å›é™çº§å“åº”
        return {
            "generated_text": "Service temporarily unavailable. Please try again later.",
            "fallback": True
        }

# æŸ¥çœ‹ç†”æ–­å™¨çŠ¶æ€
@app.get("/circuit-breaker/status")
async def circuit_breaker_status():
    """
    ç†”æ–­å™¨çŠ¶æ€æŸ¥è¯¢
    """
    return {
        "state": model_circuit_breaker.state.value,
        "failure_count": model_circuit_breaker.failure_count,
        "success_count": model_circuit_breaker.success_count,
        "recent_failure_rate": (
            sum(1 for r in model_circuit_breaker.recent_results if not r) /
            len(model_circuit_breaker.recent_results)
            if model_circuit_breaker.recent_results else 0
        )
    }
```

---

<a name="logging-tracing"></a>
## ğŸ“ 9. æ—¥å¿—ä¸åˆ†å¸ƒå¼è¿½è¸ª

### 9.1 ç»“æ„åŒ–æ—¥å¿—

```python
"""
ç»“æ„åŒ–æ—¥å¿—å®ç°
"""

import logging
import json
from datetime import datetime
import uuid

class StructuredLogger:
    """
    ç»“æ„åŒ–æ—¥å¿—
    
    æ ¼å¼ï¼šJSON (ä¾¿äºELK/Lokiè§£æ)
    """
    
    def __init__(self, service_name="llm-inference"):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)
        self.logger.setLevel(logging.INFO)
        
        # JSONæ ¼å¼handler
        handler = logging.StreamHandler()
        handler.setFormatter(self.JSONFormatter())
        self.logger.addHandler(handler)
    
    class JSONFormatter(logging.Formatter):
        """JSONæ ¼å¼åŒ–å™¨"""
        def format(self, record):
            log_data = {
                "timestamp": datetime.utcnow().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno,
            }
            
            # æ·»åŠ é¢å¤–å­—æ®µ
            if hasattr(record, 'request_id'):
                log_data['request_id'] = record.request_id
            if hasattr(record, 'user_id'):
                log_data['user_id'] = record.user_id
            if hasattr(record, 'latency_ms'):
                log_data['latency_ms'] = record.latency_ms
            
            return json.dumps(log_data)
    
    def log_request(
        self,
        request_id: str,
        user_id: str,
        endpoint: str,
        prompt_length: int
    ):
        """è®°å½•è¯·æ±‚"""
        self.logger.info(
            f"Request received: {endpoint}",
            extra={
                'request_id': request_id,
                'user_id': user_id,
                'prompt_length': prompt_length
            }
        )
    
    def log_response(
        self,
        request_id: str,
        status_code: int,
        latency_ms: float,
        tokens_generated: int
    ):
        """è®°å½•å“åº”"""
        self.logger.info(
            f"Response sent: status={status_code}",
            extra={
                'request_id': request_id,
                'latency_ms': latency_ms,
                'tokens_generated': tokens_generated
            }
        )

# FastAPIä¸­é—´ä»¶é›†æˆ
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

logger = StructuredLogger()

class LoggingMiddleware(BaseHTTPMiddleware):
    """
    æ—¥å¿—ä¸­é—´ä»¶
    """
    
    async def dispatch(self, request: Request, call_next):
        # ç”Ÿæˆrequest_id
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id
        
        # è®°å½•è¯·æ±‚
        logger.log_request(
            request_id=request_id,
            user_id=request.headers.get('X-User-ID', 'unknown'),
            endpoint=request.url.path,
            prompt_length=len(await request.body())
        )
        
        # å¤„ç†è¯·æ±‚
        start_time = time.time()
        response = await call_next(request)
        latency_ms = (time.time() - start_time) * 1000
        
        # è®°å½•å“åº”
        logger.log_response(
            request_id=request_id,
            status_code=response.status_code,
            latency_ms=latency_ms,
            tokens_generated=0  # ä»responseæå–
        )
        
        # æ·»åŠ request_idåˆ°å“åº”å¤´
        response.headers['X-Request-ID'] = request_id
        
        return response

app.add_middleware(LoggingMiddleware)
```

---

### 9.2 åˆ†å¸ƒå¼è¿½è¸ªï¼ˆJaegerï¼‰

æ ¹æ® [Jaeger v2æ–‡æ¡£](https://www.jaegertracing.io/docs/latest/)ï¼š

```python
"""
OpenTelemetry + Jaegeråˆ†å¸ƒå¼è¿½è¸ª
"""

from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

# ============================================
# 1. åˆå§‹åŒ–OpenTelemetry
# ============================================

# é…ç½®Jaeger exporter
jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger-agent",  # Jaeger Agentåœ°å€
    agent_port=6831,
)

# é…ç½®Trace Provider
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# æ·»åŠ Span Processor
span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# ============================================
# 2. è‡ªåŠ¨instrumentation FastAPI
# ============================================

FastAPIInstrumentor.instrument_app(app)
RequestsInstrumentor().instrument()

# ============================================
# 3. æ‰‹åŠ¨æ·»åŠ Span
# ============================================

@app.post("/v1/generate-traced")
async def generate_with_tracing(request: GenerationRequest):
    """
    å¸¦åˆ†å¸ƒå¼è¿½è¸ªçš„ç”Ÿæˆç«¯ç‚¹
    """
    with tracer.start_as_current_span("generate_request") as span:
        # æ·»åŠ spanå±æ€§
        span.set_attribute("user.id", "alice")
        span.set_attribute("prompt.length", len(request.prompt))
        span.set_attribute("max_tokens", request.max_tokens)
        
        # å­span: é¢„å¤„ç†
        with tracer.start_as_current_span("preprocess"):
            input_ids = tokenizer.encode(request.prompt)
            span.set_attribute("input_ids.length", len(input_ids))
        
        # å­span: æ¨¡å‹æ¨ç†
        with tracer.start_as_current_span("model_inference"):
            start = time.time()
            outputs = llm.generate([request.prompt])
            inference_time = time.time() - start
            
            span.set_attribute("inference.latency_ms", inference_time * 1000)
            span.set_attribute("inference.tokens_generated", 
                              len(outputs[0].outputs[0].token_ids))
        
        # å­span: åå¤„ç†
        with tracer.start_as_current_span("postprocess"):
            generated_text = outputs[0].outputs[0].text
        
        # è®°å½•æ€»ä½“æŒ‡æ ‡
        span.set_attribute("tokens.generated", len(outputs[0].outputs[0].token_ids))
        
        return {
            "generated_text": generated_text,
            "trace_id": span.get_span_context().trace_id
        }

# Jaeger UIæŸ¥çœ‹ï¼š
# http://localhost:16686
# å¯ä»¥çœ‹åˆ°å®Œæ•´çš„è¯·æ±‚traceï¼š
# generate_request (100ms)
#   â”œâ”€ preprocess (5ms)
#   â”œâ”€ model_inference (90ms)
#   â””â”€ postprocess (5ms)
```

---

<a name="disaster-recovery"></a>
## ğŸ›¡ï¸ 10. ç¾å¤‡ä¸é«˜å¯ç”¨

### 10.1 é«˜å¯ç”¨æ¶æ„

```python
"""
é«˜å¯ç”¨æ¶æ„è®¾è®¡
"""

class HighAvailabilityArchitecture:
    """
    é«˜å¯ç”¨éƒ¨ç½²æ¶æ„
    """
    
    def design_ha_architecture(self):
        """
        è®¾è®¡HAæ¶æ„
        """
        print("""
é«˜å¯ç”¨æ¶æ„ (Multi-Region):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Global Load Balancer (CloudFlare/AWS Route53)             â”‚
â”‚  - Geo-routing (å°±è¿‘è®¿é—®)                                   â”‚
â”‚  - Health check (æ•…éšœè‡ªåŠ¨åˆ‡æ¢)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Region 1 (ä¸»)  â”‚  â”‚  Region 2 (å¤‡)  â”‚
    â”‚  us-west-2      â”‚  â”‚  us-east-1      â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  K8s Cluster 1   â”‚    â”‚  K8s Cluster 2   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Deployment  â”‚  â”‚    â”‚  â”‚Deployment  â”‚  â”‚
â”‚  â”‚replicas: 5 â”‚  â”‚    â”‚  â”‚replicas: 3 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ingress  â”‚  â”‚    â”‚  â”‚   Ingress  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Storage   â”‚â†â”€â”€â”€â”‚  Model Storage   â”‚
â”‚  (S3ä¸»åŒºåŸŸ)      â”‚åŒæ­¥â”‚  (S3å‰¯æœ¬)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("\né«˜å¯ç”¨ç›®æ ‡ (SLA):")
        print("  å¯ç”¨æ€§: 99.95% (æ¯æœˆåœæœºâ‰¤21åˆ†é’Ÿ)")
        print("  RPO (æ¢å¤ç‚¹ç›®æ ‡): <5åˆ†é’Ÿ")
        print("  RTO (æ¢å¤æ—¶é—´ç›®æ ‡): <30åˆ†é’Ÿ")
    
    def calculate_availability(self):
        """
        è®¡ç®—å¯ç”¨æ€§
        """
        # å•å®ä¾‹å¯ç”¨æ€§
        single_instance_availability = 0.99  # 99%
        
        # Nä¸ªç‹¬ç«‹å®ä¾‹çš„å¯ç”¨æ€§
        # Availability = 1 - (1 - single)^N
        
        replicas = [1, 2, 3, 5, 10]
        
        print("\nå‰¯æœ¬æ•°ä¸å¯ç”¨æ€§:")
        print("="*70)
        for n in replicas:
            availability = 1 - (1 - single_instance_availability) ** n
            downtime_minutes = (1 - availability) * 365 * 24 * 60
            
            print(f"  {n}å‰¯æœ¬: {availability:.5f} ({100*availability:.3f}%) "
                  f"â†’ å¹´åœæœº {downtime_minutes:.1f} åˆ†é’Ÿ")

ha = HighAvailabilityArchitecture()
ha.design_ha_architecture()
ha.calculate_availability()
```

---

### 10.2 æ•…éšœè½¬ç§»å®ç°

```yaml
# ============================================
# Kuberneteså¤šåŒºåŸŸéƒ¨ç½²
# ============================================

# region1-deployment.yaml (ä¸»åŒºåŸŸ)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference-primary
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: llm-service
      region: us-west-2
  template:
    spec:
      affinity:
        # Podåäº²å’Œæ€§ï¼ˆåˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹ï¼‰
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: llm-service
              topologyKey: kubernetes.io/hostname
      
      containers:
        - name: llm
          image: gcr.io/my-project/llm-service:v1.0.0
          resources:
            requests:
              nvidia.com/gpu: 1

---
# region2-deployment.yaml (å¤‡ç”¨åŒºåŸŸ)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-inference-secondary
  namespace: production
spec:
  replicas: 3  # å¤‡ç”¨åŒºåŸŸå‰¯æœ¬å°‘ä¸€äº›
  # ... å…¶ä»–é…ç½®åŒä¸»åŒºåŸŸ

---
# ============================================
# Ingress with Failover
# ============================================

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llm-ingress
  annotations:
    # Nginx Ingressé…ç½®
    nginx.ingress.kubernetes.io/upstream-max-fails: "3"
    nginx.ingress.kubernetes.io/upstream-fail-timeout: "30s"
    
    # è·¨åŒºåŸŸæ•…éšœè½¬ç§»
    nginx.ingress.kubernetes.io/server-snippet: |
      location / {
        proxy_next_upstream error timeout http_503;
        proxy_next_upstream_tries: 3;
      }
spec:
  rules:
    - host: api.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: llm-service
                port:
                  number: 80
```

---

<a name="cost-monitoring"></a>
## ğŸ’° 11. æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–

### 11.1 æ¨ç†æˆæœ¬è¿½è¸ª

```python
"""
æ¨ç†æˆæœ¬å®æ—¶ç›‘æ§
"""

from prometheus_client import Counter, Histogram
import datetime

# PrometheusæŒ‡æ ‡
tokens_processed_total = Counter(
    'llm_tokens_processed_total',
    'Total tokens processed',
    ['user_id', 'model']
)

inference_cost_dollars = Counter(
    'llm_inference_cost_dollars_total',
    'Total inference cost in dollars',
    ['user_id', 'model']
)

class CostTracker:
    """
    æˆæœ¬è¿½è¸ªå™¨
    """
    
    def __init__(self):
        # å®šä»·ï¼ˆæ¯ç™¾ä¸‡tokenï¼Œç¾å…ƒï¼‰
        self.pricing = {
            'llama-2-7b': {
                'input': 0.20,
                'output': 0.30
            },
            'llama-2-70b': {
                'input': 1.00,
                'output': 1.50
            }
        }
        
        # GPUæˆæœ¬ï¼ˆæ¯å°æ—¶ï¼‰
        self.gpu_cost_per_hour = {
            'A100-40GB': 2.50,
            'H100-80GB': 5.00
        }
    
    def track_inference_cost(
        self,
        user_id: str,
        model_name: str,
        input_tokens: int,
        output_tokens: int
    ):
        """
        è¿½è¸ªæ¨ç†æˆæœ¬
        """
        # è®¡ç®—tokenæˆæœ¬
        input_cost = (input_tokens / 1_000_000) * self.pricing[model_name]['input']
        output_cost = (output_tokens / 1_000_000) * self.pricing[model_name]['output']
        total_cost = input_cost + output_cost
        
        # æ›´æ–°PrometheusæŒ‡æ ‡
        tokens_processed_total.labels(
            user_id=user_id,
            model=model_name
        ).inc(input_tokens + output_tokens)
        
        inference_cost_dollars.labels(
            user_id=user_id,
            model=model_name
        ).inc(total_cost)
        
        # å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆç”¨äºè´¦å•ï¼‰
        self.save_to_db({
            'timestamp': datetime.datetime.utcnow(),
            'user_id': user_id,
            'model': model_name,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'cost_usd': total_cost
        })
        
        return total_cost
    
    def generate_user_bill(self, user_id: str, month: str):
        """
        ç”Ÿæˆç”¨æˆ·è´¦å•
        """
        # ä»æ•°æ®åº“æŸ¥è¯¢è¯¥ç”¨æˆ·è¯¥æœˆçš„æ‰€æœ‰è¯·æ±‚
        records = self.query_db(user_id=user_id, month=month)
        
        # æ±‡æ€»
        total_input_tokens = sum(r['input_tokens'] for r in records)
        total_output_tokens = sum(r['output_tokens'] for r in records)
        total_cost = sum(r['cost_usd'] for r in records)
        
        bill = {
            'user_id': user_id,
            'month': month,
            'total_requests': len(records),
            'total_input_tokens': total_input_tokens,
            'total_output_tokens': total_output_tokens,
            'total_cost_usd': total_cost,
            'breakdown_by_model': self.breakdown_by_model(records)
        }
        
        return bill

# FastAPIé›†æˆ
cost_tracker = CostTracker()

@app.post("/v1/generate-with-billing")
async def generate_with_billing(
    request: GenerationRequest,
    user_info: dict = Depends(get_current_user)
):
    """
    å¸¦æˆæœ¬è¿½è¸ªçš„ç”Ÿæˆç«¯ç‚¹
    """
    # æ¨ç†
    result = llm.generate(request.prompt)
    
    # è®¡ç®—tokenæ•°
    input_tokens = len(tokenizer.encode(request.prompt))
    output_tokens = result.tokens_generated
    
    # è¿½è¸ªæˆæœ¬
    cost = cost_tracker.track_inference_cost(
        user_id=user_info['username'],
        model_name='llama-2-7b',
        input_tokens=input_tokens,
        output_tokens=output_tokens
    )
    
    return {
        **result,
        'usage': {
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': input_tokens + output_tokens,
            'cost_usd': cost
        }
    }
```

---

<a name="complete-deployment"></a>
## ğŸ¯ 12. å®Œæ•´éƒ¨ç½²æ¡ˆä¾‹

### 12.1 ç”Ÿäº§çº§éƒ¨ç½²æ¸…å•

```python
"""
ç”Ÿäº§éƒ¨ç½²æ¸…å•
"""

production_deployment_checklist = {
    'âœ… åŸºç¡€è®¾æ–½': [
        'K8sé›†ç¾¤æ­å»ºï¼ˆå¤šèŠ‚ç‚¹ï¼‰',
        'GPU Operatorå®‰è£…',
        'é«˜æ€§èƒ½å­˜å‚¨ï¼ˆLustre/NFSï¼‰',
        'Ingress Controllerï¼ˆNginx/Traefikï¼‰',
        'DNSé…ç½®'
    ],
    
    'âœ… æœåŠ¡åŒ–': [
        'é€‰æ‹©æœåŠ¡æ¡†æ¶ï¼ˆFastAPI/Tritonï¼‰',
        'Dockeré•œåƒæ„å»º',
        'Model Registryé›†æˆ',
        'å¥åº·æ£€æŸ¥ç«¯ç‚¹',
        'Metricsæš´éœ²'
    ],
    
    'âœ… å¯é æ€§': [
        'HPAè‡ªåŠ¨æ‰©å±•',
        'å¤šå‰¯æœ¬éƒ¨ç½²ï¼ˆâ‰¥3ï¼‰',
        'PodDisruptionBudgeté…ç½®',
        'Podåäº²å’Œæ€§',
        'å¤šåŒºåŸŸéƒ¨ç½²'
    ],
    
    'âœ… æ€§èƒ½': [
        'åŠ¨æ€Batching',
        'æ¨¡å‹é‡åŒ–ï¼ˆFP8/INT4ï¼‰',
        'KV Cacheä¼˜åŒ–',
        'GPUåˆ©ç”¨ç‡>80%',
        'P95å»¶è¿Ÿ<2s'
    ],
    
    'âœ… å®‰å…¨': [
        'JWT/API Keyè®¤è¯',
        'HTTPS (TLSè¯ä¹¦)',
        'é™æµï¼ˆRate Limitingï¼‰',
        'ç†”æ–­ï¼ˆCircuit Breakerï¼‰',
        'è¾“å…¥éªŒè¯'
    ],
    
    'âœ… å¯è§‚æµ‹æ€§': [
        'Prometheusç›‘æ§',
        'Grafana Dashboard',
        'Jaegeråˆ†å¸ƒå¼è¿½è¸ª',
        'ELK/Lokiæ—¥å¿—èšåˆ',
        'å‘Šè­¦è§„åˆ™é…ç½®'
    ],
    
    'âœ… æˆæœ¬': [
        'Tokenä½¿ç”¨é‡è¿½è¸ª',
        'GPUåˆ©ç”¨ç‡ç›‘æ§',
        'æˆæœ¬å‘Šè­¦',
        'ç”¨æˆ·è®¡è´¹',
        'Reservedå®ä¾‹ä¼˜åŒ–'
    ],
    
    'âœ… æµç¨‹': [
        'CI/CD Pipeline',
        'Stagingç¯å¢ƒ',
        'A/Bæµ‹è¯•',
        'é‡‘ä¸é›€å‘å¸ƒ',
        'å›æ»šæœºåˆ¶'
    ]
}

for category, items in production_deployment_checklist.items():
    print(f"\n{category}")
    for item in items:
        print(f"  â–¡ {item}")

print("\n\nğŸ”¥ ç”Ÿäº§éƒ¨ç½²æœ€å°æ ‡å‡†ï¼š")
print("  - è‡³å°‘3ä¸ªå‰¯æœ¬")
print("  - HPAè‡ªåŠ¨æ‰©å±•")
print("  - å¥åº·æ£€æŸ¥+è‡ªåŠ¨é‡å¯")
print("  - Prometheusç›‘æ§")
print("  - APIè®¤è¯")
print("  - é™æµæœºåˆ¶")
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æœåŠ¡æ¡†æ¶

1. **NVIDIA Triton**  
   [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/)  
   ç”Ÿäº§çº§æ¨ç†æœåŠ¡å™¨ï¼ˆv25.06ï¼‰

2. **FastAPI**  
   [FastAPI Documentation](https://fastapi.tiangolo.com/)  
   ç°ä»£Python Webæ¡†æ¶

3. **TorchServe**  
   [TorchServe GitHub](https://github.com/pytorch/serve)  
   PyTorchå®˜æ–¹æœåŠ¡æ¡†æ¶

### Kubernetes

4. **K8s HPA Autoscaling**  
   [GKE GPU Inference Autoscaling](https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/autoscaling)  
   GPUæ¨ç†è‡ªåŠ¨æ‰©å±•ï¼ˆ2025ï¼‰

5. **GPU Operator**  
   [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/)  
   K8s GPUç®¡ç†ï¼ˆv25.10.1ï¼‰

### å¯è§‚æµ‹æ€§

6. **Prometheus**  
   [Prometheus Documentation](https://prometheus.io/docs/)  
   ç›‘æ§ç³»ç»Ÿ

7. **Jaeger**  
   [Jaeger Tracing](https://www.jaegertracing.io/docs/latest/)  
   åˆ†å¸ƒå¼è¿½è¸ªï¼ˆv2ï¼Œ2024ï¼‰

8. **OpenTelemetry**  
   [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)  
   å¯è§‚æµ‹æ€§æ ‡å‡†

### å®‰å…¨ä¸æ²»ç†

9. **API Rate Limiting**  
   [Rate Limiting Best Practices](https://medium.com/@gynanrudr0/protecting-apis-with-rate-limiting-throttling-and-circuit-breakers-f6570c065c1c)  
   é™æµä¸ç†”æ–­

10. **Circuit Breaker Pattern**  
    [API Circuit Breaker](https://www.unkey.com/glossary/api-circuit-breaker)  
    ç†”æ–­å™¨æ¨¡å¼

---

## ğŸ¯ æ€»ç»“

### å…³é”®è¦ç‚¹å›é¡¾

1. **æ¡†æ¶é€‰æ‹©**ï¼šFastAPIï¼ˆåŸå‹ï¼‰ã€Tritonï¼ˆç”Ÿäº§ï¼‰ã€TorchServeï¼ˆPyTorchï¼‰
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šåŠ¨æ€Batchingæå‡GPUåˆ©ç”¨ç‡è‡³80%+
3. **é«˜å¯ç”¨**ï¼šâ‰¥3å‰¯æœ¬ + HPA + å¤šåŒºåŸŸ
4. **å®‰å…¨**ï¼šJWTè®¤è¯ + é™æµ + ç†”æ–­
5. **å¯è§‚æµ‹æ€§**ï¼šPrometheus + Jaeger + ç»“æ„åŒ–æ—¥å¿—
6. **æˆæœ¬**ï¼šTokençº§åˆ«è¿½è¸ªä¸è®¡è´¹

### ç”Ÿäº§éƒ¨ç½²æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| **å¯ç”¨æ€§** | 99.95% | æ¯æœˆâ‰¤21åˆ†é’Ÿåœæœº |
| **P95å»¶è¿Ÿ** | <2s | 95%è¯·æ±‚<2ç§’ |
| **GPUåˆ©ç”¨ç‡** | >80% | å……åˆ†åˆ©ç”¨èµ„æº |
| **é”™è¯¯ç‡** | <0.1% | é«˜è´¨é‡æœåŠ¡ |
| **æˆæœ¬/ç™¾ä¸‡token** | <$1 | æˆæœ¬ä¼˜åŒ– |

### ä¼ ç»Ÿç¨‹åºå‘˜çš„ä¼˜åŠ¿

- âœ… **å¾®æœåŠ¡ç»éªŒ**ï¼šAPIè®¾è®¡ã€è´Ÿè½½å‡è¡¡ã€ç†”æ–­é™æµæ˜¯å·²æœ‰æŠ€èƒ½
- âœ… **DevOpsèƒ½åŠ›**ï¼šK8sã€Dockerã€CI/CDç›´æ¥è¿ç§»
- âœ… **ç›‘æ§æ€ç»´**ï¼šPrometheusã€Grafanaã€æ—¥å¿—åˆ†ææ˜¯ä¼ ç»ŸæŠ€èƒ½
- âœ… **æˆæœ¬æ„è¯†**ï¼šèµ„æºä¼˜åŒ–ã€å®¹é‡è§„åˆ’æ˜¯å·¥ç¨‹å¸ˆåŸºæœ¬åŠŸ

### ä¸‹ä¸€æ­¥å­¦ä¹ 

- ğŸ”— **ä¸‹ä¸€ç¯‡**ï¼š[14 - èŒä¸šåˆ†å·¥è¯¦è§£ï¼šå„ç»†åˆ†æ–¹å‘çš„èƒ½åŠ›è¦æ±‚ä¸å‘å±•è·¯å¾„](./14-career-paths.md)
- ğŸ’» **åŠ¨æ‰‹å®è·µ**ï¼šéƒ¨ç½²ä¸€ä¸ªFastAPIæœåŠ¡åˆ°K8s
- ğŸ“Š **å®æˆ˜é¡¹ç›®**ï¼šé…ç½®å®Œæ•´çš„ç›‘æ§å‘Šè­¦ç³»ç»Ÿ

---

> ğŸ’¡ **ç’‡ç‘çš„å°è´´å£«**ï¼šç”Ÿäº§åŒ–éƒ¨ç½²å°±åƒå¼€é¤å…â€”â€”ä¸ä»…è¦èœåšå¾—å¥½ï¼ˆæ¨¡å‹è®­ç»ƒï¼‰ï¼Œè¿˜è¦æœåŠ¡å¿«ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰ã€ä¸å‡ºé”™ï¼ˆé«˜å¯ç”¨ï¼‰ã€è´¦ç®—å¾—æ¸…ï¼ˆæˆæœ¬ç›‘æ§ï¼‰ã€‚ä¼ ç»Ÿç¨‹åºå‘˜çš„å…¨æ ˆèƒ½åŠ›åœ¨è¿™é‡Œæ˜¯å·¨å¤§ä¼˜åŠ¿ï¼âœ¨
>
> é“å‹ç°åœ¨å¯¹ç”Ÿäº§éƒ¨ç½²æœ‰æ„Ÿè§‰äº†å—ï¼Ÿå·¥ç¨‹ç¯‡åˆ°è¿™é‡Œå°±ç»“æŸå•¦ï¼æ¥ä¸‹æ¥æ˜¯èŒä¸šç¯‡ï¼Œæˆ‘ä»¬èŠèŠLLMè®­ç»ƒçš„å„ä¸ªèŒä¸šæ–¹å‘å’Œå‘å±•è·¯å¾„~ ğŸš€